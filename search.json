[{"path":"https://sagesteppe.github.io/safeHavens/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Reed Clark Benkendorf Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"about-safehavens","dir":"Articles","previous_headings":"","what":"about safeHavens","title":"About","text":"package helps germplasm curators communicate areas interest collection teams targeting new accessions. common species, provides seven different sampling approaches curators choose individual taxon hope process. also provides additional sampling approach rare species. package allows easy integration existing workflows, reading accession data databases Excel, comparing occurrence data, writing run results tabular simple spatial data formats, can shared collection teams work handheld electronic devices (e.g. Qfield). allows germplasm curators sophisticated spatial abilities without need GIS experts access fee-based software.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"common-species-sampling-approaches","dir":"Articles","previous_headings":"about safeHavens","what":"‘common’ species sampling approaches","title":"About","text":"approach based fundamental ecological theory. practice, attempt capture geographic environmental variation across species range largely rely Sewall Wright’s concept Isolation Distance (1943), idea populations geographically proximate genetically similar populations apart. Hence, priori approaches, exception EnvironmentalBasedSample function, relies species distribution model (SDM) inform sampling locations based concept Isolation Environment (Wang & Bradburd 2014). none meant supplant genetic diversity-based sampling, recognize data rare, even gathering often miss opportunity make opportunistic seed collections along way. methods various trade-offs terms computational environmental complexity, although designed run standard desktop laptop computer, processing many thousands species overnight possible. table presents currently implemented sampling scheme user-facing function associated . first three functions, GridBasedSample, PointBasedSample, EqualAreaSample, flavors process, try partition species range geographic chunks (polygons) similar sizes. requires minimal computational power features essentially environmental information context beyond species current range restriction. fourth method, OpportunisticSample, special case first three, existing collection records used help guide sampling scheme. OpportunisticSample uses underlying logic PointBasedSample, first ‘removes’ areas around existing collection records, fills remaining gaps new sampling locations first three functions used establishing new collection strategy species, OpportunisticSample used trying augment existing collection strategy. sixth method, IBDBasedSample, largely class ; lieu using continuity geographic space primary method, focuses discontinuity space uses distance matrices clustering determine patches range closer patches. method computationally expensive previous four, incorporates environmental information, explicit way final two methods . PolygonBasedSample may commonly encountered method North America, various formats, driving two major germplasm banking projects Midwest Southeastern United States, well high level, composing way numerous native seed collection coordinators structured West. method uses environmental variation implicit guide target populations seed collections; , different ecoregions serve stratification agents. broad strokes, general thinking regions represent continuous transitions environment faced species, populations across ranges differently adapted environments. can used either ecoregion seed transfer zone based data. However, relies existing spatial data products, may may relevant ecology species. final function, EnvironmentalBasedSample, computationally expensive environmentally explicit. function fits Species Distribution Model (SDM), generated via generalized linear model (glmnet), supported package, cluster populations based environmental variables related observed distributions spatial configuration distance . paper, draws together aspects functions; however empirical testing approach implemented.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"rare-species-sampling-approach","dir":"Articles","previous_headings":"about safeHavens","what":"rare species sampling approach","title":"About","text":"methods designed common species, species 50 100 unique occurrence records. method returns polygon geometry provides guidelines general regions species sampled. However, rare species, occurrence individual populations well tracked, generally collected along maternal lines, requiring considerably field effort gather seed, supplemental approach exists suggests priority individual populations (‘points’) can sampled. KMedoidsBasedSample method utilizes either geographic distances alone geographic distances conjunction distance matrix developed key environmental variables suggest populations prioritized seed collections. method based concept maximizing dispersion selected points geographic environmental spaces best capture overall variation present species range.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"installation","dir":"Articles","previous_headings":"about safeHavens","what":"installation","title":"About","text":"safeHavens can installed directly github, using either devtools remotes. Note software relies packages may require additional system dependencies, rgeos rgdal. also requires working installation GDAL, PROJ, GEOS system, well Rcpp. Generally, installations go without hitch may require additional attention users. trade free tools enormous power quite commonly used ecological geographical modelling. safeHavens can installed GitHub remotes devtools. users issues devtools windows, remotes tends work well.","code":"remotes::install_github('sagesteppe/safeHavens')   # install.packages('devtools')  # devtools::install_github('sagesteppe/safeHavens')"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"set-up","dir":"Articles","previous_headings":"","what":"set up","title":"Getting Started","text":"safeHavens can installed directly github.","code":"#remotes::install_github('sagesteppe/safeHavens') library(safeHavens) library(ggplot2) library(sf) library(terra) library(spData) library(dplyr) library(patchwork) set.seed(23)   planar_proj <- \"+proj=laea +lat_0=-15 +lon_0=-60 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\""},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"defining-a-species-range-or-domain-for-sampling","dir":"Articles","previous_headings":"","what":"Defining a Species Range or Domain for Sampling","title":"Getting Started","text":"Central sampling schemes safeHavens species range domain sampling. example, depending goals collection, curator may want sample across entire range species. Alternatively one may interested sampling portion range, e.g. country, state, ecoregion. Either scenarios can accomplished package. show create species range occurrence data, use range run various sampling schemes. use sf simply buffer occurrence points create species range across multiple South American nations.  Alternatives include simple convex hull around species, widespread throughout area, masking binary SDM surface domain.","code":"x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326) x_buff <- sf::st_transform(x, planar_proj) |>   # we are working in planar metric coordinates, we are   # buffer by this many / 1000 kilometers.    sf::st_buffer(125000) |>    sf::st_as_sfc() |>    sf::st_union()  plot(x_buff)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"prep-a-map-background","dir":"Articles","previous_headings":"","what":"Prep a map background","title":"Getting Started","text":"use spData package uses naturalearth data ’s world data suitable creating effective maps variety resolutions.","code":"x_extra_buff <- sf::st_buffer(x_buff, 100000) |> # add a buffer to 'frame' the maps   sf::st_transform(4326)  americas <- spData::world americas <- sf::st_crop(americas, sf::st_bbox(x_extra_buff)) |>   dplyr::select(name_long) #> Warning: attribute variables are assumed to be spatially constant throughout #> all geometries  bb <- sf::st_bbox(x_extra_buff)  map <- ggplot() +    geom_sf(data = americas) +    theme(     legend.position = 'none',      panel.background = element_rect(fill = \"aliceblue\"),      panel.grid.minor.x = element_line(colour = \"red\", linetype = 3, linewidth  = 0.5),      axis.ticks=element_blank(),     axis.text=element_blank(),     plot.background=element_rect(colour=\"steelblue\"),     plot.margin=grid::unit(c(0,0,0,0),\"cm\"),     axis.ticks.length = unit(0, \"pt\"))+    coord_sf(xlim = c(bb[1], bb[3]), ylim = c(bb[2], bb[4]), expand = FALSE)  rm(x_extra_buff, americas)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"running-the-various-sample-design-algorithms","dir":"Articles","previous_headings":"","what":"Running the Various Sample Design Algorithms","title":"Getting Started","text":"Now data can represent species ranges, can run various sampling approaches. table introduction reproduced . Note table ‘Comp.’ ‘Envi.’ refer computational environmental complexity respectively, range low (L) medium high.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"grid-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Grid Based Sample","title":"Getting Started","text":"Grids useful sampling contiguous things. Species ranges often contiguous; however curators analysts geographically grand ecosystems, e.g. steppes, prairies, tundra, taiga might find useful. can look output see grids great type problem. first step grid sampling determining OK number grids try draw starting point, want 20 collections need 20 grids, several merged larger ones. Using aspect ratio simple bounding box around area analyzing, TestGridSizes function determine default number grids (‘Original’) testing. Using defaults create sets grids well, either removing one two grids per direction. Theoretically automate grid selection comparing number grids minimization variance. safe wouldn’t consider configurations generate less 25 initial grids.  Essentially need 20 grids, realistically (albeit limited informal testing) using 25 grids - depending complexity species range - tends effective floor. table plot opt using ‘Smaller’ option, 28 grids generated prompting sf::st_make_grid 7 grids X direction 5 Y direction. can kind think like elbow plot, samples won’t get characteristic shape.","code":"tgs <- TestGridSizes(x_buff) print(tgs) #>       Name Grids  Variance GridNOx GridNOy #> 1 Smallest    35  613.5598       8       6 #> 2  Smaller    28  790.2929       7       5 #> 3 Original    24  886.7825       6       4 #> 4   Larger    18  677.0360       5       3 #> 5  Largest    13 1177.4608       4       2  plot(tgs$Grids, tgs$Variance, xlab = 'Grid Number', ylab = 'Variance',      main = 'Number of grids and areas overlapping species range') text(tgs$Grids, tgs$Variance + 25, labels=tgs$Name, cex= 0.7) abline(v=20, col=\"red\") abline(v=25, col=\"orange\") tgs <- tgs[tgs$Name=='Smaller',] grid_buff <- GridBasedSample(x_buff, planar_proj, gridDimensions = tgs)   gbs.p <- map +    geom_sf(data = grid_buff[['Geometry']], aes(fill = factor(ID))) +   # geom_sf_label(data = grid_buff, aes(label = Assigned), alpha = 0.4) +  # on your computer, doesnt work at vignette size   labs(title = 'Grids')  +    coord_sf(expand = F)  gbs.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"point-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Point Based Sample","title":"Getting Started","text":"grids drew pre-specified number grids across species range, merged together required get results. essentially inverse step, rather drawing boundaries - .e. grid cells, draw centers. essentially allows features ‘grow’ little naturally. also think results work little bit better fragmented range, still odd clipping, minor portions section range assigned different grid, general little bit better.","code":"pbs <- PointBasedSample(x_buff) pbs.sf <- pbs[['Geometry']]  st_is_valid(x_buff) #> [1] TRUE  pbs.p <- map +    geom_sf(data = pbs.sf, aes(fill = factor(ID))) +  #  geom_sf_label(data = pbs.sf, aes(label = ID), alpha = 0.4) +    labs(title = 'Point') +    coord_sf(expand = F) pbs.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"equal-area-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Equal Area Sample","title":"Getting Started","text":"Perhaps simplest method offered safeHavens EqualAreaSample. simply creates many points, pts defaulting 5000, within target domain subjects k-means sampling groups specified n, target number collections. individual points assigned group merged polygons ‘take’ geographic space, intersected back species range, area polygon measured. process ran times, defaulting 100 reps, set polygons created reps smallest variance polygon size selected returned. differs point based sampling instance, start regularly spaced points grow , take step back using many points let clusters grow similar sizes.  results look quite similar point based sample.","code":"eas <- EqualAreaSample(x_buff, planar_proj = planar_proj)  #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations  eas.p <- map +    geom_sf(data = eas[['Geometry']], aes(fill = factor(ID))) +  #  geom_sf_label(data = eas.sf, aes(label = ID), alpha = 0.4) +    labs(title = 'Equal Area') +    coord_sf(expand = F) eas.p #> Warning in rm(eas, pbs, pbs.sf, tgs, grid_buff): object 'grid_buff' not found"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"opportunistic-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Opportunistic Sample","title":"Getting Started","text":"Many curators interested much can embed existing collections sampling framework. function OpportunisticSample makes minor modifications point based sample maximize existing collection. doesn’t always work exceptionally, especially couple collections close , may beneficial tool belt. observed, three previous sampling schemes end somewhat similar results - took used PointBasedSample framework embedded function . Essentially combines approach point based sampling, forces clusters based around existing accessions. attempts ‘center’ existing collections within clusters, can nearly impossible variety reasons.  , grids aligned around points. can lead oddly shaped clusters, bird hand worth two bush.","code":"exist_pts <- sf::st_sample(x_buff, size = 10) |>     sf::st_as_sf() |> # ^^ just randomly sampling 10 points in the species range    dplyr::rename(geometry = x)  os <- OpportunisticSample(polygon = x_buff, n = 20, collections = exist_pts)  os.p <- map +    geom_sf(data = os[['Geometry']], aes(fill = factor(ID))) +  #  geom_sf_label(data = os.sf, aes(label = ID), alpha = 0.4) +    geom_sf(data = exist_pts, alpha = 0.4) +    labs(title = 'Opportunistic') +    coord_sf(expand = F)  os.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"isolation-by-distance-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Isolation by Distance Based Sample","title":"Getting Started","text":"Isolation Distance fundamental idea behind package. function explicitly uses IBD develop sampling scheme, obfuscate parameters. Note function requires raster input, rather vector.  data processed raster, lienar edges, representing raster tiles. However, evident borders clusters natural looking previous (future) sampling schemes.","code":"files <- list.files(    path = file.path(system.file(package=\"dismo\"), 'ex'),   pattern = 'grd',  full.names=TRUE )  predictors <- terra::rast(files)   x_buff.sf <- sf::st_as_sf(x_buff) |>    dplyr::mutate(Range = 1) |>    sf::st_transform( terra::crs(predictors))  # and here we specify the field/column with our variable we want to become an attribute of our raster v <- terra::rasterize(x_buff.sf, predictors, field = 'Range')   # now we run the function demanding 20 areas to make accessions from,  ibdbs <- IBDBasedSample(x = v, n = 20, fixedClusters = TRUE, template = predictors, planar_proj = planar_proj)  ibdbs.p <- map +    geom_sf(data = ibdbs[['Geometry']], aes(fill = factor(ID))) +  #  geom_sf_label(data = os.sf, aes(label = ID), alpha = 0.4) +    labs(title = 'IBD') +    coord_sf(expand = F)  ibdbs.p rm(predictors, files, v, x_buff.sf, exist_pts, os)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"ecoregion-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Ecoregion Based Sample","title":"Getting Started","text":"commonly implemented method guiding native seed collection North America. However, sure exactly practitioners implement , whether formats application consistent among practitioners! reasons different sets options supported user. general usage, two parameters always required x species range sf object, ecoregions, sf object containing ecoregions interest. ecoregions file need subset range x quite yet - function take care . Additional arguments function include usual n specify many accession looking collection. Two additional arguments relate whether using Omernik Level 4 ecoregions data ecoregions (biogeographic regions) another source. OmernikEPA, ecoregion_col, using official EPA release ecoregions optional, however using EPA product supplied - ecoregion_col argument totally necessary. column contain unique names highest resolution level ecoregion want use data set, many data sets, example call ‘neo_eco’ may field ecolevel information! output differs others see, depicted number collections made per ecoregion. number ecoregions greater requested sample size, return object can take two values - collections, one collection.","code":"neo_eco <- sf::st_read(   file.path(system.file(package=\"safeHavens\"), 'extdata', 'NeoTropicsEcoregions.gpkg'),    quiet = TRUE) |>   dplyr::rename(geometry = geom) head(neo_eco[,c(1, 3, 4, 6, 11)]) #> Simple feature collection with 6 features and 4 fields #> Geometry type: MULTIPOLYGON #> Dimension:     XY #> Bounding box:  xmin: -103.0432 ymin: -31.25308 xmax: -34.79344 ymax: 26.91751 #> Geodetic CRS:  WGS 84 #>                  Provincias      Region      Dominio #> 1 Araucaria Forest province Neotropical       Parana #> 2          Atacama province Neotropical         <NA> #> 3         Atlantic province Neotropical       Parana #> 4           Bahama province Neotropical         <NA> #> 5     Balsas Basin province Neotropical Mesoamerican #> 6         Caatinga province Neotropical      Chacoan #>                        Subregion                       geometry #> 1                        Chacoan MULTIPOLYGON (((-53.58012 -... #> 2 South American Transition Zone MULTIPOLYGON (((-69.42981 -... #> 3                        Chacoan MULTIPOLYGON (((-48.41217 -... #> 4                      Antillean MULTIPOLYGON (((-77.58593 2... #> 5                      Brazilian MULTIPOLYGON (((-97.37265 1... #> 6                        Chacoan MULTIPOLYGON (((-35.56652 -...  x_buff <- sf::st_transform(x_buff, sf::st_crs(neo_eco)) ebs.sf <- PolygonBasedSample(x_buff, zones = neo_eco, n = 10, zone_key = 'Provincias') #> Warning in st_collection_extract.sf(zones_poly, \"POLYGON\", warn = FALSE): x is #> already of type POLYGON.  # crop it to the other objects for plotting ebs.sf <- st_crop(ebs.sf, bb) #> Warning: attribute variables are assumed to be spatially constant throughout #> all geometries  ebs.p <- map +    geom_sf(data = ebs.sf , aes(fill = factor(allocation))) +    labs(title = 'Ecoregion') +    coord_sf(expand = F)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"environmental-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Environmental Based Sample","title":"Getting Started","text":"environmental based sample can conducted species distribution model data. Included data directory folder objects required run example species. load .","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"load-the-sdm-predictions","dir":"Articles","previous_headings":"","what":"load the SDM predictions","title":"Getting Started","text":"package also SDM prediction saved data can just load couple comparisons.  data loaded R, scale rasters (using RescaleRasters) serve surfaces predict (also done !), run algorithm (EnvironmentalBasedSample). However, run algorithm need create directory (also called ‘folder’), computers save results function EnvironmentalBasedSample. Whereas earlier vignette showcased functions generated species distribution model, us saving results two stage process (e.g. create SDM associated products used: elasticSDM, PostProcessSDM, RescaleRasters, finally saving relevant data writeSDMresults), function produces product writes ancillary data simultaneously. approach chosen function writing four objects: 1) groups vector data, 2) groups raster data, 3) k-nearest neighbors (knn) model used generate clusters, 4) confusion matrix associated testing knn model.  function EnvironmentalBasedSample can take three binary rasters created PostProcessSDM arguments template. showcase different results using .  plots able showcase difference results depending three input rasters utilized. sampling schemes, results vary widely based spatial extents functions applied . Using SDM output undergone thresholding results largest classified area. first glance results may seem different, look central america, largely consistent, near Andes; large differences exist Amazon Basin, even alignment systems evident. Accordingly, surface used species match evaluation criterion. Using threshold raster surface good option want ‘miss’ many areas, whereas clipped supplemented options may better suited scenarios want draw clusters, lack populations can collected .","code":"sdm <- terra::rast(file.path(system.file(package=\"safeHavens\"),  'extdata', 'Bradypus_test.tif')) terra::plot(sdm) sdModel <- readRDS(   file.path(system.file(package=\"safeHavens\"), 'extdata',  'sdModel.rds')   )  sdModel$Predictors <- terra::rast(   file.path(system.file(package=\"safeHavens\"), 'extdata', 'Predictors.tif') ) rr <- RescaleRasters( # you may have already done this!   model = sdModel$Model,   predictors = sdModel$Predictors,    training_data = sdModel$TrainData,    pred_mat = sdModel$PredictMatrix)  # create a directory to hold the results from EBS real quick.  # we will default to placing it in your current working directory.  # If you are a data management freak don't worry too much about this.  # The code to remove the directory will be provided below.  getwd() # this is where the folder is going to located IF YOU DON'T RUN the code below.  #> [1] \"/home/runner/work/safeHavens/safeHavens/vignettes\" p <- file.path('~', 'Documents') # in my case I'll dump it in Documents real quick, this should work on  # Linux and Mac, but I don't think Windows?  # dir.create(file.path(p, 'safeHavens-Vignette')) # now create the directory.   ENVIbs <- EnvironmentalBasedSample(   pred_rescale = rr$RescaledPredictors,    write2disk = FALSE,    path = file.path(p, 'safeHavens-Vignette'), # we are not writing, but showing how to provide argument   taxon = 'Bradypus_test',    f_rasts = sdm, n = 20,    lyr = 'Supplemented',   fixedClusters = TRUE,    n_pts = 500,    planar_proj = planar_proj,   buffer_d = 3, prop_split = 0.8) #> Joining with `by = join_by(x, y)` #> Warning in st_point_on_surface.sfc(st_geometry(x)): st_point_on_surface may not #> give correct results for longitude/latitude data  ENVIbs.p <- map +    geom_sf(data = ENVIbs[['Geometry']], aes(fill = factor(ID))) +    #geom_sf_label(data = ENVIbs, aes(label = ID), alpha = 0.4) +    labs(title = 'Environmental') +    coord_sf(expand = FALSE) #> Coordinate system already present. #> ℹ Adding new coordinate system, which will replace the existing one.  ENVIbs.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"comparision-of-different-sampling-schemes","dir":"Articles","previous_headings":"","what":"Comparision of different sampling schemes","title":"Getting Started","text":", ’ve made got maps look ! look relatively similar plotted one another, let’s plot simultaneously see ’s still case.  , top three figures appear quite similar, Opportunistic method deviating slightly form . mind isolation distance (IBD) show biggest different, seems made sense naturally occurring patchiness species range. Environmental also seems partition feature space quite well. Notably drawing couple clusters Pacific lowlands Northern Andes mountains.","code":"pbs.p + eas.p + os.p  +  ibdbs.p + ebs.p + ENVIbs.p +    plot_layout(ncol = 3)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"prepare-data","dir":"Articles","previous_headings":"","what":"prepare data","title":"Rare Species Sampling Schema","text":"Load required packages. use Bradypus data included dismo package . create base map used GettingStarted. functions package handle sf objects directly, function actually just use simple data frame sites, simplfy handing data C++ optimization routines. input maximizeDispersion function list two elements: distance matrix, data frame site locations attributes. data frame must contain following columns. second required element, distance matrix, can calculated greatCircleDistance function package. Please use rather st_distance sf consistency, units differ slightly. want use sf::st_distance, make sure convert units match scale greatCircleDistance function, otherwise results incorrect. optimization routine requires least one ‘required’ site specified. select site closest geographic center sites required site. Normally can refer existing accessions, administrative units, preserves helping implement germplasm collection, fortunate enough already samples least guaranteed access. function bootstraps sites simulate true distribution distribution species, also bootstraps coordinate uncertainty site. randomly assign 20% sites coordinate uncertainty 1 km 40 km. Note argument always meters.","code":"#remotes::install_github('sagesteppe/safeHavens') library(safeHavens) library(ggplot2) library(patchwork) set.seed(99) x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326) #> Warning: attribute variables are assumed to be spatially constant throughout #> all geometries n_sites <- nrow(x)  df <- data.frame(   site_id = seq_len(n_sites),   required = FALSE,   coord_uncertainty = 0,    lon = sf::st_coordinates(x)[,1],    lat = sf::st_coordinates(x)[,2] )  head(df) #>   site_id required coord_uncertainty      lon      lat #> 1       1    FALSE                 0 -65.4000 -10.3833 #> 2       2    FALSE                 0 -65.3833 -10.3833 #> 3       3    FALSE                 0 -65.1333 -16.8000 #> 4       4    FALSE                 0 -63.6667 -17.4500 #> 5       5    FALSE                 0 -63.8500 -17.4000 #> 6       6    FALSE                 0 -64.4167 -16.0000 dist_mat <- sapply(1:nrow(df), function(i) {    greatCircleDistance(      df$lat[i], df$lon[i],      df$lat, df$lon    )  }) dists2c <- greatCircleDistance(   median(df$lat),    median(df$lon),    df$lat,    df$lon ) df[order(dists2c)[1],'required'] <- TRUE uncertain_sites <- sample(   setdiff(seq_len(n_sites),    which(df$required)),    size = round(n_sites*0.2, 0)   ) df$coord_uncertainty[uncertain_sites] <- runif(length(uncertain_sites), 1000, 40000) # meters"},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"run-kmedoidsbasedsample-based-only-on-geographic-distances","dir":"Articles","previous_headings":"","what":"Run KMedoidsBasedSample based only on geographic distances","title":"Rare Species Sampling Schema","text":"input function distance matrix, site data. funtion KMedoidsBasedSample several parameters control run parameters. function operates relatively quick bootstraps sites, take smidge time longer complex scenarios. recommened using least 999 bootstraps real world applications.","code":"test_data <- list(   distances = dist_mat,   sites = df   )  str(test_data) #> List of 2 #>  $ distances: num [1:116, 1:116] 0 1.83 714.09 807.71 797.93 ... #>  $ sites    :'data.frame':   116 obs. of  5 variables: #>   ..$ site_id          : int [1:116] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ required         : logi [1:116] FALSE FALSE FALSE FALSE FALSE FALSE ... #>   ..$ coord_uncertainty: num [1:116] 0 0 0 0 0 ... #>   ..$ lon              : num [1:116] -65.4 -65.4 -65.1 -63.7 -63.9 ... #>   ..$ lat              : num [1:116] -10.4 -10.4 -16.8 -17.4 -17.4 ...  rm(x, n_sites, uncertain_sites, dists2c) st <- system.time( {     geo_res <- KMedoidsBasedSample(  ## reduce some parameters for faster run.        input_data = test_data,       n = 5,       n_bootstrap = 10,       dropout_prob = 0.1,       n_local_search_iter = 10,       n_restarts = 2     )   } ) #> Sites: 116 | Seeds: 1 | Requested: 5 | Coord. Uncertain: 19 | BS Replicates: 10 #>   |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |==============                                                        |  20%  |                                                                              |=====================                                                 |  30%  |                                                                              |============================                                          |  40%  |                                                                              |===================================                                   |  50%  |                                                                              |==========================================                            |  60%  |                                                                              |=================================================                     |  70%  |                                                                              |========================================================              |  80%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100% st #>    user  system elapsed  #>  27.210   0.018  27.231 rm(st)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"return-output-structure","dir":"Articles","previous_headings":"Run KMedoidsBasedSample based only on geographic distances","what":"return output structure","title":"Rare Species Sampling Schema","text":"Various elements returned output list. stability score shows often frquently selected network sites selected bootstrapped runs. stability data frame shows often site selected across bootstrap runs. Many users may find combindation input data columns, need carry results. Run parameters saved settings element.","code":"str(geo_res) #> List of 5 #>  $ input_data     :'data.frame': 116 obs. of  9 variables: #>   ..$ site_id          : int [1:116] 47 21 5 83 100 6 106 19 95 86 ... #>   ..$ required         : logi [1:116] TRUE FALSE FALSE FALSE FALSE FALSE ... #>   ..$ coord_uncertainty: num [1:116] 0 0 0 37284 13617 ... #>   ..$ lon              : num [1:116] -74.3 -55.1 -63.9 -79.8 -74.1 ... #>   ..$ lat              : num [1:116] 4.58 -2.83 -17.4 9.17 -2.37 ... #>   ..$ cooccur_strength : num [1:116] 40 28 24 20 20 16 16 12 12 8 ... #>   ..$ is_seed          : logi [1:116] TRUE FALSE FALSE FALSE FALSE FALSE ... #>   ..$ selected         : logi [1:116] TRUE TRUE FALSE TRUE FALSE TRUE ... #>   ..$ sample_rank      : int [1:116] 1 2 3 4 4 5 5 6 6 7 ... #>  $ selected_sites : int [1:5] 6 21 47 83 106 #>  $ stability_score: num 0.2 #>  $ stability      :'data.frame': 116 obs. of  3 variables: #>   ..$ site_id         : int [1:116] 47 21 5 83 100 6 106 19 95 86 ... #>   ..$ cooccur_strength: num [1:116] 40 28 24 20 20 16 16 12 12 8 ... #>   ..$ is_seed         : logi [1:116] TRUE FALSE FALSE FALSE FALSE FALSE ... #>  $ settings       :'data.frame': 1 obs. of  4 variables: #>   ..$ n_sites     : num 5 #>   ..$ n_bootstrap : num 10 #>   ..$ dropout_prob: num 0.1 #>   ..$ n_uncertain : int 19 head(geo_res$stability_score) #> [1] 0.2 head(geo_res$stability) #>     site_id cooccur_strength is_seed #> 47       47               40    TRUE #> 21       21               28   FALSE #> 5         5               24   FALSE #> 83       83               20   FALSE #> 100     100               20   FALSE #> 6         6               16   FALSE head(geo_res$input_data) #>     site_id required coord_uncertainty      lon      lat cooccur_strength #> 47       47     TRUE              0.00 -74.3000   4.5833               40 #> 21       21    FALSE              0.00 -55.1333  -2.8333               28 #> 5         5    FALSE              0.00 -63.8500 -17.4000               24 #> 83       83    FALSE          37283.66 -79.8167   9.1667               20 #> 100     100    FALSE          13616.63 -74.0833  -2.3667               20 #> 6         6    FALSE              0.00 -64.4167 -16.0000               16 #>     is_seed selected sample_rank #> 47     TRUE     TRUE           1 #> 21    FALSE     TRUE           2 #> 5     FALSE    FALSE           3 #> 83    FALSE     TRUE           4 #> 100   FALSE    FALSE           4 #> 6     FALSE     TRUE           5 head(geo_res$settings) #>   n_sites n_bootstrap dropout_prob n_uncertain #> 1       5          10          0.1          19"},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"visualize-the-selection-results","dir":"Articles","previous_headings":"Run KMedoidsBasedSample based only on geographic distances","what":"visualize the selection results","title":"Rare Species Sampling Schema","text":"","code":"map +    geom_point(data = geo_res$input_data,    aes(     x = lon,      y = lat,      shape = required,      size = cooccur_strength,     color = selected     )   ) +  # ggrepel::geom_label_repel(aes(label = site_id), size = 4) +    theme_minimal() +    labs(title = 'Priority Selection Status of Sites; Geographic Distances') map +    geom_point(data = geo_res$input_data,      aes(       x = lon,        y = lat,        shape = required,        size = -sample_rank,       color = sample_rank       )     ) +  # ggrepel::geom_label_repel(aes(label = sample_rank), size = 4) +   theme_minimal()"},{"path":[]},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"extract-prep-environmental-distances","dir":"Articles","previous_headings":"run KMedoidsBasedSample with environmental distances","what":"extract prep environmental distances","title":"Rare Species Sampling Schema","text":"environmental distances, use PCA transformation environmental variables. simply scrape 100 random points raster layers calculate PCA. predict PCA raster layers across entire study area. take first two layers, calculate environmental distances based two layers.  keep first two PCA layers environmental distance calculation. layers increase dimenstionality, may lead less useful results. Note ’s fine use euclidean distance calculation , values truly position plot.","code":"files <- list.files(   path = file.path(system.file(package=\"dismo\"), 'ex'),    pattern = 'grd',  full.names=TRUE ) predictors <- terra::rast(files) # import the independent variables rm(files) pts <- terra::spatSample(predictors, 100, na.rm = TRUE) pts <- pts[, names(pts)!='biome' ] # remove categorical variable for distance calc  pca_results <- stats::prcomp(pts, scale = TRUE) round(pca_results$sdev^2 / sum(pca_results$sdev^2), 2) # variance explained #> [1] 0.58 0.26 0.10 0.04 0.02 0.00 0.00 0.00 pca_raster <- terra::predict(predictors, pca_results)  terra::plot(terra::subset(pca_raster, c(1:2))) # prediction of the pca onto a new raster rm(pts, predictors, pca_results) env_values <- terra::extract(pca_raster,    sf::st_coordinates(     sf::st_as_sf(       df,        coords = c('lon', 'lat'),        crs = 4326     )   ) )[,1:2] plot(env_values, main = 'environmental distance of points from first two PCA axis') env_dist_mat <- as.matrix(     dist(env_values)   )  rm(pca_raster) test_data <- list(   distances = env_dist_mat,   sites = df   )  st <- system.time(    {     env_res <- KMedoidsBasedSample(  ## reduce some parameters for shorter run time.       input_data = test_data,       n = 5,       n_bootstrap = 10,       dropout_prob = 0.1,       n_local_search_iter = 50,       n_restarts = 2     )   } ) #> Sites: 116 | Seeds: 1 | Requested: 5 | Coord. Uncertain: 19 | BS Replicates: 10 #>   |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |==============                                                        |  20%  |                                                                              |=====================                                                 |  30%  |                                                                              |============================                                          |  40%  |                                                                              |===================================                                   |  50%  |                                                                              |==========================================                            |  60%  |                                                                              |=================================================                     |  70%  |                                                                              |========================================================              |  80%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%  rm(dist_mat, env_dist_mat) st #>    user  system elapsed  #>  35.130   0.012  35.145 rm(st) head(env_res$stability_score) #> [1] 0.2 map +    geom_point(data = env_res$input_data,      aes(       x = lon,        y = lat,        shape = required,        size = cooccur_strength,       color = selected       )     ) +  # ggrepel::geom_label_repel(aes(label = site_id), size = 4) +    theme_minimal() +    labs(title = 'Priority Selection Status of Sites; Environmental')"},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"alternative-methods-for-required-central-points","dir":"Articles","previous_headings":"","what":"alternative methods for required central points","title":"Rare Species Sampling Schema","text":"example use point median geographic center populations. can also identify population near highest density populations. Intuitively, suggested population high genetic diversity. Likewise can identify population near ‘center’ environmental variable space. Personally consider ‘pop centered’ population important required site center design .","code":"dens <- with(df, MASS::kde2d(lon, lat, n = 200)) max_idx <- which(dens$z == max(dens$z), arr.ind = TRUE)[1,] max_point <- c(dens$x[max_idx[1]], dens$y[max_idx[2]])  pops_centre <- sweep(df[c('lon', 'lat')], 2, max_point, \"-\") pop_centered_id <- which.min(rowSums(abs(pops_centre^2)))  rm(dens, max_idx, max_point, pops_centre) env_centered <- sweep(env_values, 2, sapply(env_values, median), \"-\") env_centered_id <- which.min(rowSums(abs(env_centered^2)))  rm(env_values) # geographic centroid was pt 47 centers <- df[ c(env_centered_id, pop_centered_id, 47), ]  centers$type <- c('Environmental', 'Population', 'Geographic')  map +   geom_point(     data = df,      aes(x = lon, y = lat)     ) +    geom_point(     data = centers,       aes(x = lon, y = lat),     col = '#FF1493', size = 4     ) +    ggrepel::geom_label_repel(     data = centers,      aes(label = type, x = lon, y = lat)     ) +    theme_minimal() +    labs(title = 'Possbilities for centers') rm(env_centered_id, env_centered, pop_centered_id)"},{"path":[]},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"background","dir":"Articles","previous_headings":"Species Distribution Modelling","what":"Background","title":"Species Distribution Models","text":"vignette details steps required create Species Distribution Model (SDM) using functions provided safeHavens. SDM required use EnvironmentalBasedSample function, complex sampling scheme provided package. SDM created using elastic net generalized linear model implemented via glmnet package. SDM post-processed create binary raster map suitable unsuitable habitat, used rescale environmental predictor variables according contributions model. rescaled variables used clustering algorithm partition species range environmentally distinct regions germplasm sampling. goal SDM’s create model accurately predicts species located environmental space can predicted geographic space. goal models understand degree direction various environmental features correlate species observed range. model intended replace well-structured thought-SDM; rather, intended provide quick model can used inform sampling strategies.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"about","dir":"Articles","previous_headings":"Species Distribution Modelling","what":"About","title":"Species Distribution Models","text":"authors prone creating many ‘advanced’ SDMs models -house pipelines developing , think juice worth squeeze rely powerhouse R packages heavy lifting. main packages used dismo, caret, glmnet, CAST, terra. dismo package provides user-friendly interface working species occurrence raster data, well helper functions thresholding evaluating models. caret package provides nice interface working machine learning models, glmnet package provides elastic net model. CAST package provides functions spatially informed cross-validation, important SDMs. terra package provides functions working raster data. packages well documented, maintained.","code":""},{"path":[]},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"prep-data","dir":"Articles","previous_headings":"Steps to create an SDM","what":"prep data","title":"Species Distribution Models","text":"First need georeference occurrence data species interest. common species recommened rgbif package download occurrence data GBIF. maintain smaller package size, integrate tutorial dismo use built data set Bradypus variegatus (Brown-throated sloth). also leverage raster data provided dismo well.","code":"library(safeHavens) x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)  planar_proj <- 3857 # Web Mercator for planar distance calcs  files <- list.files(   path = file.path(system.file(package=\"dismo\"), 'ex'),    pattern = 'grd',  full.names=TRUE ) predictors <- terra::rast(files) # import the independent variables"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"fit-the-model","dir":"Articles","previous_headings":"Steps to create an SDM","what":"fit the model","title":"Species Distribution Models","text":"fit SDM using elasticSDM. arguments requires occurrence data x, raster stack predictors, quantile_v offset used create pseudo-absence data, planar_proj used calculate distances occurrence data possible pseudo-absence points. quantile_v used create pseudo-absence data calculating distance occurrence point nearest neighbor, selecting quantile distances create buffer around occurrence point. Points outside buffer used pseudo-absences. hood function uses caret help glmnet, provides output easy explore interact . Elastic net modelS bridge world lasso ridge regression blending alpha parameter. Lasso alpha 0, Ridge alpha 1. Lasso regression perform automated variable selection, can drop (‘shrink’) features model, whereas Ridge keep variables correlated features exist split contributions . elastic net blends propensity drop retain variables whenever used. caret test range alphas determine much ridge lasso regression characteristics best data hand. MAKE NOTE PCNM MORAN EIGNENVECTORS .","code":"sdModel <- elasticSDM(   x = x,   predictors = predictors,   quantile_v = 0.025,   planar_proj = planar_proj   ) #> Warning:  #> Grid searches over lambda (nugget and sill variances) with  minima at the endpoints:  #>   (GCV) Generalized Cross-Validation  #>    minimum at  right endpoint  lambda  =  1.656291e-07 (eff. df= 174.8 )"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"explore-the-output","dir":"Articles","previous_headings":"Steps to create an SDM > fit the model","what":"explore the output","title":"Species Distribution Models","text":"can see elastic net decided top model. used Accuracy rather Kappa main criterion model selection. can see selected model works predicting state test data. Note accuracy results slightly higher CV folds. bug, CV folds testing holdouts, brand new set holdouts. main reason confusion matrix results likely higher due spatial auto-correlation address typical ‘random split’ test train data. order minimize effects spatial-autocorrelation model use CAST hood allows spatially informed cross validation. Consider output CVStructure bit realistic.","code":"sdModel$CVStructure #> glmnet  #>  #> 184 samples #>   6 predictor #>   2 classes: '0', '1'  #>  #> No pre-processing #> Resampling: Bootstrapped (25 reps)  #> Summary of sample sizes: 184, 184, 184, 184, 184, 184, ...  #> Resampling results across tuning parameters: #>  #>   alpha  lambda        Accuracy   Kappa     #>   0.10   0.0006204077  0.8360016  0.6712790 #>   0.10   0.0062040769  0.8282214  0.6560044 #>   0.10   0.0620407692  0.8279379  0.6559269 #>   0.55   0.0006204077  0.8365349  0.6722483 #>   0.55   0.0062040769  0.8287096  0.6571131 #>   0.55   0.0620407692  0.8182452  0.6371623 #>   1.00   0.0006204077  0.8376016  0.6743248 #>   1.00   0.0062040769  0.8293442  0.6582937 #>   1.00   0.0620407692  0.8107098  0.6222415 #>  #> Accuracy was used to select the optimal model using the largest value. #> The final values used for the model were alpha = 1 and lambda = 0.0006204077.  coef(sdModel$Model, s = \"lambda.min\") #> 7 x 1 sparse Matrix of class \"dgCMatrix\" #>              s=lambda.min #> (Intercept)  -4.138796551 #> bio17        -0.007649011 #> biome        -0.176893842 #> bio1          0.023751253 #> bio8         -0.005606085 #> bio12         0.001275749 #> PCNM1       -18.787653011 sdModel$ConfusionMatrix #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1 #>          0 16  0 #>          1  7 22 #>                                            #>                Accuracy : 0.8444           #>                  95% CI : (0.7054, 0.9351) #>     No Information Rate : 0.5111           #>     P-Value [Acc > NIR] : 3.102e-06        #>                                            #>                   Kappa : 0.6909           #>                                            #>  Mcnemar's Test P-Value : 0.02334          #>                                            #>             Sensitivity : 1.0000           #>             Specificity : 0.6957           #>          Pos Pred Value : 0.7586           #>          Neg Pred Value : 1.0000           #>              Prevalence : 0.4889           #>          Detection Rate : 0.4889           #>    Detection Prevalence : 0.6444           #>       Balanced Accuracy : 0.8478           #>                                            #>        'Positive' Class : 1                #>"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"binarize-the-output","dir":"Articles","previous_headings":"Steps to create an SDM > fit the model","what":"binarize the output","title":"Species Distribution Models","text":"SDM’s produce continuous surfaces displaying predicted probability suitable habitat across landscape. However binary surfaes (Yes/Suitable), .e. less probable suitable habitat taxon exists particular location (grid cell)? often required ; function PostProcessSDM used binarize predictions. Going continuous surface binary surface loses information. goal binary surface understand species likely found, sample germplasm areas. many ways threshold SDM, many caveats associated process; Frank Harrell written topic post-processing statistics. Historically, assessing probability output 0.5 probability used threshold, probabilities beneath considered ‘Suitable / ’, probabilities classified ‘Suitable / Yes’. works cases, thresholding outside domain statistics realm practice. motto implementing models, slight elaboration George Box’s aphorism, “models wrong, useful - want wrong?”. goal sampling germplasm conservation maximize representation allelic diversity across range species. achieve , need understanding species actual range, hence better predict species present , predict absent actually grows. Accordingly, default thresholding statistic Sensitivity. However, function supports threshold options dismo::threshold. may wondering function named PostProcessSDM rather ThresholdSDM, reason discontinuity function performs additional processes thresholding. Geographic buffers created around intitial occurence points ensure none known occurrence points ‘missing’ output binary map. two edged sword, address notion dispersal limitation, realize suitable habitat occupied habitat. PostProcessSDM function creates cross validation folds, selects training data. fold calculates distance occurrence point ’s nearest neighbor. summarize distances can understand distribution distances quantile. use selected quantile, serve buffer. Area predicted suitable habitat outside buffer become ‘cut ’ (masked) binary raster map, areas within buffer distance known occurrences currently masked reassigned probabilities. theory behind process underdeveloped nascent, come gut analyst. Bradypus data set use 0.25 quantile, saying “Neighbors generally 100km apart, happy risk saying 25km within occurrence occupied suitable habitat”. Increasing value say 1.0 mean suitable habitat removed, decreasing makes maps conservative. cost increasing distances greatly sampling methods may puts grids many areas without populations collect . can compare results applying function side side using output function.","code":"terra::plot(sdModel$RasterPredictions) threshold_rasts <- PostProcessSDM(   rast_cont = sdModel$RasterPredictions,    test = sdModel$TestData,   planar_proj = planar_proj,   thresh_metric = 'sensitivity',    quant_amt = 0.5   ) #> 1000 prediction points are sampled from the modeldomain terra::plot(threshold_rasts$FinalRasters)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"rescale-predictor-variables","dir":"Articles","previous_headings":"Steps to create an SDM > fit the model","what":"rescale predictor variables","title":"Species Distribution Models","text":"glmnet used three main reasons, 1) gives directional coefficients documents 1 unit increase independent variable varies response. 2) maintains automated feature selection reducing work analyst needs . 3) glmnet re-scales variables model generation combined beta-coefficients allows partitioning environmental space regions environmentally similar individual species. way raster stack becomes representative model, can use values basis hierarchical cluster later . create copy raster predictions standardized variable, equivalent input glmnet, mulitplied beta-coefficient. also write beta coefficients writeSDMresults function right afterwards.  can see variables ‘close’ scale, work clustering algorithm. layers color (maybe yellow?) means variance, ’s term shrunk model. dealt internally future function. scales variables exact, weighed coefficients model {r Show beta Coefficients model print(rr$BetaCoefficients) can also look coefficients variable. glmnet returns ‘untransformed’ variables, .e. coefficients scale input rasters, calculate BC right afterwards. safeHavens generates kinds things runs functions elasticSDM, PostProcessSDM, RescaleRasters. Given one sampling scheme may followed quite time, best practice save many objects.","code":"rr <- RescaleRasters(   model = sdModel$Model,   predictors = sdModel$Predictors,    training_data = sdModel$TrainData,    pred_mat = sdModel$PredictMatrix)  terra::plot(rr$RescaledPredictors)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"save-results","dir":"Articles","previous_headings":"Steps to create an SDM > fit the model","what":"save results","title":"Species Distribution Models","text":"","code":"bp <- '~/Documents/assoRted/StrategizingGermplasmCollections'  writeSDMresults(   cv_model = sdModel$CVStructure,    pcnm = sdModel$PCNM,    model = sdModel$Model,    cm = sdModel$ConfusionMatrix,    coef_tab = rr$BetaCoefficients,    f_rasts = threshold_rasts$FinalRasters,   thresh = threshold_rasts$Threshold,   file.path(bp, 'results', 'SDM'), 'Bradypus_test')  # we can see that the files were placed here using this.  list.files( file.path(bp, 'results', 'SDM'), recursive = TRUE )"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"wrapping-up","dir":"Articles","previous_headings":"Steps to create an SDM > fit the model","what":"wrapping up","title":"Species Distribution Models","text":", steps make species distribution model - rather get coefficients species distribution model! play around example data set compare buffered distance results end - head next vignette!","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/WorkedExample.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Worked Example","text":"previous tutorials focus individual functions package, little show integrate common workflows help develop spatial products sampling. show minimal example curator may load occurrence data species, apply couple sampling approaches data, write results downstream use. use two species native Southwestern United States & Mexico example, eventually narrowing focus discuss one sake brevity.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/WorkedExample.html","id":"data-prep","dir":"Articles","previous_headings":"Introduction","what":"Data prep","title":"Worked Example","text":"need install rgbif follow along example. rgbif amazing package maintained ROpenSci acess Global Biodiversity Information Facility database within R. Note (free, esay get), GBIF profile required larger data downloads, example need R package. also show ‘quick’ approach altering buffer sizes (see ‘Getting Started’), want need install gstat sp geostatics focused packages dependencies safeHavens. function raw .Rmd vignette file, folded brevity. Using rgbif ’ll download occurrence data couple species, just code set mapping multiple species realistic manner. Take quick look data see clear errors. One points location incorrect, ’s latitude great - remove .  Map data , keep ggplot around basemap rest vignette.","code":"library(safeHavens) library(sf) # spatial data ## Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE library(dplyr) # general data handling ##  ## Attaching package: 'dplyr' ## The following objects are masked from 'package:stats': ##  ##     filter, lag ## The following objects are masked from 'package:base': ##  ##     intersect, setdiff, setequal, union library(tidyr) # general data handling library(purrr) # mapping functions across lists. library(ggplot2) ## for maps  library(rgbif) # species occurrence data. library(spData) # example cartography data ## To access larger datasets in this package, install the spDataLarge ## package with: `install.packages('spDataLarge', ## repos='https://nowosad.github.io/drat/', type='source')` ## optional packages library(gstat) library(sp) ## small subset of useful columns for example cols = c('decimalLatitude', 'decimalLongitude', 'dateIdentified', 'species', 'acceptedScientificName', 'datasetName',    'coordinateUncertaintyInMeters', 'basisOfRecord', 'institutionCode', 'catalogNumber')  ## download species data using scientificName, can use keys and lookup tables for automating many taxa.  cymu <- rgbif::occ_search(scientificName = \"Vesper multinervatus\", limit = 1000)  ### check to see what CRS are in here, these days usually standardized to wgs84 (epsg:4326) table( cymu[['data']]['geodeticDatum']) ## geodeticDatum ## WGS84  ##   676 ## subset the data to relevant columns  cymu_cols <- cymu[['data']][,cols]  ## and repeat this again so a second set of data are on hand  bowa <- rgbif::occ_search(scientificName = \"Bouteloua warnockii\", limit = 1000) bowa_cols <- bowa[['data']][,cols]  ## contrived multispecies example.  spp <- bind_rows(bowa_cols, cymu_cols) |>   drop_na(decimalLatitude, decimalLongitude) |> # any missing coords need dropped.    st_as_sf(coords = c( 'decimalLongitude', 'decimalLatitude'), crs = 4326, remove = F)  rm(cymu, bowa, cols, cymu_cols, bowa_cols) western_states <- spData::us_states |> ## for making a quick basemap.    dplyr::filter(REGION == 'West' & ! NAME %in% c('Montana', 'Washington', 'Idaho', 'Oregon', 'Wyoming') |    NAME %in% c('Oklahoma', 'Texas', 'Kansas')) |>   dplyr::select(NAME, geometry) |>   st_transform(4326)  ggplot() +   geom_sf(data = western_states) +   geom_sf(data = spp, aes(color = species, shape = species)) +   theme_void() +   theme(legend.position = 'bottom') ## check the outlying record.  arrange(spp, by = decimalLatitude, desc=FALSE) |>   head(5) ## Simple feature collection with 5 features and 10 fields ## Geometry type: POINT ## Dimension:     XY ## Bounding box:  xmin: -102.8461 ymin: 26.2436 xmax: -101.343 ymax: 26.30833 ## Geodetic CRS:  WGS 84 ## # A tibble: 5 × 11 ##   decimalLatitude decimalLongitude dateIdentified species acceptedScientificName ##             <dbl>            <dbl> <chr>          <chr>   <chr>                  ## 1            26.2            -103. NA             Boutel… Bouteloua warnockii G… ## 2            26.2            -103. NA             Boutel… Bouteloua warnockii G… ## 3            26.2            -103. NA             Boutel… Bouteloua warnockii G… ## 4            26.3            -101. NA             Boutel… Bouteloua warnockii G… ## 5            26.3            -101. NA             Boutel… Bouteloua warnockii G… ## # ℹ 6 more variables: datasetName <chr>, coordinateUncertaintyInMeters <dbl>, ## #   basisOfRecord <chr>, institutionCode <chr>, catalogNumber <chr>, ## #   geometry <POINT [°]> ## remove it based on it's latitude.  spp <- filter(spp, decimalLatitude <= 40) bb <- st_transform(spp, 5070) |>   st_buffer(100000) |>   st_transform(4326) |>   st_bbox()  western_states <- st_crop(western_states, bb) ## Warning: attribute variables are assumed to be spatially constant throughout ## all geometries base <- ggplot() +   geom_sf(data = western_states, color = 'white') +   geom_sf(data = spp, aes(color = species, fill = species)) +   theme_void() +   theme(legend.position = 'bottom')  base rm(western_states)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/WorkedExample.html","id":"using-safehavens","dir":"Articles","previous_headings":"Introduction","what":"using safeHavens","title":"Worked Example","text":"Now use safeHavens functionality set-environment data GBIF.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/WorkedExample.html","id":"create-species-ranges","dir":"Articles","previous_headings":"Introduction > using safeHavens","what":"Create species ranges","title":"Worked Example","text":"showcase two main methods generating species range geometries, used packages functions. rely st_concave_hull function sf. st_concave_hull, ratio parameter can control ELASTICITY? hulls, ratio = 1 creating convex hull (akin st_convex_hull function, also sf), ratio = 0.0, creating true concave hull. Results 0.4 look overly reduced species . Visualize concave hulls.  Visualize convex hulls.  sake example use concave ranges going forward.","code":"sppL <- split(spp, f = spp$species)  concavities <- function(x, d, rat){    species <- x[['species']][1]   out <- st_transform(x, 5070) |>     st_buffer(dist = d) |>     st_union() |>      st_concave_hull(ratio = rat) |>      st_sf() |>     rename(geometry = 1) |>     mutate(species, .before = geometry)  }  spp_concave <- sppL |>   purrr::map(~ concavities(.x, d = 20000, rat = 0.4)) spp_convex <- sppL |>   purrr::map(~ concavities(.x, d = 20000, rat = 1.0))  rm(concavities) base +   geom_sf(data = spp_concave[[1]], aes(fill = species), alpha = 0.2) +   geom_sf(data = spp_concave[[2]],  aes(fill = species), alpha = 0.2) base +   geom_sf(data = spp_convex[[1]],  aes(fill = species), alpha = 0.2) +   geom_sf(data = spp_convex[[2]], aes(fill = species), alpha = 0.2) rm(spp_convex)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/WorkedExample.html","id":"perform-sampling-for-the-species-ranges","dir":"Articles","previous_headings":"Introduction","what":"Perform sampling for the species ranges","title":"Worked Example","text":"First perform equal area sampling relatively low amount points repetitions.  plot equal area sampling shows … perform isolation distance (IBD) based sampling.  plot isolation--distance sampling shows individual sample areas.","code":"eas <- spp_concave |>   purrr::map(~ EqualAreaSample(.x, n = 10, pts = 250, planar_proj = 5070, reps = 25))  base +   geom_sf(data = eas[[2]][['Geometry']], aes(fill = factor(ID)), alpha = 0.2) # create an arbitrary template for example - best to do this over your real range of all species collections # so that it can be recycled across species.  template <- terra::rast(terra::ext(bb), crs = terra::crs(spp), resolution = c(0.1, 0.1)) terra::values(template) <- 0  # the species range now gets 'burned' into the raster template.  spp_concave <- purrr::map(spp_concave, \\(x) {   st_as_sf(x) |>      mutate(Range = 1, .before = geometry) |>     st_transform(4326) |>      terra::rasterize(template, field = 'Range')  })  ## the actual sampling happens here within `map` ibd_samples <- spp_concave |>   purrr::map(~ IBDBasedSample(.x, n = 10, fixedClusters = FALSE, template = template, planar_proj = 5070)) ## Loading required package: lattice ##  ## Attaching package: 'caret' ## The following object is masked from 'package:purrr': ##  ##     lift base + ## visualize for a single taxon.    geom_sf(data = ibd_samples[[1]][['Geometry']], aes(fill = factor(ID)), alpha = 0.2) rm(spp_concave)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/WorkedExample.html","id":"prioritize-sample-areas","dir":"Articles","previous_headings":"Introduction > Perform sampling for the species ranges","what":"prioritize sample areas","title":"Worked Example","text":"addition creating spatial geometries can used guide sampling efforts, safeHavens can also help provide visualize guidance general areas germplasm can sampled try maximize distance samples. Please note safeHavens can offer suggestions sample, prioritized suggestions may align reality - consider rules thumbs! made, couple hundred large native seed collections, coordinated couple hundred , many years - argue beggars choosers, basing deliverable metrics purely around data like can difficult - degree reporting autonomy must maintained. function PrioritizeSample offers two levels prioritization. coarse level suggest relative order sample areas sampled , 1:n; function seeks minimize variance ( related measure) sample samples collected. effect tries stratify sampling across species range. second part function almost ‘heamap’ can used show concentric rings around geographic center sample unit. Keeping number rings low allows pragmatic communication desirable/less desirable portions range ideally collect .  map shows possible general order guide prioritization individual sample areas, simplified visuals within sample areas attempt target.","code":"ibd_samples_priority <- ibd_samples %>%   purrr::map(~ st_transform(.x$Geometry, 5070)) |>    purrr::map(~ list(PrioritizeSample(.x, n_breaks = 3)))  base +    geom_sf(data = ibd_samples_priority[[2]][[1]][['Geometry']], aes(fill = factor(Level)), alpha = 0.2) rm(bb)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/WorkedExample.html","id":"wrapping-up","dir":"Articles","previous_headings":"Introduction > Perform sampling for the species ranges","what":"wrapping up","title":"Worked Example","text":"Upon completion runs results species can written individual geopackages long term storage. strong benefit geopackage ’s ability hold multiple geometry types (polygons, points, rasters, etc.), can sometimes lost kept separate directories. Write data long term storage. Clean environment","code":"## create a directory to hold the outputs p2Collections <- file.path('~', 'Documents', 'WorkedExample_Output') dir.create(p2Collections, showWarnings = FALSE)  ## write out the raster template for future use.  dir.create(file.path(p2Collections, 'IBD_raster_template'), showWarnings = FALSE)  ## to save template cells need to have values...  terra::writeRaster(template,    filename = file.path(p2Collections, 'IBD_raster_template', 'IBD_template.tif'), overwrite = FALSE)  ## save each species as geopackage.  for(i in seq_along(sppL)){    fp = file.path(p2Collections, paste0(gsub(' ', '_', sppL[[i]]$species[1]), '.gpkg'))    ### GBIF occurrence points   st_write(sppL[[i]], dsn = fp, layer =  'occurrence_points', quiet = TRUE)    ### results of equal area sampling     st_write(eas[[i]]$Geometry,  dsn = fp, layer =  'equal_area_samples', quiet = TRUE, append = TRUE)    ### ibd based sample (note can be reconstruced from the hulls of ibd samples priority)   st_write(ibd_samples[[i]]$Geometry, dsn = fp, layer =  'ibd_samples', quiet = TRUE, append = TRUE)    ### prioritized information for the IBD samples.    st_write(ibd_samples_priority[[i]][[1]]$Geometry, dsn = fp, layer =  'ibd_sampling_priority', quiet = TRUE, append = TRUE)    message(format(object.size(fp), standard = \"IEC\", units = \"MiB\", digits = 4)) }  rm(p2Collections, fp, i) rm(spp, sppL, eas, ibd_samples, ibd_samples_priority, base, vario_estimate, template) #rm(vario_estimate, buffered)"},{"path":"https://sagesteppe.github.io/safeHavens/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Reed Benkendorf. Author, maintainer.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Benkendorf R (2026). safeHavens: Developing sampling schemas ex situ conservation plant germplasm. R package version 0.0.0.9000, https://sagesteppe.github.io/safeHavens/.","code":"@Manual{,   title = {safeHavens: Developing sampling schemas for ex situ conservation of plant germplasm},   author = {Reed Benkendorf},   year = {2026},   note = {R package version 0.0.0.9000},   url = {https://sagesteppe.github.io/safeHavens/}, }"},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"safehavens-","dir":"","previous_headings":"","what":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","title":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","text":"goal package provide germplasm curators easily referable spatial data sets help prioritize field collection efforts.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","text":"provides functionality seven sampling schemes various curators interested , many likely outperform others certain species areas. package also creates species distribution models, goal germplasm sampling, rather predicting ranges fine resolutions, making inference; interested functionality R several dozen packages tailored purposes.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"description","dir":"","previous_headings":"","what":"Description","title":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","text":"package helps germplasm curators communicate areas interest collection teams collect new material accession. provides seven different sampling approaches curators choose individual taxon hope process.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","text":"safeHavens available github. can installed using remotes devtools like : installed can attached use like package github CRAN","code":"install.packages('devtools') devtools::install_github('sagesteppe/safeHavens')  install.packages('remotes') # remotes is very similar and  a good alternativ for this use case. remotes::install_github('sagesteppe/safeHavens') library(safeHavens)"},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","text":"safeHavens seven user facing functions generating sampling schemes.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"available-sampling-schemes","dir":"","previous_headings":"Usage","what":"Available Sampling Schemes","title":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","text":"following table shows eight sampling approaches available safeHavens, computational complexity (Comp.) environmental data requirements (Envi.): L = Low, M = Medium, H = High. species distribution modelling section couple functions essential achieving EnvironmentalBasedSample design, : elasticSDM, PostProcessSDM, RescaleRasters writeSDMresults.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"acknowledgements","dir":"","previous_headings":"","what":"Acknowledgements","title":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","text":"Edzer Pebesma, Krzysztof Dyba help seamless installations Ubuntu, getting pkgdown website running.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"function utilizes output elastic net GLM model create weights matrix features relevant species distribution identify clusters throughout range incorporating PCNM/MEM data coordinates implement spatial contiguity.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"","code":"EnvironmentalBasedSample(   pred_rescale,   f_rasts,   lyr = \"Supplemented\",   taxon,   path = \".\",   n = 20,   fixedClusters = TRUE,   n_pts = 500,   planar_proj,   coord_wt = 2.5,   buffer_d = 3,   prop_split = 0.8,   write2disk = FALSE,   ... )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"pred_rescale rasterstack predictor layers rescaled represent beta coefficients elastic net (glmnet::glmnet) modelling process. See ?RescaleRasters implementation functionality. f_rasts rasters output SDM workflow. lyr Character. name layer want use analysis one : 'Threshold', 'Clipped', Supplemented'. missing defaults 'Supplemented'. taxon Character. name taxonomic entity models created. final raster clusters, results KNN classifier trainings, details clustering procedure (fixedClusters=TRUE). path root path output data saved, use WriteSDMresults. Defaults current working directory, check getwd(). n Numeric. number clusters desired. fixedClusters Boolean. Defaults TRUE, create n clusters. False use NbClust::NbClust determine number clusters. n_pts Numeric. number points use generating clusters, randomly sampled within mask area mask. Defaults 500. planar_proj Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. coord_wt Numeric. amount weigh coordinates distance matrix relative important variable identified elastic net regression. Defaults 2.5, setting 1 make value equivalent environmental PCNM variables. metric increases spatial contiguity clusters identified. buffer_d Numeric. using two-stage sampling increase sample size (number points) uncommon clusters, distance buffer individual pts located cells possible sampling. Defaults 3 allows area encompassing around 45-50 raster cells nearby possibly sampled. reasonable default coarsely gridded data (sensible grain type packages use cases), moderate resolution data (e.g. 250m-1km) may require higher values find meaningful differences variables locations. prop_split Numeric. proportion records used training KNN classifier. Defaults 0.8 use 80% records training 20% independent test sample. write2disk Boolean. Whether write results disk . Defaults FALSE. ... arguments passed NbClust::NbClust optimizing cluster numbers. Defaults using method = 'complete', compare results 20 methods select cluster number commonly generated algorithms. min.nc set default 5, overcome clusters use (easily overwritten supplying argument minc.nc=2, set lower), max.nc = 20 congruence many seed collection endeavors (overwritten max.nc = 10 example).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"Writes four objects disk, returns one object R session (optional).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Create equal area polygons over a geographic range — EqualAreaSample","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"function creates n geographic clusters geographic area (x), typically species range, using kmeans clustering.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"","code":"EqualAreaSample(   x,   n = 20,   pts = 5000,   planar_proj,   returnProjected,   reps = 100,   BS.reps = 9999 )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"x SF object terra spatraster. range generate clusters. n Numeric. number clusters desired. Defaults 20. pts Numeric. number points use generating clusters, placed grid like fashion across x. exact number points used may deviate slightly user submitted value allow equidistant spacing across x. Defaults 5,000. planar_proj Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. returnProjected Boolean. Whether return data set original input CRS (FALSE), new projection (TRUE). Defaults FALSE. reps Numeric. number times rerun voronoi algorithm, set polygons similar sizes, measured using variance areas selected. Defaults 100. BS.reps number bootstrap replicates evaluating results.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"list containing two objects, first results bootstrap simulations. second sf dataframe containing polygons smallest amount variance size.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"","code":"nc <- sf::st_read(system.file(\"shape/nc.shp\", package=\"sf\"), quiet = TRUE) |> dplyr::select(NAME)  set.seed(1) system.time(   zones <- EqualAreaSample(nc, n = 20, pts = 500, planar_proj = 32617, reps = 50) ) #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #>    user  system elapsed  #>   5.201   0.018   5.222   plot(nc, main = 'Counties of North Carolina')  plot(zones$Geometry, main = 'Clusters')  zones$SummaryData #>                  Metric       Value #> 1     variance.observed  9507223610 #> 2        quantile.0.001  9565302650 #> 3             lwr.95.CI  9507223610 #> 4             upr.95.CI 10692510134 #> 5    Voronoi.reps.asked          50 #> 6 Voronoi.reps.received          50 #> 7               BS.reps        9999"},{"path":"https://sagesteppe.github.io/safeHavens/reference/GridBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","title":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","text":"function creates 20 grid cells geographic area (x), typically species range.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/GridBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","text":"","code":"GridBasedSample(x, planar_proj, gridDimensions)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/GridBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","text":"x SF object terra spatraster. range generate clusters. planar_proj Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. gridDimensions single row form ouput TestGridSizes optimal number grids generate.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/GridBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","text":"simple features (sf) object containing final grids saving computer. See vignette questions saving two main types spatial data models (vector - used , raster).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/GridBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","text":"","code":"if (FALSE)  # not ran to bypass CRAN check time limits. ~6 seconds to treat Rhode Island.  ri <- spData::us_states |>  dplyr::filter(NAME == 'Rhode Island') |>   sf::st_transform(32615)  sizeOptions <- TestGridSizes(ri) #> Error: object 'ri' not found head(sizeOptions) # in this case let's shoot for 33 and see what happens #> Error: object 'sizeOptions' not found sizeOptions <- sizeOptions[sizeOptions$Name == 'Original',] #> Error: object 'sizeOptions' not found  output <- GridBasedSample(ri, 5070, gridDimensions = sizeOptions) #> Error: object 'ri' not found plot(output$Geometry) #> Error: object 'output' not found  # \\dontrun{}"},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"Create n seed collection areas based distance geographic (great circle) distance points.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"","code":"IBDBasedSample(   x,   n,   fixedClusters = TRUE,   n_pts = 1000,   template,   prop_split = 0.8,   min.nc = 5,   max.nc = 20,   planar_proj )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"x Raster surface sample points within, e.g. output SDM$Supplemented. n Numeric. number clusters desired. fixedClusters Boolean. Defaults TRUE, create n clusters. False use NbClust::NbClust determine optimal number clusters. n_pts Numeric. number points use generating clusters, randomly sampled within mask area mask. Defaults 1000, generally allows enough points split KNN training. template Raster. raster file can used template plotting. prop_split Numeric. proportion records used training KNN classifier. Defaults 0.8 use 80% records training 20% independent test sample. min.nc Numeric. Minimum number clusters test fixedClusters=FALSE, defaults 5. max.nc Numeric. Maximum number clusters test fixedClusters=FALSE, defaults 20. planar_proj Numeric. Optional. planar projection use sf::st_point_on_surface ensuare valid spatial operations.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"simple features (sf) object containing final grids saving computer. See vignette questions saving two main types spatial data models (vector - used , raster).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"","code":"planar_proj <- \"+proj=laea +lat_0=-15 +lon_0=-60 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"  x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326) x_buff <- sf::st_transform(x, planar_proj) |>  sf::st_buffer(125000) |> # we are working in planar metric coordinates, we are  sf::st_as_sfc() |> # buffer by this many / 1000 kilometers.   sf::st_union()  files <- list.files( # note that for this process we need a raster rather than    path = file.path(system.file(package=\"dismo\"), 'ex'), # vector data to accomplish   pattern = 'grd',  full.names=TRUE ) # this we will 'rasterize' the vector using terra predictors <- terra::rast(files) # this can also be done using 'fasterize'. Whenever # we rasterize a product, we will need to provide a template raster that our vector # will inherit the cell size, coordinate system, etc. from   x_buff.sf <- sf::st_as_sf(x_buff) |>    dplyr::mutate(Range = 1) |>    sf::st_transform(terra::crs(predictors))  # and here we specify the field/column with our variable we want to become  # an attribute of our raster v <- terra::rasterize(x_buff.sf, predictors, field = 'Range')   # now we run the function demanding 20 areas to make accessions from,  ibdbs <- IBDBasedSample(x = v, n = 20, fixedClusters = TRUE,     template = predictors, planar_proj = planar_proj) #> Loading required package: ggplot2 #> Loading required package: lattice plot(ibdbs[['Geometry']])"},{"path":"https://sagesteppe.github.io/safeHavens/reference/KMedoidsBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Maximize Dispersion Site Selection — KMedoidsBasedSample","title":"Maximize Dispersion Site Selection — KMedoidsBasedSample","text":"function operates individual points - representing populations, rather drawing convex hulls polygons around emulate species range. designed rare species, individual populations relatively scarce, e.g. < 100, decent location data. perform bootstrap re-sampling better estimate true range extent species, well coordinate jittering better address geo-location quality. running n_bootstrap simulations identify individual networks sites (co-location) resilient perturbations, less affected data quality issues. arguments takes known locations populations, solve n priority collection sites. Along process also generate priority ranking sites, indicating naive possible order prioritizing collections; although opportunity never discard site. required input parameter column indicating whether site required. Required sites (1 - many < n_sites) serve fixed parameters optimization scenario greatly speed run time. can represent: existing collections, collections strong chance happenging due funding agency mechanism, otherwise single population closet geographic center species. Notably solve 'around' site, hence solves purely theoretical, linked pragmatic element. Theoretically one can substitute geographic distance matrix environmental distance matrix. However, function internally recalculate distances bootstrapped points. See vignette example creating quick environmental distance matrix using simple PCA bioclim variables.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/KMedoidsBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maximize Dispersion Site Selection — KMedoidsBasedSample","text":"","code":"KMedoidsBasedSample(   input_data,   n = 5,   n_bootstrap = 999,   dropout_prob = 0.1,   n_local_search_iter = 100,   n_restarts = 3,   verbose = TRUE,   distance_type = \"geographic\",   min_jitter_dist = 10000 )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/KMedoidsBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maximize Dispersion Site Selection — KMedoidsBasedSample","text":"input_data list two elements: 'distances' (distance matrix) 'sites' (data frame site metadata). n number sites want select priority collection. Note results return rank prioritization sites data. n_bootstrap Number bootstrap replicates perform. dropout_prob Probability dropping non-seed sites bootstrap replicate, give sites generally keep 0.2. Set 0 disable dropout. n_local_search_iter Number local search iterations per restart. n_restarts Number random restarts per bootstrap replicate. verbose Whether print progress information. print message run settings, progress bar bootstraps. distance_type Character. Defaults 'geographic', otherwise 'environmental'. geogra'phic coordinates uncertainty greater min_jitter_dist coordinate jittering performed. min_jitter_dist Minimum coordinate uncertainty (meters) initiate jittering site coordinates.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/KMedoidsBasedSample.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Maximize Dispersion Site Selection — KMedoidsBasedSample","text":"Select subset sites maximize spatial dispersion sites using k-medioids clustering.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/KMedoidsBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Maximize Dispersion Site Selection — KMedoidsBasedSample","text":"","code":"if (FALSE) { # \\dontrun{ library(ggplot2)   ### create sample data   n_sites <- 30 # number of known populations  df <- data.frame(    site_id = seq_len(n_sites),    lat = runif(n_sites, 25, 30), # play with these to see elongated results.     lon = runif(n_sites, -125, -120),    required = FALSE,    coord_uncertainty = 0  )  #function can accept a required point, here arbitrarily place near geographic center  dists2c <- greatCircleDistance(    median(df$lat),     median(df$lon),     df$lat,     df$lon  )  df[order(dists2c)[1],'required'] <- TRUE    ## we will simulate coordinate uncertainty on a number of sites.    uncertain_sites <- sample(setdiff(seq_len(n), which(df$required)), size = min(6, n_sites-3))  df$coord_uncertainty[uncertain_sites] <- runif(length(uncertain_sites), 5000, 100000) # meters    # the function can take up to take matrices. the first (required) is a geographic distance  # matrix. calculate this with the `greatCircleDistance` fn from the package for consistency.   # (it will be recalculated during simulations). `sf` gives results in slightly diff units.   dist_mat <- sapply(1:nrow(df), function(i) {    greatCircleDistance(      df$lat[i], df$lon[i],      df$lat, df$lon    )  })   # the input data is a list, the distance matrix, and the df of actual point locations.   head(df)   test_data <- list(distances = dist_mat, sites = df)  rm(dist_mat, df, n, uncertain_sites, dists2c)   # small quick run     system.time(       res <- maximizeDispersion(  ## reduce some parameters for faster run.         input_data = test_data,        n_bootstrap = 500,        n_local_search_iter = 50,        n_restarts = 2      )    )  ### first selected   ggplot(data = res$input_data,     aes(      x = lon,       y = lat,       shape = required,       size = cooccur_strength,      color = selected      )    ) +    geom_point() +   #  ggrepel::geom_label_repel(aes(label = site_id), size = 4) +     theme_minimal() +     labs(main = 'Priority Selection Status of Sites')   ### order of sampling priority ranking plot.  ggplot(data = res$input_data,     aes(      x = lon,       y = lat,       shape = required,       size = -sample_rank,      color = sample_rank      )    ) +    geom_point() +  #   ggrepel::geom_label_repel(aes(label = sample_rank), size = 4) +    theme_minimal()    } # }"},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Design additional collections around already existing collections — OpportunisticSample","title":"Design additional collections around already existing collections — OpportunisticSample","text":"function utilizes regular, nearly case existing collections, grid points develop sampling scheme n polygons.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Design additional collections around already existing collections — OpportunisticSample","text":"","code":"OpportunisticSample(polygon, n, collections, reps, BS.reps)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Design additional collections around already existing collections — OpportunisticSample","text":"polygon input sf polygon, .e. species range administrative unit, sampling desired. n Numeric. total number desired collections. Defaults 20. collections sf point geometry data set existing collections made. reps arguments passed np.boot BS.reps number bootstrap replicates evaluating results.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Design additional collections around already existing collections — OpportunisticSample","text":"list containing two sublists, first 'SummaryData' details number voronoi polygons generated, results bootstrap simulations. second 'Geometry', contains final spatial data products, can written end. See vignette questions saving two main types spatial data models (vector - used , raster).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Design additional collections around already existing collections — OpportunisticSample","text":"","code":"#' Design additional collections around already existing collections ri <- spData::us_states |>   dplyr::select(NAME) |>   dplyr::filter(NAME == 'Rhode Island') |>   sf::st_transform(32617) existing_collections <- sf::st_sample(ri, size = 5) |>   sf::st_as_sf() |>   dplyr::rename(geometry = x)  system.time(   out <- OpportunisticSample(polygon = ri, BS.reps=4999)  ) # set very low for example #>    user  system elapsed  #>   6.619   0.034   6.656  # the function is actually very fast; 150 voronoi reps, with 9999 BS should only take about # 7 seconds per species so not much concern on the speed end of things. ggplot2::ggplot() +    ggplot2::geom_sf(data = out$Geometry, ggplot2::aes(fill = ID)) +    ggplot2::geom_sf(data = existing_collections)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"function utilizes regular, nearly case existing collections, grid points develop sampling scheme n polygons.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"","code":"PointBasedSample(polygon, n = 20, collections, reps = 100, BS.reps = 9999)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"polygon input sf polygon, .e. species range administrative unit, sampling desired. n Numeric. total number desired collections. Defaults 20. collections sf point geometry data set existing collections made. reps arguments passed np.boot BS.reps number bootstrap replicates evaluating results.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"list containing two objects, first results bootstrap simulations. second sf dataframe containing polygons smallest amount variance size.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"","code":"#' Utilize a grid based stratified sample for drawing up polygons ri <- spData::us_states |>   dplyr::select(NAME) |>   dplyr::filter(NAME == 'Rhode Island') |>   sf::st_transform(32617)     system.time(   out <- PointBasedSample(polygon = ri, reps = 10, BS.reps = 10) # set very low for example  ) #>    user  system elapsed  #>   0.594   0.017   0.611  # the function is actually very fast; 150 voronoi reps, with 9999 BS should only take about # 2 seconds per species so not much concern on the speed end of things! head(out$SummaryData) #>                  Metric    Value #> 1     variance.observed 11155783 #> 2        quantile.0.001 11166690 #> 3             lwr.95.CI 11155783 #> 4             upr.95.CI 11188935 #> 5    Voronoi.reps.asked       10 #> 6 Voronoi.reps.received        8 plot(out$Geometry)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PolygonBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample spatial zones within a species range — PolygonBasedSample","title":"Sample spatial zones within a species range — PolygonBasedSample","text":"Intersect vector data file spatial zones (ecoregions, provisional seed transfer zones, spatial partitions) range focal taxon select n zones sample. fewer n zones exist, extra samples allocated using specified method. n zones exist, zones selected using specified method.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PolygonBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample spatial zones within a species range — PolygonBasedSample","text":"","code":"PolygonBasedSample(   x,   zones,   zone_key,   n = 20,   decrease_method = c(\"Largest\", \"Smallest\", \"Most\", \"Assist-warm\", \"Assist-drier\"),   increase_method = c(\"Largest\", \"Smallest\", \"Most\", \"Assist-warm\", \"Assist-drier\"),   warmest_col = NULL,   precip_col = NULL )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PolygonBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample spatial zones within a species range — PolygonBasedSample","text":"x sf object. Species range simple feature. zones sf object. Spatial zones vector data (ecoregions, PSTZs, etc.). zone_key Character. Column name identifying unique zones (required). n Numeric. Desired total number samples. Default = 20. decrease_method Character. Method n < number zones. One : \"Largest\", \"Smallest\", \"\", \"Assist-warm\", \"Assist-drier\". Default = \"Largest\". increase_method Character. Method n > number zones. One : \"Largest\", \"Smallest\", \"\", \"Assist-warm\", \"Assist-drier\". Default = \"Largest\". warmest_col Character. Column name warmest temperature metric (required \"Assist-warm\" method). precip_col Character. Column name precipitation metric (required \"Assist-drier\" method).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PolygonBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample spatial zones within a species range — PolygonBasedSample","text":"sf object selected zones/polygons allocation column indicating number samples per polygon.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PolygonBasedSample.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sample spatial zones within a species range — PolygonBasedSample","text":"Simple features can store polygon data 'MULTIPOLYGON' (polygons class stored collectively) 'POLYGON' (individual polygon unique entry). function cast MULTIPOLYGONS POLYGONS needed, pre-casting improve performance. Available methods zone selection: Largest: Select zones total area (descending) Smallest: Select zones total area (ascending) : Select zones polygons (highest fragmentation) Assist-warm: Select warmest zones (requires warmest_col) Assist-drier: Select driest zones (requires precip_col)","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PolygonBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample spatial zones within a species range — PolygonBasedSample","text":"","code":"if (FALSE) { # \\dontrun{ library(tidyverse)  sr_mat <- rbind(   c(0,0), c(10,0), c(10,10), c(0,10), c(0,0) ) sr_poly <- sf::st_polygon(list(sr_mat)) x <- sf::st_sf(id = 1, geometry = sf::st_sfc(sr_poly))  rm(sr_mat, sr_poly)  zone_polys = data.frame(   ## randomly generate some points in XY space.    x = runif(10, min = -2, max = 12),   y = runif(10, min = -2, max = 12) ) |>   # conver to spatial points   sf::st_as_sf(coords = c('x', 'y')) |>   ## allocate XY space to it's nearest point   sf::st_union() |>   sf::st_voronoi() |>   ## extract the contiguous pieces of XY space around points   sf::st_collection_extract('POLYGON') |>   sf::st_as_sf() |>   ## make up seed zones on the fly, assign multiple polygons to some zones.    dplyr::mutate(pstz_key = sample(LETTERS[1:7], size = 10, replace = T)) |>   dplyr::rename('geometry' = x) |>   sf::st_crop(x)  bp <- ggplot2::ggplot(x) +    ggplot2::geom_sf(fill = NA, lwd = 2) +    ggplot2::geom_sf(data = zone_polys, ggplot2::aes(fill = pstz_key))   bp +    ggplot2::geom_sf_label(data = zone_polys, ggplot2::aes(label = pstz_key))  ######################################################################  # example #1: request same numer of samples as zones - all zones returned.   res1 <- PolygonBasedSample(    x = x,     n = length(unique(zone_polys[['pstz_key']])),     zones = zone_polys,     zone_key  = \"pstz_key\",    increase_method = \"Most\"  )  bp +   geom_sf(data = res1, alpha = 0.9) +    geom_sf_label(data = pstz,aes(label = pstz_key))   ## note that we get the largest polygon from EACH group to sample from.   #####################################################################  # Example #2: request fewer samples than zones -> subset by method - choosing largest by area   res2 <- PolygonBasedSample(    x = x, n = 3, zones = zone_polys, zone_key  = \"pstz_key\", increase_method = \"Largest\" )  res2 |>   group_by(pstz_key) |>   mutate(total_area = sum(poly_area)) |>   sf::st_drop_geometry() |>   arrange(-total_area)|>   knitr::kable()  bp + # picks, the three largest    geom_sf(data = res2, alpha = 0.9) +    geom_sf_label(data = zone_polys, aes(label = pstz_key))  #######################################################################  # Example #3: request fewer samples than zones -> subset by method - choosing smallest by area  res3 <- PolygonBasedSample(   x = x, n = 3, zones = zone_polys, zone_key = \"pstz_key\", increase_method = \"Smallest\")  res3 |>   group_by(pstz_key) |>   mutate(total_area = sum(poly_area)) |>   sf::st_drop_geometry() |>   arrange(total_area) |>   knitr::kable()  ## returns the largest polygon (poly_area) within the `pstz_key` group, ranked by (total_area)  bp + # picks, the n smallest - too small to see sometimes   geom_sf(data = filter(res3, allocation == 0), alpha = 0.9) +    geom_sf_label(data = zone_polys, aes(label = pstz_key))   #################################################################### # Example #4: request more samples than zones -> allocate extras to Largest pSTZs  ## note that is really a rounding rule - 'Largest' favors giving extra collections to the largest  ## polygons while 'smallest' favors giving them smaller polygons. It is really mostly for edge cases ## and the two will generally behave similarly on contrived examples.  res4 <- PolygonBasedSample(    x = x, n = 12, zones = zone_polys, zone_key = \"pstz_key\", increase_method = \"Largest\")    res4 |>   group_by(pstz_key) |>   summarize(total_area = sum(poly_area),  Total_Allocation = sum(allocation)) |>   sf::st_drop_geometry() |>   arrange(-total_area) |>   knitr::kable()  bp +    theme(legend.position = 'none') +    geom_sf(data = res4, aes(fill = as.factor(allocation))) +    geom_sf_label(data = res4, aes(label = allocation))   #################################################################### # Example #5: request more samples than zones -> allocate extras to pSTZs with most polygons res5 <- PolygonBasedSample(    x = x, n = 14, zones = zone_polys, zone_key = \"pstz_key\", increase_method = \"Most\")    res5 |>   group_by(pstz_key) |>   summarize(Count = n(), Total_Allocation = sum(allocation)) |>   sf::st_drop_geometry() |>   arrange(-Count) |>   knitr::kable() } # }"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":null,"dir":"Reference","previous_headings":"","what":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"last 'analytical' portion SD modelling process. produce binary (Yes/ NA) rasters species suitable habitat based three step process. first step uses dismo::thresholds determine feature raster want maximuize, case want raster less likely capture presence omit . can subset predicted habitat, dispersed comparing nearest neighbor distances observed points. Finally, can add back areas raster know species observed, hopefully missed original SDM, always suspicious points difficult fit light inertia rest species.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"","code":"PostProcessSDM(   rast_cont,   test,   train,   thresh_metric,   quant_amt,   planar_projection )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"rast_cont raw unaltered (except masked) raster predictions (x$RasterPredictions). test test data partition elasticSDM function (x$TestData). train train data partition elasticSDM function (x$TrainData). thresh_metric ?dismo::threshold options, defaults 'sensitivity' quant_amt quantile nearest neighbors distance use steps 2 3. defaults 0.25, using median nearest neighbor distance 10 bootstrapping replicates estimating buffer restrict SDM surface , minimum 10 bootstrap reps adding surface presence points placed binary suitable habitat. planar_projection Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"list containing two options. 1) spatraster 4 layers, ) continuous probabilities suitable habitat feed elasticSDM, B) raster binary format based specified thresholding statistic, C) binary raster B + habitat clipped buffer distances determined measuring nearest neighbor distances thresholding quantile D) binary raster C, adding distance points initially cells classified thresholding suitable habitat. D' general basis future steps, either B, C serve alternatives. 2) threshold statistics calculated dismo dataframe.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PrioritizeSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine which areas in a sample unit should be prioritized — PrioritizeSample","title":"Determine which areas in a sample unit should be prioritized — PrioritizeSample","text":"function offers guidance prioritizing sample locations orders. two parts, first essentially creates simple 'heatmap' tapering geographic center polygon sampled germplasm collection. keep spatial data 'light' , use also hard angled edges (ala vectorization raster), suggest n_breaks per polygon. levels side function relate increasing distances geographic center polygon. cells denoted '1' ideal areas sample polygon maintain well spaced distances across focal taxons range. second part offers loose order prioritizing general order collections. Using user specified metric attempts 'spread' samples across range reduce variance distances samples case desired number samples achieved. within individual sample units returned *Sample function. goal either method avoid collectors teams 'cheat' system repeatedly collecting along border two grid cells. understand many teams may collecting closely due species biology, land management, restrictions, goal function try guide dispersing activity. method used computes geometric centroid region, center falls outside grid, snapped back onto nearest location default. centers cell calculated remaining area grid distances calculated centers locations. final processing n_breaks applied based distances desired cell center partition space different priority collection units. Note submitting data PolygonBasedSample, column n, must maintained.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PrioritizeSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine which areas in a sample unit should be prioritized — PrioritizeSample","text":"","code":"PrioritizeSample(   x,   n_breaks = 3,   verbose = TRUE,   metric = c(\"var\", \"sd\", \"energy\", \"cv\") )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PrioritizeSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine which areas in a sample unit should be prioritized — PrioritizeSample","text":"x sf/tibble/dataframe. set sample grids *Sample functions n_breaks Numeric. number breaks return function, defaults 3. Values beyond 5 questionable utility. verbose Bool. Whether print messages console , defaults TRUE. metric character. metric minimize ordering zones. Options \"var\" (variance), \"sd\" (standard deviation), \"energy\" (sum squared distances), \"cv\" (coefficient variation).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PrioritizeSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine which areas in a sample unit should be prioritized — PrioritizeSample","text":"sf object containing prioritization zones within sample unit, columns: ID, SampleOrder, Level, geometry. ID corresponds sample unit, SampleOrder order prioritize sampling unit, Level priority level within unit (1 highest), geometry spatial geometry prioritization zones.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PrioritizeSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine which areas in a sample unit should be prioritized — PrioritizeSample","text":"","code":"if (FALSE) { # \\dontrun{ nc <- sf::st_read(system.file(\"shape/nc.shp\", package=\"sf\"), quiet = TRUE) |>   dplyr::select(NAME) |>   sf::st_transform(5070) # should be in planar coordinate system.   set.seed(1) zones <- EqualAreaSample(nc, n = 20, pts = 1000, planar_proj = 32617, reps = 100)  # the function requires an input sampling strategy to create the prioritization areas ps <- PrioritizeSample(zones$Geometry, n_breaks = 3, metric = 'energy')  ggplot2::ggplot() +    ggplot2::geom_sf(data = ps[['Geometry']],   ggplot2::aes(fill = factor(Level)), color = 'white', lwd = 1) +    ggplot2::theme_void() +    ggplot2::labs(fill = 'Within Zone Priority:', title = 'Focal areas to center sampling within') +   ggplot2::theme(legend.position= 'bottom')  ps[['Geometry']] |> ### to visualize without the priority zones within.    dplyr::group_by(SampleOrder) |>    dplyr::summarize(geometry = sf::st_union(geometry)) |>    ggplot2::ggplot() +    ggplot2::geom_sf(ggplot2::aes(fill = SampleOrder), color = 'white') +   ggplot2::geom_sf_label(ggplot2::aes(label = SampleOrder), color = 'white', size = 7) +    ggplot2::labs(fill = 'Sample Order', title = 'Priority guidance for sampling order') +    ggplot2::theme_void() +    ggplot2::theme(legend.position= 'bottom') } # }"},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":null,"dir":"Reference","previous_headings":"","what":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"rescaled rasters can used clustering, predicting results cluster analysis back space final product.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"","code":"RescaleRasters(model, predictors, training_data, pred_mat)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"model final output model glmnet elasticSDM predictors raster stack use process elasticSDM training_data data went glmnet model, used calculating variance required scaling process. elasticSDM pred_mat Prediction matrix elasticSDM","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"list two objects. 1) rescaled raster stack. 2) table standardized unstandardized coefficients glmnet model.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/TestGridSizes.html","id":null,"dir":"Reference","previous_headings":"","what":"Get an estimate for how many grids to draw over a species range — TestGridSizes","title":"Get an estimate for how many grids to draw over a species range — TestGridSizes","text":"function uses dimensions species grid estimate many grids need added x y directions cover 20 grid cells roughly equal areas.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/TestGridSizes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get an estimate for how many grids to draw over a species range — TestGridSizes","text":"","code":"TestGridSizes(target)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/TestGridSizes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get an estimate for how many grids to draw over a species range — TestGridSizes","text":"target species range simple feature (sf) object.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/TestGridSizes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get an estimate for how many grids to draw over a species range — TestGridSizes","text":"dataframe testing results grid combination. user needs select optimal grid size based tradeoff minimizing variance, without creating many grids need erased. Rhode Island example use 'Original' option asks 4 x grids 7 y grids.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/TestGridSizes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get an estimate for how many grids to draw over a species range — TestGridSizes","text":"","code":"ri <- spData::us_states |> dplyr::select(NAME) |>    dplyr::filter(NAME == 'Rhode Island') |>    sf::st_transform(32617)  sizeOptions <- TestGridSizes(ri) head(sizeOptions) #>       Name Grids    Variance GridNOx GridNOy #> 1 Smallest    49    9.242133       6       9 #> 2  Smaller    37  355.799568       5       8 #> 3 Original    25 1038.567100       4       7 #> 4   Larger    16 1245.553777       3       6 #> 5  Largest    11 1322.092849       2       5"},{"path":"https://sagesteppe.github.io/safeHavens/reference/WesternEcoregions.html","id":null,"dir":"Reference","previous_headings":"","what":"Omernik level 4 ecoregions of California and Oregon — WesternEcoregions","title":"Omernik level 4 ecoregions of California and Oregon — WesternEcoregions","text":"vector data set Omernik level 4 ecoregions clipped California Oregon. Downloaded https://www.epa.gov/eco-research/level-iii--iv-ecoregions-continental-united-states simplified using mapshaper. vector data set Bioregions developed Morrone et al. 2022 Downloaded https://neotropicalmap.atlasbiogeografico.com/ simplified using mapshaper.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/WesternEcoregions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Omernik level 4 ecoregions of California and Oregon — WesternEcoregions","text":"","code":"data(WesternEcoregions)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/WesternEcoregions.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Omernik level 4 ecoregions of California and Oregon — WesternEcoregions","text":"spatial vector data set. spatial vector data set.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/WesternEcoregions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Omernik level 4 ecoregions of California and Oregon — WesternEcoregions","text":"L4_KEY. Full name codes Level 4 ecoregions US_L4CODE. Codes level 4 ecoregions US_L4NAME. Names level 4 ecoregions Provincias Full name bioregion","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/assignGrid_pts.html","id":null,"dir":"Reference","previous_headings":"","what":"place random points in the polygon which will be dissolved with the larger polygons — assignGrid_pts","title":"place random points in the polygon which will be dissolved with the larger polygons — assignGrid_pts","text":"function ran within GridBasedSample place points throughout polygon geometries merged larger polygons assign neighboring polygons based much area want grow polygons .","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/assignGrid_pts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"place random points in the polygon which will be dissolved with the larger polygons — assignGrid_pts","text":"","code":"assignGrid_pts(neighb_grid, focal_grid, props, nf_pct)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/assignGrid_pts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"place random points in the polygon which will be dissolved with the larger polygons — assignGrid_pts","text":"neighb_grid  focal_grid  props  nf_pct","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/createPCNM_fitModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Create global/regional PCNM surfaces and fit elastic net regression to all covariates — createPCNM_fitModel","title":"Create global/regional PCNM surfaces and fit elastic net regression to all covariates — createPCNM_fitModel","text":"function lifting SDM workflow. create PCNM/MEM surfaces subset local/global maps. can use Thin plate regression predict onto actual raster surface can used prediction downstream.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/createPCNM_fitModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create global/regional PCNM surfaces and fit elastic net regression to all covariates — createPCNM_fitModel","text":"","code":"createPCNM_fitModel(x, planar_proj, ctrl, indices_knndm, sub, test)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/createPCNM_fitModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create global/regional PCNM surfaces and fit elastic net regression to all covariates — createPCNM_fitModel","text":"x training data sf/tibble/dataframe planar_proj Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. ctrl control object created character SDM function. indices_knndm sdm function sub subset predictors elasticSDM test test data partition elasticSDM","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/createPCNM_fitModel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create global/regional PCNM surfaces and fit elastic net regression to all covariates — createPCNM_fitModel","text":"use cross validation determine suitable glmnet model alpha lambda, fit using glmnet. returns three objects spit environment, 1) pcnm, surfaces eigenvectors used glmnet model (including shrunk ), 2) glmnet model 3) fitting information carets process.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a quick SDM using elastic net regression — elasticSDM","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"function quickly creates SDM using elastic net regression, properly format data downstream use safeHavens workflow. Note elastic net models used couple important reasons: rescale input independent variables modelling, allowing us combine raw data beta coefficients use clustering algorithms downstream. also allowing 'shrinking' terms models shrinking terms models able get levels ecological inference prohibited older model selection frameworks.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"","code":"elasticSDM(x, predictors, planar_projection, domain, quantile_v = 0.025)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"x (simple feature) sf data set occurrence data species. predictors terra 'rasterstack' variables serve indepedent predictors. planar_projection Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. domain Numeric, many times larger make entire domain analysis simple bounding box around occurrence data x. quantile_v Numeric, variable used thinning input data, e.g. quantile = 0.05 remove records within lowest 5% distance iteratively, remaining records apart distance . want essentially thinning happen just supply 0.01. Defaults 0.025.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"list 12 objects, subsequently used downstream  SDM Post processing sequence, think best written disk. actual model prediction raster surface present first list 'RasterPredictions', indepedent variables used final model present 'Predictors, just global PCNM/MEM raster surfaces 'PCNM'. fit model 'Model', cross validation folds stored 'CVStructure', results single test/train partition 'ConfusionMatrix', two data split 'TrainData' 'TestData' finally 'PredictMatrix' used classifying test data confusion matrix.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"","code":"if (FALSE) { # \\dontrun{   x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv'))  x <- x[,c('lon', 'lat')]  x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)   files <- list.files(    path = file.path(system.file(package=\"dismo\"), 'ex'),     pattern = 'grd',  full.names=TRUE )  predictors <- terra::rast(files)  sdModel <- elasticSDM(    x = x, predictors = predictors, quantile_v = 0.025,    planar_projection =      '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs')        terra::plot(sdModel$RasterPredictions) } # }"},{"path":"https://sagesteppe.github.io/safeHavens/reference/greatCircleDistance.html","id":null,"dir":"Reference","previous_headings":"","what":"Haversine Distance Calculation — greatCircleDistance","title":"Haversine Distance Calculation — greatCircleDistance","text":"Calculate geographic distances geoid. results accurate distance calculations planar system. Function mostly used internally maximize_dispersion","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/greatCircleDistance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Haversine Distance Calculation — greatCircleDistance","text":"","code":"greatCircleDistance(lat1, lon1, lat2, lon2)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/greatCircleDistance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Haversine Distance Calculation — greatCircleDistance","text":"lat1 Double. column holding coords 'focal' population lon1 Double. column holding coords 'focal' population lat2 Double. column holding coords 'non-focal' population lon2 Double. column holding coords 'non-focal' population","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/greatCircleDistance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Haversine Distance Calculation — greatCircleDistance","text":"calculate distances sites (Haversine formula)","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/greatCircleDistance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Haversine Distance Calculation — greatCircleDistance","text":"","code":"n_sites <- 5 # number of known populations  df <- data.frame(    site_id = seq_len(n_sites),    lat = runif(n_sites, 25, 30),     lon = runif(n_sites, -125, -120)  )  dist_mat <- sapply(1:nrow(df), function(i) {    greatCircleDistance(      df$lat[i], df$lon[i],      df$lat, df$lon    )  }) #head(dist_mat)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/healPolygons.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean up unioned geometries - part 1 — healPolygons","title":"Clean up unioned geometries - part 1 — healPolygons","text":"function uses sf::st_snap remove small lines artifacts associated unioning polygons. ran within snapGrids","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/healPolygons.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean up unioned geometries - part 1 — healPolygons","text":"","code":"healPolygons(x)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/healPolygons.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean up unioned geometries - part 1 — healPolygons","text":"x output snapgrids","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/order_by_distance_variance.html","id":null,"dir":"Reference","previous_headings":"","what":"Order zones by minimizing distance variance — order_by_distance_variance","title":"Order zones by minimizing distance variance — order_by_distance_variance","text":"Order set spatial zones based minimizing distance variance","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/order_by_distance_variance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Order zones by minimizing distance variance — order_by_distance_variance","text":"","code":"order_by_distance_variance(x, metric = c(\"var\", \"sd\", \"energy\", \"cv\"))"},{"path":"https://sagesteppe.github.io/safeHavens/reference/order_by_distance_variance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Order zones by minimizing distance variance — order_by_distance_variance","text":"x sf object containing zones ordered metric character. metric minimize ordering zones. Options \"var\" (variance), \"sd\" (standard deviation), \"energy\" (sum squared distances), \"cv\" (coefficient variation).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/order_by_distance_variance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Order zones by minimizing distance variance — order_by_distance_variance","text":"numeric vector representing order zones based specified distance variance metric","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/reduceFinalGrids.html","id":null,"dir":"Reference","previous_headings":"","what":"More sliver fixing — reduceFinalGrids","title":"More sliver fixing — reduceFinalGrids","text":"instances tiny little grids tagged along processing. just give arbitrary nearest feature. annoying chance (... millionth area...) arbitrary random point missed, areas tend remarkable inconsequential, worth randomly reassigning neighbor","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/reduceFinalGrids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"More sliver fixing — reduceFinalGrids","text":"","code":"reduceFinalGrids(final_grids)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/reduceFinalGrids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"More sliver fixing — reduceFinalGrids","text":"final_grids truthfully nearly final point.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. dplyr any_of magrittr %>%","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/snapGrids.html","id":null,"dir":"Reference","previous_headings":"","what":"turn the point grid suggestions made by assignGrid_pts into polygons — snapGrids","title":"turn the point grid suggestions made by assignGrid_pts into polygons — snapGrids","text":"function part grid based sampling process turn small grid cells, broken , larger existing grid cells.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/snapGrids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"turn the point grid suggestions made by assignGrid_pts into polygons — snapGrids","text":"","code":"snapGrids(x, neighb_grid, focal_grid)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/snapGrids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"turn the point grid suggestions made by assignGrid_pts into polygons — snapGrids","text":"x output assignGrid_pts neighb_grid neighboring grid options. focal_grid grid reassign area .","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/split_cols.html","id":null,"dir":"Reference","previous_headings":"","what":"split and extract the temperature values from Tmin and AHM columns — split_cols","title":"split and extract the temperature values from Tmin and AHM columns — split_cols","text":"Programmed Bower provisional seed zone products, helper function separating recovering values columns data.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/split_cols.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"split and extract the temperature values from Tmin and AHM columns — split_cols","text":"","code":"split_cols(dat, y, sep = \"-\")"},{"path":"https://sagesteppe.github.io/safeHavens/reference/split_cols.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"split and extract the temperature values from Tmin and AHM columns — split_cols","text":"dat data frame columns required split. y character. column name split. sep character. separator values split . Default '-'.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/split_cols.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"split and extract the temperature values from Tmin and AHM columns — split_cols","text":"","code":"df = data.frame(   'Tmin_class' = c('10 - 15 Deg. F.', '15 - 20 Deg. F.', '> 55 Deg. F.' ),   'AHM_class' = c('2 - 3', '6 - 12', '3 - 6') ) split_cols(df, 'Tmin_class') #>   lower upper median range #> 1    10    15   12.5     5 #> 2    15    20   17.5     5 #> 3    55    55   55.0     0 split_cols(df, 'AHM_class') #>   lower upper median range #> 1     2     3    2.5     1 #> 2     6    12    9.0     6 #> 3     3     6    4.5     3"},{"path":"https://sagesteppe.github.io/safeHavens/reference/trainKNN.html","id":null,"dir":"Reference","previous_headings":"","what":"partition data and train a simple KNN model — trainKNN","title":"partition data and train a simple KNN model — trainKNN","text":"Simply use partition test data quickly train simple model","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/trainKNN.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"partition data and train a simple KNN model — trainKNN","text":"","code":"trainKNN(x, split_prop)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/trainKNN.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"partition data and train a simple KNN model — trainKNN","text":"x weighted matrix including class ID column 'ID' split_prop prop data partitions.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":null,"dir":"Reference","previous_headings":"","what":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"function used write wide range values fitPredictOperationalize process. create multiple subdirectories within user specified path. include: 'Rasters' raster stack four final rasters go, 'Fitting' details model fitting caret placed, 'Models' final fit model go, 'Evaluation' evaluation statistics placed, 'Threshold' results form dismo::threshold placed.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"","code":"writeSDMresults(   path,   taxon,   cv_model,   pcnm,   model,   cm,   coef_tab,   f_rasts,   thresh )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"path root path 5 folders created, exist. taxon name taxonomic entity models created. cv_model cross validation data elasticSDM pcnm pcnm/mem rasters elasticSDM model final glmnet model elasticSDM cm confusion matrix elasticSDM coef_tab coefficient table RescaleRasters f_rasts final rasters RescaleRasters thresh threshold statistics PostProcessSDM","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"objects, objects specified, written disk.","code":""}]
