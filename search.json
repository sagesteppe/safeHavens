[{"path":"https://sagesteppe.github.io/safeHavens/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Reed Clark Benkendorf Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"about-safehavens","dir":"Articles","previous_headings":"","what":"about safeHavens","title":"About","text":"package helps germplasm curators identify communicate areas interest collection teams targeting new accessions. common species, provides multiple sampling approaches curators choose individual taxon aim collect. also provides additional sampling approach rare species. package allows integration existing workflows, reading accession data databases Excel, comparing occurrence data, writing results tabular (e.g.. CSV, database, Excel, etc.) simple spatial data formats (e.g., geopackages, shapefiles, etc.), can shared collection teams work handheld electronic devices (e.g., Qfield (free, preferred), ESRI products (paid, legacy)). allows germplasm curators sophisticated spatial capabilities without need GIS experts paid software.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"common-species-sampling-approaches","dir":"Articles","previous_headings":"about safeHavens","what":"‘common’ species sampling approaches","title":"About","text":"approach based fundamental genetics ecological theory, loosely aligns concepts landscape genetics/genomics. practice, functions attempt capture geographic environmental variation across species range. end, evolve simply partitioning species range n equal area, regularly spaced areas, modeling isolation distance, resistance, environment, sampling existing spatial data ecoregions seed transfer zones. geographic distance functions largely rely Sewall Wright’s concept Isolation Distance (1943), now empirically supported idea populations geographically proximate genetically similar populations apart. approaches package priori, exception EnvironmentalBasedSample function, relies species distribution model (SDM) inform sampling locations based concept Isolation Environment (Wang & Bradburd 2014); however still field valdiation. None sampling approaches meant supplant genetic diversity-based sampling, recognize data rare, even gathering often miss opportunity make opportunistic seed collections along way. methods various trade-offs terms computational environmental complexity, although designed run standard desktop laptop computer, processing couple thousand species overnight, faster methods, possible. table presents currently implemented sampling scheme user-facing function associated .","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"geographic-distance","dir":"Articles","previous_headings":"about safeHavens","what":"geographic distance","title":"About","text":"first two functions, PointBasedSample, EqualAreaSample, flavors process, partition species range geographic clusters similar sizes. OpportunisticSample method special case PointBasedSample, existing collection records used help guide sampling scheme, words additional samples designed around existing samples. OpportunisticSample uses underlying logic PointBasedSample, first ‘removes’ areas around existing collection records, fills remaining gaps new sampling locations PointBasedSample EqualAreaSample functions used establishing new collection strategy species, OpportunisticSample used augmenting existing collection strategy. IBDBasedSample also relies geographic distance, lieu using continuity geographic space primary method, focuses discontinuity space uses distance matrices clustering identify portions species range closer others. method slightly computationally expensive previous geographic distance methods.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"using-regions","dir":"Articles","previous_headings":"about safeHavens","what":"using regions","title":"About","text":"PolygonBasedSample may commonly encountered method North America, various formats, driving two major germplasm banking projects Midwest Southeastern United States, well high level, composing way numerous native seed collection coordinators structured West. method uses environmental variation implicit guide target populations seed collections; , different ecoregions serve stratification agents. broad strokes, general thinking regions represent continuous transitions environment faced species, populations across ranges differently adapted environments. can used either ecoregion seed transfer zone based data. However, relies existing spatial data products, may may relevant ecology species.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"resistance-distance","dir":"Articles","previous_headings":"about safeHavens","what":"resistance distance","title":"About","text":"IBRSurface workstream allows users parameterize general cost surface landscape. least-cost paths occupied portions species range calculated clustered. Spatial data end process can passed PolygonBasedSample one collection per zone desired.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"environmental-distance","dir":"Articles","previous_headings":"about safeHavens","what":"environmental distance","title":"About","text":"EnvironmentalBasedSample, computationally expensive environmentally explicit. function fits Species Distribution Model (SDM), generated via generalized linear model (glmnet), supported package, cluster populations based environmental variables related observed distributions spatial configuration distance . paper, draws together aspects functions; however empirical testing approach implemented. Spatial data end process can passed PolygonBasedSample one collection per zone desired.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"rare-species-sampling-approach","dir":"Articles","previous_headings":"","what":"rare species sampling approach","title":"About","text":"methods designed common species, species 50 100 unique occurrence records. method returns polygon geometry provides guidelines general regions species sampled. However, rare species, occurrence individual populations tracked, generally collected along maternal lines, requiring considerably field effort gather seed, supplemental approach exists suggests priority individual populations (‘points’) can sampled. KMedoidsBasedSample method utilizes either geographic distances alone geographic distances conjunction distance matrix developed key environmental variables suggest populations prioritized seed collections. method based concept maximizing dispersion selected points geographic environmental spaces best capture overall variation present species range.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"installation","dir":"Articles","previous_headings":"rare species sampling approach","what":"installation","title":"About","text":"safeHavens can installed directly github, using either devtools remotes. Note software relies packages may require additional system dependencies, rgeos rgdal. also requires working installation GDAL, PROJ, GEOS system. can occasionally finicky install, effectively open-source geospatial softwares available . safeHavens can installed GitHub remotes devtools. users issues devtools Windows, remotes tends work well.","code":"remotes::install_github('sagesteppe/safeHavens')   # install.packages('devtools')  # devtools::install_github('sagesteppe/safeHavens')"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"set-up","dir":"Articles","previous_headings":"","what":"set up","title":"Getting Started","text":"safeHavens can installed directly github.","code":"# remotes::install_github('sagesteppe/safeHavens@IBR') library(safeHavens) library(ggplot2) library(sf) library(terra) library(spData) library(dplyr) library(patchwork) set.seed(23)   planar_proj <- \"+proj=laea +lat_0=-15 +lon_0=-60 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\""},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"defining-a-species-range-or-domain-for-sampling","dir":"Articles","previous_headings":"","what":"Defining a Species Range or Domain for Sampling","title":"Getting Started","text":"Central sampling schemes safeHavens species range, domain, sampling. example, depending goals collection, curator may want sample across entire range species. Alternatively one may interested sampling portion range, e.g. country, state, ecoregion. Either scenarios can supported package. show create species range occurrence data, use range various sampling schemes. use sf simply buffer occurrence points create species range across multiple South American nations.  Alternatives include simple convex hull around species, widespread throughout area (see ‘Worked Example’ example), masking binary SDM surface domain (see , ‘Species Distribution Model’).","code":"x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326) x_buff <- sf::st_transform(x, planar_proj) |>   # we are working in planar metric coordinates, we are   # buffer by this many / 1000 kilometers.    sf::st_buffer(125000) |>    sf::st_as_sfc() |>    sf::st_union()  plot(x_buff)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"prep-a-base-map","dir":"Articles","previous_headings":"","what":"Prep a base map","title":"Getting Started","text":"use spData package uses naturalearth ’s world data suitable creating maps variety resolutions.","code":"x_extra_buff <- sf::st_buffer(x_buff, 100000) |> # add a buffer to 'frame' the maps   sf::st_transform(4326)  americas <- spData::world americas <- sf::st_crop(americas, sf::st_bbox(x_extra_buff)) |>   dplyr::select(name_long) Warning: attribute variables are assumed to be spatially constant throughout all geometries  bb <- sf::st_bbox(x_extra_buff)  map <- ggplot() +    geom_sf(data = americas) +    theme(     legend.position = 'none',      panel.background = element_rect(fill = \"aliceblue\"),      panel.grid.minor.x = element_line(colour = \"red\", linetype = 3, linewidth  = 0.5),      axis.ticks=element_blank(),     axis.text=element_blank(),     plot.background=element_rect(colour=\"steelblue\"),     plot.margin=grid::unit(c(0,0,0,0),\"cm\"),     axis.ticks.length = unit(0, \"pt\"))+    coord_sf(xlim = c(bb[1], bb[3]), ylim = c(bb[2], bb[4]), expand = FALSE)  rm(x_extra_buff, americas)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"running-the-various-sample-design-algorithms","dir":"Articles","previous_headings":"","what":"Running the Various Sample Design Algorithms","title":"Getting Started","text":"Now data can represent species ranges, can run various sampling approaches. table introduction reproduced . Note table ‘Comp.’ ‘Envi.’ refer computational environmental complexity respectively, range low (L) medium high.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"point-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Point Based Sample","title":"Getting Started","text":"PointBasedSample simply samples grid regularly space points across domain, assigns areas nearest point cluster. work well common species without many gaps distributions.","code":"pbs <- PointBasedSample(x_buff, reps = 50, BS.reps = 333) pbs.sf <- pbs[['Geometry']]  pbs.p <- map +    geom_sf(data = pbs.sf, aes(fill = factor(ID))) +  #  geom_sf_label(data = pbs.sf, aes(label = ID), alpha = 0.4) +    labs(title = 'Point') +    coord_sf(expand = F)  pbs.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"equal-area-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Equal Area Sample","title":"Getting Started","text":"Perhaps simplest method offered safeHavens EqualAreaSample. creates many points, pts defaulting 5000, within target domain subjects k-means clustering groups specified n, target number collections. individual points assigned group merged polygons ‘take’ geographic space, intersected back species range, area polygon measured. process ran times, defaulting 100 reps, set polygons created reps smallest variance polygon size selected returned. differs point based sampling instance, start regularly spaced points grow , take step back using many points let clusters grow similar sizes.  results look quite similar point based sample.","code":"eas <- EqualAreaSample(x_buff, planar_proj = planar_proj)   eas.p <- map +    geom_sf(data = eas[['Geometry']], aes(fill = factor(ID))) +  #  geom_sf_label(data = eas.sf, aes(label = ID), alpha = 0.4) +    labs(title = 'Equal Area') +    coord_sf(expand = F) eas.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"opportunistic-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Opportunistic Sample","title":"Getting Started","text":"Users may interested can embed existing collections sampling framework. function OpportunisticSample makes minor modifications point based sample designs around existing collections doesn’t always work exceptionally, especially couple collections close , old saying goes “bird hand worth two bush”. observed, previous sampling schemes somewhat similar results - used PointBasedSample framework embed function.  , grids aligned around existing collections. results function can lead oddly shaped clusters, bird hand worth two bush.","code":"exist_pts <- sf::st_sample(x_buff, size = 10) |>     sf::st_as_sf() |> # ^^ randomly sampling 10 points in the species range    dplyr::rename(geometry = x)  os <- OpportunisticSample(polygon = x_buff, n = 20, collections = exist_pts, reps = 50, BS.reps = 333)  os.p <- map +    geom_sf(data = os[['Geometry']], aes(fill = factor(ID))) +  #  geom_sf_label(data = os.sf, aes(label = ID), alpha = 0.4) +    geom_sf(data = exist_pts, alpha = 0.4) +    labs(title = 'Opportunistic') +    coord_sf(expand = F)  os.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"isolation-by-distance-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Isolation by Distance Based Sample","title":"Getting Started","text":"Isolation Distance fundamental idea behind package. function explicitly uses IBD develop sampling scheme, obfuscate parameters. Note function requires raster, rather vector, input.  data processed raster, 90 corners large straight lines, representing raster tiles. Regardless, evident borders clusters natural looking previous (future) sampling schemes, better matching individual polygons single classes.","code":"files <- list.files(    path = file.path(system.file(package=\"dismo\"), 'ex'),   pattern = 'grd',  full.names=TRUE )  predictors <- terra::rast(files)   x_buff.sf <- sf::st_as_sf(x_buff) |>    dplyr::mutate(Range = 1) |>    sf::st_transform( terra::crs(predictors))  # and here we specify the field/column with our variable we want to become an attribute of our raster v <- terra::rasterize(x_buff.sf, predictors, field = 'Range')   # now we run the function demanding 20 areas to make accessions from,  ibdbs <- IBDBasedSample(     x = v,      n = 20,      fixedClusters = TRUE,      template = predictors,      planar_proj = planar_proj     )  ibdbs.p <- map +    geom_sf(data = ibdbs[['Geometry']], aes(fill = factor(ID))) +  #  geom_sf_label(data = os.sf, aes(label = ID), alpha = 0.4) +    labs(title = 'IBDistance') +    coord_sf(expand = F)  ## for the sake of comparing areas below, we will intersect this to the same extents as the earlier surfaces.  ibdbs_crop <- sf::st_intersection(ibdbs[['Geometry']], sf::st_union(x_buff.sf)) ibdbs.p2 <- map +    geom_sf(data = ibdbs_crop, aes(fill = factor(ID))) +  #  geom_sf_label(data = os.sf, aes(label = ID), alpha = 0.4) +    labs(title = 'IBDistance') +    coord_sf(expand = F)  ibdbs.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"isolation-by-resistance","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Isolation by resistance","title":"Getting Started","text":"workflow requires couple steps enact. vignette dedicated detailing Isolation Resistance - check ! just load data get run vignette.","code":"ibr <- sf::st_read(   file.path(system.file(package=\"safeHavens\"), 'extdata', 'IBR.gpkg'),    quiet = TRUE)  ibr.p <- map +    geom_sf(data = ibr, aes(fill = factor(ID))) +   labs(title = 'IBResistance') +    coord_sf(expand = F)  [1m [22mCoordinate system already present.  [36mℹ [39m Adding new coordinate system, which will replace the existing one."},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"polygon-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Polygon Based Sample","title":"Getting Started","text":"commonly implemented method guiding native seed collection North America. However, sure exactly practitioners implement , whether formats application consistent among practitioners! reasons different sets options supported user. general usage, two parameters always required x species range sf object, ecoregions, sf object containing ecoregions interest. ecoregions file need subset range x quite yet - function take care . Additional arguments function include usual n specify many accession looking collection. Two additional arguments relate whether using Omernik Level 4 ecoregions data ecoregions (biogeographic regions) another source. OmernikEPA, ecoregion_col, using official EPA release ecoregions optional, however using EPA product supplied - ecoregion_col argument totally necessary. column contain unique names highest resolution level ecoregion want use data set, many data sets, example call ‘neo_eco’ may field ecolevel information! output differs others see, depicted number collections made per ecoregion. number ecoregions greater requested sample size, return object can take two values - collections, one collection.","code":"neo_eco <- sf::st_read(   file.path(system.file(package=\"safeHavens\"), 'extdata', 'NeoTropicsEcoregions.gpkg'),    quiet = TRUE) |>   dplyr::rename(geometry = geom) head(st_drop_geometry(neo_eco)[,c('Provincias', 'Dominio', 'Subregion')])                  Provincias      Dominio                      Subregion 1 Araucaria Forest province       Parana                        Chacoan 2          Atacama province         <NA> South American Transition Zone 3         Atlantic province       Parana                        Chacoan 4           Bahama province         <NA>                      Antillean 5     Balsas Basin province Mesoamerican                      Brazilian 6         Caatinga province      Chacoan                        Chacoan  x_buff <- sf::st_transform(x_buff, sf::st_crs(neo_eco)) ebs.sf <- PolygonBasedSample(x_buff, zones = neo_eco, n = 20, zone_key = 'Provincias')  # crop it to the other objects for plotting ebs.sf <- st_crop(ebs.sf, bb)  ebs.p <- map +    geom_sf(data = ebs.sf , aes(fill = factor(allocation))) +    labs(title = 'Ecoregion') +    coord_sf(expand = F)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"environmental-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Environmental Based Sample","title":"Getting Started","text":"EnvironmentalBasedSample can used species distribution model data. included ouputs vignette ‘Species Distibution Model’ package available vignette.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"load-the-sdm-predictions","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms > Environmental Based Sample","what":"load the SDM predictions","title":"Getting Started","text":"load results sdm processing package data. load threshold predictions  data loaded R, scale rasters (using RescaleRasters) serve surfaces predict (also done !), run algorithm (EnvironmentalBasedSample). However, run algorithm need create directory (also called ‘folder’), computers save results function EnvironmentalBasedSample. Whereas earlier vignette showcased functions generated species distribution model, us saving results two stage process (e.g. create SDM associated products used: elasticSDM, PostProcessSDM, RescaleRasters, finally saving relevant data writeSDMresults), function produces product writes ancillary data simultaneously. approach chosen function writing four objects: 1) groups vector data 2) groups raster data 3) k-nearest neighbors (knn) model used generate clusters 4) confusion matrix associated testing knn model  function EnvironmentalBasedSample can take three binary rasters created PostProcessSDM arguments template. showcase different results using .  plots able showcase difference results depending three input rasters utilized. sampling schemes, results vary widely based spatial extents functions applied . Using SDM output undergone thresholding results largest classified area. first glance results may seem different, look central america, largely consistent, near Andes; large differences exist Amazon Basin, even alignment systems evident. Accordingly, surface used species match evaluation criterion. Using threshold raster surface good option want ‘miss’ many areas, whereas clipped supplemented options may better suited scenarios want draw clusters, lack populations can collected .","code":"sdModel <- readRDS(   file.path(system.file(package=\"safeHavens\"), 'extdata',  'sdModel.rds')   )  sdModel$RasterPredictions <- terra::unwrap(sdModel$RasterPredictions) sdModel$Predictors <- terra::unwrap(sdModel$Predictors) sdModel$PCNM <- terra::unwrap(sdModel$PCNM) sdm <- terra::rast(   file.path(system.file(package=\"safeHavens\"), 'extdata',  'SDM_thresholds.tif')   ) terra::plot(sdm) rr <- RescaleRasters( # you may have already done this!   model = sdModel$Model,   predictors = sdModel$Predictors,    training_data = sdModel$TrainData,    pred_mat = sdModel$PredictMatrix   )  # create a directory to hold the results from EBS real quick.  # we will default to placing it in your current working directory.  getwd() # this is where the folder is going to be created if you do not run the code below.  [1] \"/home/runner/work/safeHavens/safeHavens/vignettes\" p <- file.path(path.expand('~'), 'Documents') # in my case I'll dump it in Documents real quick, this should work on   # optional, intentionally create a directory to hold results # dir.create(file.path(p, 'safeHavens-Vignette'))  planar_proj <- \"+proj=laea +lat_0=-15 +lon_0=-60 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"  ENVIbs <- EnvironmentalBasedSample(   pred_rescale = rr$RescaledPredictors,    write2disk = FALSE, # we are not writing, but showing how to provide some arguments   path = file.path(p, 'safeHavens-Vignette'),    taxon = 'Bradypus_test',    f_rasts = sdm,    coord_wt = 2,   n = 20,    lyr = 'Supplemented',   fixedClusters = TRUE,    n_pts = 500,    planar_proj = planar_proj,   buffer_d = 3,   prop_split = 0.8   )  ## for the sake of comparing areas below, we will intersect this to the same extents as the earlier surfaces.  ENVIbs_crop <- sf::st_intersection(ENVIbs[['Geometry']], sf::st_union(x_buff.sf)) Warning: attribute variables are assumed to be spatially constant throughout all geometries  ENVIbs.p <- map +    geom_sf(data = ENVIbs_crop, aes(fill = factor(ID))) +    #geom_sf_label(data = ENVIbs, aes(label = ID), alpha = 0.4) +    labs(title = 'Environmental') +    coord_sf(expand = FALSE)  [1m [22mCoordinate system already present.  [36mℹ [39m Adding new coordinate system, which will replace the existing one.  ENVIbs.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"comparision-of-different-sampling-schemes","dir":"Articles","previous_headings":"","what":"Comparision of different sampling schemes","title":"Getting Started","text":", maps look ! looked relatively similar plotted one another, let’s plot simultaneously see ’s still case.  geographic based samples (Point, Equal Area, Opportunistic, IBE) quite similar. mind isolation distance (IBD) show biggest different, makes sense splitting sampling areas along naturally occurring patches species range. application PolygonBasedSample data difficult evaluate sense data sets, able identify desired regions sampling. show pane. Isolation Resistance similar IBD, however makes fewer splits Central America, instead picking Peru. Relative IBR Isolation Environment tends split locations across Andes NW coastal regions South America. ranges relatively contigious - enough sensibly sample . Contiguity ranges approach can controlled coord_wt argument.","code":"pbs.p + eas.p + os.p  +  ibdbs.p2 + ibr.p + ENVIbs.p +    plot_layout(ncol = 3)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/IsolationByResistance.html","id":"isolation-by-resistance-sampling","dir":"Articles","previous_headings":"","what":"Isolation by Resistance Sampling","title":"Isolation By Resistance","text":"Landscape genetics laddered null hypothesis regarding genetic structure species. first null, Panmixia, populations show differentiation, observed species , highly connected populations; treated sampling scenario package. second null Isolation Distance, main idea behind functions package, populations become differentiated function geographic distance - culminating function ‘IBDBasedSample’. final null hypothesis Isolation Resistance, differentiation populations driven environmental distances across populations decreasing gene flow. IBD easily calculated geographic distances, IBR requires parameterized cost surface, raster format, depicts costs organism’s movement maternal paternal genetic material. safeHavens supports simple workflow paramterize cost surface, can used support basic IBR type sampling.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/IsolationByResistance.html","id":"example","dir":"Articles","previous_headings":"Isolation by Resistance Sampling","what":"Example","title":"Isolation By Resistance","text":"’ll use species occurrence data dismo . spatial data sets suitable nearly users IBR applications included safeHavens example, full data sets shipped package. include: - lakes, Lehner et al. 2025, GLWD - oceans Natural Earth, https://www.naturalearthdata.com/downloads/10m-physical-vectors/ - rivers FAO, - tri (terrain ruggedness index), Amatulli et al. 2018, https://www.earthenv.org/topography ’ll load species occurrence data make explicitly spatial.","code":"library(terra) library(sf) library(dplyr) library(ggplot2) library(safeHavens) x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)  planar_proj <- 3857 # Web Mercator for planar distance calcs"},{"path":"https://sagesteppe.github.io/safeHavens/articles/IsolationByResistance.html","id":"data-prep","dir":"Articles","previous_headings":"Isolation by Resistance Sampling","what":"data prep","title":"Isolation By Resistance","text":"environmental variable data sources needs pre-processing feed IBR workflow, just need converted vector raster formats - process call ‘rasterization’. also rescale tri rasters, put similar numeric range data sets. ’ll convert spatVect objects spatRasters, clip area analysis around species records. vector data need converted raster format using function like rasterize, terra.","code":"tri <- terra::rast(file.path(system.file(package = \"safeHavens\"), \"extdata\", \"tri.tif\")) names(tri) <- 'tri'  rescale_rast <- function(r, new_min = 0, new_max = 1) {   r_min <- global(r, \"min\", na.rm = TRUE)[[1]]   r_max <- global(r, \"max\", na.rm = TRUE)[[1]]   ((r - r_min) / (r_max - r_min)) * (new_max - new_min) + new_min }  tri <- rescale_rast(tri, 0, 100)  lakes_v <- sf::st_read(   file.path(system.file(package = \"safeHavens\"),  \"extdata\", \"lakes.gpkg\"),     quiet = T) |>   filter(TYPE == 'Lake') |> # remove reservoirs for our purposes (time scale)   select(geometry = geom) |>    mutate(Value = 1) |> # this will be rasterized.    terra::vect()  ocean_v <- sf::st_read(   file.path(system.file(package = \"safeHavens\"), \"extdata\", \"oceans.gpkg\"),    quiet = T) |>   select(geometry = geom) |>   mutate(Value = 1) |>   terra::vect()  rivers_v <- sf::st_read(   file.path(system.file(package = \"safeHavens\"), \"extdata\", \"rivers.gpkg\"),    quiet = T) |>   select(geometry = geom) |>   mutate(Value = 1) |>   terra::vect() x_buff <- sf::st_transform(x, planar_proj) |>   # huge buffer for the bbox.    st_buffer(200000) |>    st_transform(crs(lakes_v)) |>   st_as_sfc() |>    st_union() |>   vect() |>   ext()  lakes_v <- crop(lakes_v, x_buff) rivers_v <- crop(rivers_v, x_buff) ocean_v <- crop(ocean_v, x_buff) tri <- crop(tri, x_buff) ocean_r <- rasterize(ocean_v, tri, field = 'Value', background = 0.1) lakes_r <- rasterize(lakes_v, tri, field = 'Value',  background = 0.1) rivers_r <- rasterize(rivers_v, tri, field = 'Value',  background = 0.1)  par(mfrow=c(2, 2)) plot(rivers_r) plot(lakes_r) plot(ocean_r) plot(tri) dev.off() #> null device  #>           1  rm(ocean_v, lakes_v, rivers_v)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/IsolationByResistance.html","id":"actual-workflow-","dir":"Articles","previous_headings":"Isolation by Resistance Sampling","what":"actual workflow.","title":"Isolation By Resistance","text":"first, three, functions Isolation Resistance sampling workflow used create resistance surface. requires template raster, rasters incorporated product. buildResistanceSurface essentially just requires rasters configured , user specified weights assign raster. weights convey difficult species move across landscape. plants, oceans generally get high weight, lakes rivers - still obstacles - receive lower weights. Terrain ruggedness values much likely vary based ecology species, lanscape sampled. considering weights consider influences : bird dispersal, land animal dispersal, natural growth/expansion populations. certainly difficult part workstream tune, often relies subjective expert judgement. values functions defaults - eyes work OK landscape hand; although ecologist experience region may beg differ, argue .  Note object stored memory, advocate use relatively coarsely grained raster surface (e.g. 4kmx4km cells ), pretty generalized work, speed processing. Now resistance surface, next step calculating distances areas across species range. Similar functions safeHavens use population occurrence record data directly, know spatial biases affect occurrence record distributinons. Rather buffer (buffer_dist) occurrence records, using relevant planar_proj, create raster surface locations. surface sample n points (using terra::spatSample, method = ‘spread’), identify neighboring points using graph_method, either delauney triangulation, useful high n required, complete distance matrix. testing greatly prefer results ‘complete’ calculations, work NbClust without ordinated 2-dimensional space give much better results. function return raster representing buffered populations (species range domain), points sampled terra, graph information, distance matrix least cost distance values resistance surface. buffered locations look like - currently IDs meaningless.  function perform well user pre-specified n, however min.nc parameter, passed NbClust can applied generally give analyst specified value. results clustering process visualized beneath.  classified occupied area clusters visualized .  Finally surface can sampled using PolygonBasedSample. Simply union geometries species range, supply groups x argument, lieu ecoregion pstz type surface.  nine clusters different number 20 desired collections assigned .","code":"res_surface <- buildResistanceSurface(   base_raster = rast(ocean_r), # template    oceans = ocean_r, # rasters   lakes = lakes_r,   rivers = rivers_r,   tri = tri,    w_ocean = 120, # weights to multiple  input raster values by --    w_lakes = 50, # is 1 * 50   w_rivers = 20, # is 1 *20   w_tri = 4 # ranges from 1~30, so from (1-30)*4 up to a weight of ~120.  )  plot(res_surface) pop_res_graphs <- populationResistance(   base_raster = rast(ocean_r),   populations_sf = x,   n = 150,   planar_proj = 3857,   buffer_dist = 125000,   resistance_surface = res_surface,   graph_method = 'complete' )  names(pop_res_graphs) #> [1] \"pop_raster\"     \"sampled_points\" \"spatial_graph\"  \"edge_list\"      #> [5] \"ibr_matrix\" plot(pop_res_graphs$pop_raster) ibr_groups <- IBRSurface(   base_raster = rast(ocean_r),   resistance_surface = res_surface,    pop_raster = pop_res_graphs$pop_raster,   pts_sf = pop_res_graphs$sampled_points,    ibr_matrix = pop_res_graphs$ibr_matrix,    fixedClusters = TRUE,   n = 20,   planar_proj = 3857 ) classified_pts = ibr_groups$points plot(res_surface) points(   classified_pts,    pch = as.numeric(as.factor(classified_pts$ID)),  # different symbol per ID   col = rainbow(length(unique(classified_pts$ID)))[as.factor(classified_pts$ID)],   cex = 2,   lwd = 3   ) ggplot() +   geom_sf(data = ibr_groups$geometry, aes(fill = factor(ID))) +    theme_minimal() out <- PolygonBasedSample(   x = st_union(ibr_groups$geometry),   zones = ibr_groups$geometry,    zone_key = 'ID',    n  = 30   )  ggplot(data = out) +    geom_sf(aes(fill = factor(allocation))) +    theme_minimal() +    labs(fill = 'Ct. collections\\nper cluster') +    theme(legend.position = 'bottom')"},{"path":"https://sagesteppe.github.io/safeHavens/articles/IsolationByResistance.html","id":"bonus-adding-sdm-as-a-resistance-layer-and-non-linear-scaling-","dir":"Articles","previous_headings":"Isolation by Resistance Sampling","what":"bonus: adding sdm as a resistance layer, and non-linear scaling.","title":"Isolation By Resistance","text":"default argument cluster IBR probablity surface predictions SDM. add surface. also show another way adding topographc ruggedness index, ‘bypasses’ assumed linear relationship along weighting scheme.  Note reverse predictions layer, currently higher values mean suitable habitat. want higher values going probability surface function mean resistance movement.  indeed, may actually just want remove weights suitable habitat, set arbitrarily low value  convert , integer, scale rest predictors  Note make sure SDM aligns existin layers","code":"sdm <- terra::rast(   file.path(system.file(package=\"safeHavens\"), 'extdata',  'SDM_thresholds.tif')   ) plot(sdm['Predictions']) inverted_sdm <- 1 - sdm['Predictions'] plot(inverted_sdm) inverted_sdm <- terra::ifel(inverted_sdm < 0.5, 0.01, inverted_sdm) plot(inverted_sdm) inverted_sdm = round(inverted_sdm * 100, 0) plot(inverted_sdm) inverted_sdm <- crop(inverted_sdm, ocean_r) inverted_sdm <- resample(inverted_sdm, ocean_r) inverted_sdm[is.na(inverted_sdm)] <- 99  plot(inverted_sdm)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/IsolationByResistance.html","id":"non-linear-weight","dir":"Articles","previous_headings":"Isolation by Resistance Sampling > bonus: adding sdm as a resistance layer, and non-linear scaling.","what":"non-linear weight","title":"Isolation By Resistance","text":"environmental variables expected linear effects movement. Indeed something like terrain ruggedness may small effect reaching certain thresholds.  apply fourth order polynomial input tri data accomodate non-linear effect.  can create new resistance surface","code":"x <- 1:100 rs <- function(x, to = c(0, 100)) {   rng <- range(x, na.rm = TRUE)   to[1] + (x - rng[1]) / diff(rng) * diff(to) }  # display some transformations.  par(mfrow = c(2, 3), mar = c(4, 4, 2, 1)) plot(x, rs(sqrt(x)), type = \"l\", main = \"sqrt(x)\") plot(x, rs(log1p(x)), type = \"l\", main = \"log1p(x)\") plot(x, rs(asinh(x)), type = \"l\", main = \"asinh(x)\") plot(x, rs(x^2), type = \"l\", main = \"x^2: polynomial\") plot(x, rs(x^3), type = \"l\", main = \"x^3: polynomial\") plot(x, rs(x^4), type = \"l\", main = \"x^4 polynomial\") rm(x) tri_rscl <- app(tri, function(x) { rs(x^2, to = c(1, 80)) })  par(mfrow = c(1, 2), mar = c(4, 4, 2, 1)) plot(tri, main = 'Original') plot(tri_rscl, main = 'Rescaled') res_surface <- buildResistanceSurface(    base_raster = rast(ocean_r), # template    oceans = ocean_r, # rasters   lakes = lakes_r,   rivers = rivers_r,   #tri = tri, # dont use me! i rescale.    habitat = inverted_sdm,   addtl_r = tri_rscl,    w_ocean = 120, # weights to multiple  input raster values by --    w_lakes = 70, # is 1 * 50   w_rivers = 40, # is 1 *20   w_habitat = 0.5,   #w_tri = 4 # don't use me! use addtl_wt   addtl_w = 1 ) dev.off() #> null device  #>           1 plot(res_surface)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/PredictiveProvenance.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Predictive Provenance","text":"already shown beta-coefficients species distribution models can used hierarchical clustering group species populations clusters experience similar - relevant ecology - climate. running EnvironmentalBasedSample focus also optimizing species occupied range using moran eigenvector matrix surfaces (MEMs, PCNM). alternative approach drop PCNMs SDMs focus solely environmental predictors, work identify clusters current environmental space, classify future environmental (geographic) space cluster parameters. gives us sets areas populations may pre-adapted future conditions. addition transferring realized environmental clusters populations forward, also identify novel climate spaces, using MESS (Elith 2010), hierarchical clustering. results us n identified clusters, can traverse clustering branches determine existing clusters similar novel clusters. sole workflow safeHavens seeks link current climates future scenarios, directly suggest size clusters may change, shift , allow germplasm curator identify relevant seed sources predictive provenancing. approach employed adequate guide current collections, however decisions sources use particular restoration site relatively well identified tools Seed Lot Selection Tool North America, COSST South America.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/PredictiveProvenance.html","id":"analysis","dir":"Articles","previous_headings":"","what":"Analysis","title":"Predictive Provenance","text":"workstream leans heavily EnvironmentalBasedSample workflow, passes ’s final objects PolygonBasedSample completion. Please review Species Distribution Models Getting Started proceeding .","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/PredictiveProvenance.html","id":"data-prep","dir":"Articles","previous_headings":"Analysis","what":"Data prep","title":"Predictive Provenance","text":"workflow require geodata package; geodata used retrieving climate data variety sources raster data. vignette use Worldclim. safeHavens users around world may ideal source climate data. geodata developed maintained Robert Hijman, similar terra, dismo, ton heavy lifting internally functions. example leverage data GBIF, shift geographic focus Colorado Plateau Southwestern USA. Colorado Plateau predicted experience highest rates climate change planet, already area considerably native seed development activities. focal species use Helianthella microcephala reason exists relatively homogenous portions Colorado Plateau - meaning can download relatively coarse rasters vignettes, stunning inflorescence.  download worldclim data using geodata results worldclim_global loaded dismo various sloth examples. future scenario use CMIP6 modelled future climate data 2041-2060 time window. download coarse 2.5 minute, ca ~70km equator, resolution data. note source also includes ~ 1km equator resolution data set (30 seconds), take long vignette. safeHavens offer explicit functionality rename / align naming raster surfaces. However, modelling process requires current future raster products perfectly matching raster names. chunk shows standardize names extracting bioclim variable number, end name, pad ‘bio_’ back onto front leading zero : ‘bio_01’ instead ‘bio_1’. minor variation work data sources, customizing gsub erase earlier portions file name. following vignette along locally decided plot(bio_current) plot(bio_future) find look similar. plots look similar, showing one raster stack another, can diff two products see largest changes geographic space occur.  see variables inconsistences change, extent change. mismatch conditions almost certainly lead novel climate conditions - unique combinations yet known species. handled using MESS surfaces second stage clustering approach function.","code":"library(safeHavens) library(terra) library(geodata) library(sf) library(dplyr) library(tidyr) library(ggplot2) library(patchwork) set.seed(22) cols = c('decimalLatitude', 'decimalLongitude', 'dateIdentified', 'species', 'acceptedScientificName', 'datasetName',    'coordinateUncertaintyInMeters', 'basisOfRecord', 'institutionCode', 'catalogNumber')  ## download species data using scientificName, can use keys and lookup tables for automating many taxa.  hemi <- rgbif::occ_search(scientificName = \"Helianthella microcephala\") hemi <- hemi[['data']][,cols]  |>   drop_na(decimalLatitude, decimalLongitude) |> # any missing coords need dropped.    distinct(decimalLatitude, decimalLongitude, .keep_all = TRUE) |> # no dupes can be present   st_as_sf(coords = c( 'decimalLongitude', 'decimalLatitude'), crs = 4326, remove = F)   western_states <- spData::us_states |> ## for making a quick basemap.    dplyr::filter(NAME %in%      c('Utah', 'Arizona', 'Colorado', 'New Mexico', 'Wyoming', 'Nevada', 'Idaho', 'California')) |>   dplyr::select(NAME, geometry) |>   st_transform(4326)  bb <- st_bbox(   c(     xmin = -116,      xmax = -105,      ymax = 44,      ymin = 33.5),     crs = st_crs(4326)     )  western_states <- st_crop(western_states, bb) Warning: attribute variables are assumed to be spatially constant throughout all geometries  bmap <- ggplot() +      geom_sf(data = western_states) +      geom_sf(data = hemi) +     theme_minimal() +     coord_sf(         xlim = c(bb[['xmin']], bb[['xmax']]),          ylim = c(bb[['ymin']], bb[['ymax']])         )  +     theme(       axis.title.x=element_blank(),       axis.text.x=element_blank(),       axis.ticks.x=element_blank()       )  bmap +     labs(title = 'Helianthella microcephala\\noccurrence records') # Download WorldClim bioclim at ~10 km bio_current <- worldclim_global(var=\"bioc\", res=2.5) bio_future <- cmip6_world(   model = \"CNRM-CM6-1\", ## modelling method   ssp   = \"245\", ## \"Middle of the Road\" scenario   time  = \"2041-2060\", # time period   var   = \"bioc\", # just use the bioclim variables   res   = 2.5 )  # Crop to domain - use a large BB to accomodate range shift # under future time points.  # but going too large will dilute the absence records bbox <- ext(bb)  bio_current <- crop(bio_current, bbox) bio_future <- crop(bio_future, bbox) simplify_names <- function(x){     paste0('bio_', sprintf(\"%02d\", as.numeric(gsub('\\\\D+','', names(x))))) }  names(bio_current) <- gsub('^.*5m_', '', names(bio_current)) names(bio_future) <- gsub('^.*2060_', '', names(bio_future))  names(bio_current) <- simplify_names(bio_current) names(bio_future) <- simplify_names(bio_future)  # TRUE means all names match, and are in the same position.  all(names(bio_current) == names(bio_future)) [1] TRUE  ## we will also drop some variables that are essentially identical in the study area drops <- c(   'bio_05' , 'bio_02',   'bio_11', 'bio_12'   )   bio_current <- subset(bio_current, negate = TRUE, drops) bio_future <- subset(bio_future, negate = TRUE, drops) pct_diff <- function(x, y){((x - y)/((x + y)/2)) * 100} difference <- pct_diff(bio_current, bio_future) plot(difference)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/PredictiveProvenance.html","id":"analytical-workstream","dir":"Articles","previous_headings":"Analysis","what":"Analytical workstream","title":"Predictive Provenance","text":"workflow predictive provenance builds upon EnvironmentalBasedSample workflow, relying many internal helpers, user facing functions. Note using elasticSDM need specify flag pcnm = FALSE bypass fitting model PCNM surfaces - analog future scenarios must dropped.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/PredictiveProvenance.html","id":"fit-sdm-and-rescale-raster-surfaces","dir":"Articles","previous_headings":"Analysis","what":"Fit SDM and rescale raster surfaces","title":"Predictive Provenance","text":"starting point analysis fitting quick SDM species. use eSDM_model function workstream, however essential PCNM set FALSE. way forecast predict PCNM future scenarios, fit models . warning emitted likely due collinearity bioclim variables. Rather using eDist sdm generating background points use eRandom method instead.  can view results model, OK, benefit using subset bioclim predictors allow eDist sampling upstream. bit generous classifying areas possible suitable habitat. model unsuitable applications, certain CRAN Github checks make difficult improve upon example. Using beta-coefficients model rescale current future climate scenarios rasters. happen internally next function, good see different current future scenarios idea expectations final results.   layers show single color, 0, means glmnet shrunk model. Similar differenced rasters two time points , can take absolute difference relevant raster layers see largest changes .  plot shows us areas largest changes current predicted conditions.","code":"## note we subset to just the geometries for the prediction records.  # this is because we feed in all columns to the elastic net model internally.  hemi <- select(hemi, geometry)   # as always verify our coordinate reference systems (crs) match.  st_crs(bio_current) == st_crs(hemi)  [1] TRUE  # and fit the elasticnet model  eSDM_model <- elasticSDM(     x = hemi,      predictors = bio_current,      planar_projection = 5070,      PCNM = FALSE ## set to FALSE for this workstream!!!!!     ) eSDM_model$ConfusionMatrix$byClass[   c('Sensitivity', 'Specificity', 'Recall', 'Balanced Accuracy')]       Sensitivity       Specificity            Recall Balanced Accuracy          0.9200000         0.7692308         0.9200000         0.8446154  plot(eSDM_model$RasterPredictions) bio_current_rs <- RescaleRasters(     model = eSDM_model$Model,      predictors = eSDM_model$Predictors,     training_data = eSDM_model$Train,     pred_mat = eSDM_model$PredictMatrix     )  plot(bio_current_rs$RescaledPredictors) bio_future_rs <- rescaleFuture(   eSDM_model$Model,    bio_future,    eSDM_model$Predictors,   training_data = eSDM_model$Train,   pred_mat = eSDM_model$PredictMatrix )  plot(bio_future_rs) difference_rs <- pct_diff(bio_current_rs$RescaledPredictors, bio_future_rs) plot(difference_rs)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/PredictiveProvenance.html","id":"cluster-surfaces","dir":"Articles","previous_headings":"Analysis","what":"Cluster surfaces","title":"Predictive Provenance","text":"Clusters can identified using NbClust, allowing determine optimal n using METHOD. use NbClust, rather manually specifying n, argument fixedClusters needs set FALSE. using PostProcessSDM threshhold metric used applied PredictiveProvenance.   make EnvironmentalBasedSample find suitable number climate clusters current time period. using EBS predictive provencing also set coord_wt parameter small value. remove effect spatial clustering. Similar PCNM, know distribution actual populations future.    can apply current clustering future scenario. show us currently identified clusters exist future geographic space. ProjectClusters really whole heart workflow, requires outputs elasticSDM, PostProcessSDM, EnvironmentalBasedSample. However, classifications accomodate new climate conditions. can identify areas, outside training conditions SDM using MESS. Note areas SDM extrapolating, ’s performance evaluated. However, seems possibly suitable habitat, can try plan scenario.   identify areas, sufficiently large can pass new NbClust scenario can cluster anew.  However, also need determine current areas similar novel climate clusters. can take values classifcation scenarios, cluster , identify new climate clusters share branches existing clusters. method employ identify similar existing climate groups. groups likely collect germplasm relevant future scenarios. can also take quick look cluster sizes change scenarios","code":"threshold_rasts <- PostProcessSDM(   rast_cont = eSDM_model$RasterPredictions,    test = eSDM_model$TestData,   train = eSDM_model$TrainData,   planar_proj = 5070,   thresh_metric =      'equal_sens_spec',    quant_amt = 0.25   )  plot(threshold_rasts$FinalRasters) threshold_rasts$Threshold$equal_sens_spec [1] 0.694638  bmap +    geom_sf(data =    sf::st_as_sf(     terra::as.polygons(       threshold_rasts$FinalRasters['Threshold'])       ), fill = 'cornsilk'     ) +      geom_sf(data = hemi) ENVIbs <- EnvironmentalBasedSample(   pred_rescale = bio_current_rs$RescaledPredictors,    write2disk = FALSE, # we are not writing, but showing how to provide some arguments   f_rasts = threshold_rasts$FinalRasters,    coord_wt = 0.001,    fixedClusters = FALSE,   lyr = 'Threshold',   n_pts = 500,    planar_proj = \"epsg:5070\",   buffer_d = 3,   prop_split = 0.8,   min.nc = 5,    max.nc = 15   ) *** : The Hubert index is a graphical method of determining the number of clusters.                 In the plot of Hubert index, we seek a significant knee that corresponds to a                  significant increase of the value of the measure i.e the significant peak in Hubert                 index second differences plot. *** : The D index is a graphical method of determining the number of clusters.                  In the plot of D index, we seek a significant knee (the significant peak in Dindex                 second differences plot) that corresponds to a significant increase of the value of                 the measure.    *******************************************************************  * Among all indices:                                                 * 1 proposed 5 as the best number of clusters  * 6 proposed 6 as the best number of clusters  * 1 proposed 7 as the best number of clusters  * 2 proposed 8 as the best number of clusters  * 1 proposed 11 as the best number of clusters  * 3 proposed 12 as the best number of clusters  * 1 proposed 14 as the best number of clusters  * 7 proposed 15 as the best number of clusters                      ***** Conclusion *****                               * According to the majority rule, the best number of clusters is  15      *******************************************************************   bmap +    geom_sf(data = ENVIbs$Geometry, aes(fill = factor(ID))) +    geom_sf(data = hemi) +    theme(legend.position = 'bottom') +    labs(fill = 'Cluster', title = 'Current') future_clusts <- projectClusters(   eSDM_object = eSDM_model,    current_clusters = ENVIbs,   future_predictors = bio_future,   current_predictors = bio_current,   thresholds = threshold_rasts$Threshold,    planar_proj = \"epsg:5070\",    thresh_metric = 'equal_sens_spec',   n_sample_per_cluster = 20 ) plot(future_clusts$mess) bmap +   geom_sf(data =      st_as_sf(terra::as.polygons(future_clusts$novel_mask)),      fill = 'red') +    labs(title = 'MESS regions')  [1m [22mCoordinate system already present.  [36mℹ [39m Adding new coordinate system, which will replace the existing one. current = bmap +    geom_sf(data = ENVIbs$Geometry, aes(fill = factor(ID))) +   labs(title = 'Current', fill = 'Cluster') +   theme(legend.position = \"none\")  future = bmap +    geom_sf(data = future_clusts$clusters_sf, aes(fill = factor(ID))) +    labs(title = '2041-2060', fill = 'Cluster')   current + future future_clusts$novel_similarity [1] novel_cluster_id     nearest_existing_id  avg_silhouette_width <0 rows> (or 0-length row.names) future_clusts$changes    cluster_id current_area_km2 future_area_km2 area_change_pct 1           1        4043.3230      28300.9046       599.94172 2           2       13170.6534        269.7564       -97.95184 3           3        4998.7124       1407.8829       -71.83509 4           4        4435.7130        136.0061       -96.93384 5           5       12027.6067        872.3368       -92.74721 6           6       15081.4257        952.5868       -93.68371 7           7        2576.1257          0.0000      -100.00000 8           8       33353.1496       1413.2125       -95.76288 9           9        3958.0202       3040.5491       -23.18005 10         10        6312.3643        747.9952       -88.15032 11         11        1313.6535          0.0000      -100.00000 12         12         597.1152       4630.9965       675.56168 13         13       36782.2993       4143.2314       -88.73580 14         14       32653.6727       1344.0688       -95.88387 15         15           0.0000       3528.7292              NA    centroid_shift_km 1           460.1072 2           290.3649 3           205.2148 4           251.8769 5           726.1748 6           254.4887 7                 NA 8           330.4310 9           494.6931 10           14.2699 11                NA 12          539.5743 13          298.5651 14          374.5908 15                NA"},{"path":"https://sagesteppe.github.io/safeHavens/articles/PredictiveProvenance.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Predictive Provenance","text":"functions safeHavens work identify areas populations potential maximize coverage neutral allelic (genetic) diversity across species. Large scale restoration requires availability seed sources need, funding opportunities arise use . Developping germplasm materials populations seem adapted future climate conditions essential future opportunities. function way seek identify best available seed source restoration, ala SST COSST, seek ensure best available option can timely applied occasion arises. particular importance areas able plan restorations develop seed sources using point-based analyses.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"prepare-data","dir":"Articles","previous_headings":"","what":"prepare data","title":"Rare Species Sampling Schema","text":"Load required packages. use Bradypus data included dismo package vignette. create base map used GettingStarted, note chunk ‘hidden’ rendered man pages, present raw vignette documents. functions package handle sf objects directly, function actually just use simple data.frame sites, streamline making couple calculations. input KMedoidsBasedSample function list two elements: distance matrix, data frame site locations attributes. data frame must contain following columns. second required element, distance matrix, can calculated greatCircleDistance function package. Please use rather st_distance sf consistency, units differ slightly. want use sf::st_distance, make sure convert units match scale greatCircleDistance function, otherwise results incorrect. Note can also use environmental distance matrix first two axes PCA, detailed . optimization routine requires least one ‘required’ site specified. select site closest geographic center sites required site. Normally can refer existing accessions, administrative units, nature preserves helping implement germplasm collection, fortunate enough already samples least guaranteed access. function bootstraps sites simulate true distribution species, also bootstraps coordinate uncertainty site. randomly assign 20% sites coordinate uncertainty 1 km 40 km. Note argument always meters.","code":"library(safeHavens) library(ggplot2) library(patchwork) set.seed(99) x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326) Warning: attribute variables are assumed to be spatially constant throughout all geometries n_sites <- nrow(x)  df <- data.frame(   site_id = seq_len(n_sites),   required = FALSE,   coord_uncertainty = 0,    lon = sf::st_coordinates(x)[,1],    lat = sf::st_coordinates(x)[,2] )  head(df)   site_id required coord_uncertainty      lon      lat 1       1    FALSE                 0 -65.4000 -10.3833 2       2    FALSE                 0 -65.3833 -10.3833 3       3    FALSE                 0 -65.1333 -16.8000 4       4    FALSE                 0 -63.6667 -17.4500 5       5    FALSE                 0 -63.8500 -17.4000 6       6    FALSE                 0 -64.4167 -16.0000 dist_mat <- sapply(1:nrow(df), function(i) {    greatCircleDistance(      df$lat[i], df$lon[i],      df$lat, df$lon    )  }) dists2c <- greatCircleDistance(   median(df$lat),    median(df$lon),    df$lat,    df$lon ) df[order(dists2c)[1],'required'] <- TRUE uncertain_sites <- sample(   setdiff(seq_len(n_sites),    which(df$required)),    size = round(n_sites*0.2, 0)   ) df$coord_uncertainty[uncertain_sites] <- runif(length(uncertain_sites), 1000, 40000) # meters"},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"run-kmedoidsbasedsample-based-only-on-geographic-distances","dir":"Articles","previous_headings":"","what":"Run KMedoidsBasedSample based only on geographic distances","title":"Rare Species Sampling Schema","text":"input function distance matrix, site data. funtion KMedoidsBasedSample several arguments used control run parameters. function operates relatively quick bootstraps sites, take smidge time longer complex scenarios. recommened using least 999 bootstraps real world applications.","code":"test_data <- list(   distances = dist_mat,   sites = df   )  str(test_data) List of 2  $ distances: num [1:116, 1:116] 0 1.83 714.09 807.71 797.93 ...  $ sites    :'data.frame':  116 obs. of  5 variables:   ..$ site_id          : int [1:116] 1 2 3 4 5 6 7 8 9 10 ...   ..$ required         : logi [1:116] FALSE FALSE FALSE FALSE FALSE FALSE ...   ..$ coord_uncertainty: num [1:116] 0 0 0 0 0 ...   ..$ lon              : num [1:116] -65.4 -65.4 -65.1 -63.7 -63.9 ...   ..$ lat              : num [1:116] -10.4 -10.4 -16.8 -17.4 -17.4 ...  rm(x, n_sites, uncertain_sites, dists2c) st <- system.time( {     geo_res <- KMedoidsBasedSample(  ## reduce some parameters for faster run.        input_data = test_data,       n = 5,       n_bootstrap = 10,       dropout_prob = 0.1,       n_local_search_iter = 10,       n_restarts = 2     )   } ) Sites: 116 | Seeds: 1 | Requested: 5 | Coord. Uncertain: 19 | BS Replicates: 10   |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |==============                                                        |  20%  |                                                                              |=====================                                                 |  30%  |                                                                              |============================                                          |  40%  |                                                                              |===================================                                   |  50%  |                                                                              |==========================================                            |  60%  |                                                                              |=================================================                     |  70%  |                                                                              |========================================================              |  80%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100% st    user  system elapsed   27.326   0.017  27.346"},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"return-output-structure","dir":"Articles","previous_headings":"Run KMedoidsBasedSample based only on geographic distances","what":"return output structure","title":"Rare Species Sampling Schema","text":"Five objects returned function. stability score shows often frquently selected network sites selected bootstrapped runs. stability data frame shows often site selected across bootstrap runs. Many users may find combindation input data columns, need carry results. Run parameters saved settings element.","code":"str(geo_res) List of 5  $ input_data     :'data.frame':    116 obs. of  10 variables:   ..$ site_id          : int [1:116] 47 21 5 83 100 6 106 19 95 86 ...   ..$ required         : logi [1:116] TRUE FALSE FALSE FALSE FALSE FALSE ...   ..$ coord_uncertainty: num [1:116] 0 0 0 37284 13617 ...   ..$ lon              : num [1:116] -74.3 -55.1 -63.9 -79.8 -74.1 ...   ..$ lat              : num [1:116] 4.58 -2.83 -17.4 9.17 -2.37 ...   ..$ certain          : logi [1:116] FALSE FALSE FALSE FALSE FALSE FALSE ...   ..$ cooccur_strength : num [1:116] 40 28 24 20 20 16 16 12 12 8 ...   ..$ is_seed          : logi [1:116] TRUE FALSE FALSE FALSE FALSE FALSE ...   ..$ selected         : logi [1:116] TRUE TRUE FALSE TRUE FALSE TRUE ...   ..$ sample_rank      : int [1:116] 1 2 3 4 4 5 5 6 6 7 ...  $ selected_sites : int [1:5] 6 21 47 83 106  $ stability_score: num 0.2  $ stability      :'data.frame':    116 obs. of  3 variables:   ..$ site_id         : int [1:116] 47 21 5 83 100 6 106 19 95 86 ...   ..$ cooccur_strength: num [1:116] 40 28 24 20 20 16 16 12 12 8 ...   ..$ is_seed         : logi [1:116] TRUE FALSE FALSE FALSE FALSE FALSE ...  $ settings       :'data.frame':    1 obs. of  4 variables:   ..$ n_sites     : num 5   ..$ n_bootstrap : num 10   ..$ dropout_prob: num 0.1   ..$ n_uncertain : int 19 head(geo_res$stability_score) [1] 0.2 head(geo_res$stability)     site_id cooccur_strength is_seed 47       47               40    TRUE 21       21               28   FALSE 5         5               24   FALSE 83       83               20   FALSE 100     100               20   FALSE 6         6               16   FALSE head(geo_res$input_data)     site_id required coord_uncertainty      lon      lat certain 47       47     TRUE              0.00 -74.3000   4.5833   FALSE 21       21    FALSE              0.00 -55.1333  -2.8333   FALSE 5         5    FALSE              0.00 -63.8500 -17.4000   FALSE 83       83    FALSE          37283.66 -79.8167   9.1667   FALSE 100     100    FALSE          13616.63 -74.0833  -2.3667   FALSE 6         6    FALSE              0.00 -64.4167 -16.0000   FALSE     cooccur_strength is_seed selected sample_rank 47                40    TRUE     TRUE           1 21                28   FALSE     TRUE           2 5                 24   FALSE    FALSE           3 83                20   FALSE     TRUE           4 100               20   FALSE    FALSE           4 6                 16   FALSE     TRUE           5 head(geo_res$settings)   n_sites n_bootstrap dropout_prob n_uncertain 1       5          10          0.1          19"},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"visualize-the-selection-results","dir":"Articles","previous_headings":"Run KMedoidsBasedSample based only on geographic distances","what":"visualize the selection results","title":"Rare Species Sampling Schema","text":"can plot required, selected sites.  can see, couple alternative sites close proximity selected sites also score highly. Functionally, serve subtitutes target sites. important combinations found articulated results.  believe many populations sampled can sampled, include ‘priority’ ranking results. Focus selected sites, opportunities sample beyond overlooked.","code":"map +    geom_point(data = geo_res$input_data,    aes(     x = lon,      y = lat,      shape = required,      size = cooccur_strength,     color = selected     )   ) +  # ggrepel::geom_label_repel(aes(label = site_id), size = 4) +    theme_minimal() +    labs(title = 'Priority Selection Status of Sites; Geographic Distances') map +    geom_point(data = geo_res$input_data,      aes(       x = lon,        y = lat,        shape = required,        size = -sample_rank,       color = sample_rank       )     ) +  # ggrepel::geom_label_repel(aes(label = sample_rank), size = 4) +   theme_minimal()"},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"run-kmedoidsbasedsample-with-environmental-distances","dir":"Articles","previous_headings":"","what":"run KMedoidsBasedSample with environmental distances","title":"Rare Species Sampling Schema","text":"mentioned, instead using geographic distance, can use environmental distance ordinated two dimensional space. analyst consider use variables know relevant species distrubution . However, sake example feed full stack environmental variables available dismo package.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"extract-prep-environmental-distances","dir":"Articles","previous_headings":"run KMedoidsBasedSample with environmental distances","what":"extract prep environmental distances","title":"Rare Species Sampling Schema","text":"First read rasters. environmental distances, use PCA transformation environmental variables. sample 100 random points raster layers calculate PCA, predict PCA raster layers across entire study area. take first two layers, pca, calculate environmental distances based two layers. see first two PCA axes explain large amount variance observed landscape.  keep first two PCA layers environmental distance calculation. layers increase dimensionality, may lead less useful results. Note ’s fine use euclidean distance calculation , values truly position pca plot.  ’ll ensure data proper matrix format feeding function. Similar run geographic distances, create input object, run function. run takes longer runs geographic distance matrix. environmental distance run takes 10 seconds longer. overall stability score similar geographic score. view plots see handful sites close proximity selected sites served nearly equivanently suitable substitutes. areas dropped priority sampling based method, general results pretty similar - relationship geographic environmental distance somewhat strong landscape.","code":"files <- list.files(   path = file.path(system.file(package=\"dismo\"), 'ex'),    pattern = 'grd',  full.names=TRUE ) predictors <- terra::rast(files) # import the independent variables rm(files) pts <- terra::spatSample(predictors, 100, na.rm = TRUE) pts <- pts[, names(pts)!='biome' ] # remove categorical variable for distance calc  pca_results <- stats::prcomp(pts, scale = TRUE) round(pca_results$sdev^2 / sum(pca_results$sdev^2), 2) # variance explained [1] 0.58 0.26 0.10 0.04 0.02 0.00 0.00 0.00 pca_raster <- terra::predict(predictors, pca_results) terra::plot(terra::subset(pca_raster, c(1:2))) # prediction of the pca onto a new raster rm(pts, predictors, pca_results) env_values <- terra::extract(pca_raster,    sf::st_coordinates(     sf::st_as_sf(       df,        coords = c('lon', 'lat'),        crs = 4326     )   ) )[,1:2] plot(env_values, main = 'environmental distance of points from first two PCA axis') env_dist_mat <- as.matrix(     dist(env_values)   )  rm(pca_raster) test_data <- list(   distances = env_dist_mat,   sites = df   )  st <- system.time(    {     env_res <- KMedoidsBasedSample(  ## reduce some parameters for shorter run time.       input_data = test_data,       n = 5,       n_bootstrap = 10,       dropout_prob = 0.1,       n_local_search_iter = 50,       n_restarts = 2     )   } ) Sites: 116 | Seeds: 1 | Requested: 5 | Coord. Uncertain: 19 | BS Replicates: 10   |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |==============                                                        |  20%  |                                                                              |=====================                                                 |  30%  |                                                                              |============================                                          |  40%  |                                                                              |===================================                                   |  50%  |                                                                              |==========================================                            |  60%  |                                                                              |=================================================                     |  70%  |                                                                              |========================================================              |  80%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%  rm(dist_mat, env_dist_mat) st    user  system elapsed   35.207   0.020  35.230 head(env_res$stability_score) [1] 0.2 map +    geom_point(data = env_res$input_data,      aes(       x = lon,        y = lat,        shape = required,        size = cooccur_strength,       color = selected       )     ) +  # ggrepel::geom_label_repel(aes(label = site_id), size = 4) +    theme_minimal() +    labs(title = 'Priority Selection Status of Sites; Environmental')"},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"alternative-methods-for-required-central-points","dir":"Articles","previous_headings":"","what":"alternative methods for required central points","title":"Rare Species Sampling Schema","text":"example use point median geographic center populations. can also identify population near highest density populations. Intuitively, suggested population high amount genetic diversity species, although unlikely accumulated considerably local changes effects drift overcome frequent dispersal. Alternatively can identify population near ‘center’ environmental variable space. Personally consider ‘pop centered’ population important required site center design . However, can suffer sampling bias, may want sense check recorded populations de-duplicated accomodate reality.","code":"dens <- with(df, MASS::kde2d(lon, lat, n = 200)) max_idx <- which(dens$z == max(dens$z), arr.ind = TRUE)[1,] max_point <- c(dens$x[max_idx[1]], dens$y[max_idx[2]])  pops_centre <- sweep(df[c('lon', 'lat')], 2, max_point, \"-\") pop_centered_id <- which.min(rowSums(abs(pops_centre^2)))  rm(dens, max_idx, max_point, pops_centre) env_centered <- sweep(env_values, 2, sapply(env_values, median), \"-\") env_centered_id <- which.min(rowSums(abs(env_centered^2)))  rm(env_values) # geographic centroid was pt 47 centers <- df[ c(env_centered_id, pop_centered_id, 47), ]  centers$type <- c('Environmental', 'Population', 'Geographic')  map +   geom_point(     data = df,      aes(x = lon, y = lat)     ) +    geom_point(     data = centers,       aes(x = lon, y = lat),     col = '#FF1493', size = 4     ) +    ggrepel::geom_label_repel(     data = centers,      aes(label = type, x = lon, y = lat)     ) +    theme_minimal() +    labs(title = 'Possbilities for centers') rm(env_centered_id, env_centered, pop_centered_id)"},{"path":[]},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"background","dir":"Articles","previous_headings":"Species Distribution Modelling","what":"Background","title":"Species Distribution Models","text":"vignette details steps required create Species Distribution Model (SDM) using functions provided safeHavens. SDM required use EnvironmentalBasedSample function, complex sampling scheme provided package. SDM created using elastic net generalized linear model implemented via glmnet package. SDM post-processed create binary raster map suitable unsuitable habitat, used rescale environmental predictor variables according contributions model. rescaled variables used clustering algorithm partition species range environmentally distinct regions germplasm sampling. goal SDM’s create model accurately predicts species located environmental space can predicted geographic space. goal models understand degree direction various environmental features correlate species observed range. model intended replace well-structured thought-SDM; rather, intended provide quick model can used inform sampling strategies.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"about","dir":"Articles","previous_headings":"Species Distribution Modelling","what":"About","title":"Species Distribution Models","text":"authors prone creating many ‘advanced’ SDMs models -house pipelines developing , think juice worth squeeze rely powerhouse R packages heavy lifting. main packages used dismo, caret, glmnet, CAST, terra. dismo package provides user-friendly interface working species occurrence raster data, well helper functions thresholding evaluating models. caret package provides nice interface working machine learning models, glmnet package provides elastic net model. CAST package provides functions spatially informed cross-validation, important SDMs. terra package provides functions working raster data. packages well documented, maintained.","code":""},{"path":[]},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"prep-data","dir":"Articles","previous_headings":"Steps to create an SDM","what":"prep data","title":"Species Distribution Models","text":"First need georeference occurrence data species interest. common species recommened rgbif package download occurrence data GBIF. maintain smaller package size, integrate tutorial dismo use built data set Bradypus variegatus (Brown-throated sloth). also leverage raster data provided dismo well.","code":"library(safeHavens) x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)  planar_proj <- 3857 # Web Mercator for planar distance calcs  files <- list.files(   path = file.path(system.file(package=\"dismo\"), 'ex'),    pattern = 'grd',  full.names=TRUE ) predictors <- terra::rast(files) # import the independent variables"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"fit-the-model","dir":"Articles","previous_headings":"Steps to create an SDM","what":"fit the model","title":"Species Distribution Models","text":"fit SDM using elasticSDM. arguments requires occurrence data x, raster stack predictors, quantile_v offset used create pseudo-absence data, planar_proj used calculate distances occurrence data possible pseudo-absence points. quantile_v used create pseudo-absence data calculating distance occurrence point nearest neighbor, selecting quantile distances create buffer around occurrence point. Points outside buffer used pseudo-absences. hood function uses caret help glmnet, provides output easy explore interact . Elastic net modelS bridge world lasso ridge regression blending alpha parameter. Lasso alpha 0, Ridge alpha 1. Lasso regression perform automated variable selection, can drop (‘shrink’) features model, whereas Ridge keep variables correlated features exist split contributions . elastic net blends propensity drop retain variables whenever used. caret test range alphas determine much ridge lasso regression characteristics best data hand. MAKE NOTE PCNM MORAN EIGNENVECTORS .","code":"sdModel <- elasticSDM(   x = x,   predictors = predictors,   quantile_v = 0.025,   planar_proj = planar_proj   ) Warning:  Grid searches over lambda (nugget and sill variances) with  minima at the endpoints:    (GCV) Generalized Cross-Validation     minimum at  right endpoint  lambda  =  1.656291e-07 (eff. df= 174.8 ) Warning:  Grid searches over lambda (nugget and sill variances) with  minima at the endpoints:    (GCV) Generalized Cross-Validation     minimum at  right endpoint  lambda  =  1.656291e-07 (eff. df= 174.8 ) Warning:  Grid searches over lambda (nugget and sill variances) with  minima at the endpoints:    (GCV) Generalized Cross-Validation     minimum at  right endpoint  lambda  =  1.656291e-07 (eff. df= 174.8 )"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"explore-the-output","dir":"Articles","previous_headings":"Steps to create an SDM > fit the model","what":"explore the output","title":"Species Distribution Models","text":"{r Explore SDM output - Different alpha}sdModel$CVStructure coef(sdModel$Model, s = \"lambda.min\") can see elastic net decided top model. used Accuracy rather Kappa main criterion model selection. can see selected model works predicting state test data. Note accuracy results slightly higher CV folds. bug, CV folds testing holdouts, brand new set holdouts. main reason confusion matrix results likely higher due spatial auto-correlation address typical ‘random split’ test train data. order minimize effects spatial-autocorrelation model use CAST hood allows spatially informed cross validation. Consider output CVStructure bit realistic.","code":"sdModel$ConfusionMatrix Confusion Matrix and Statistics            Reference Prediction  0  1          0 16  1          1  7 21                                                         Accuracy : 0.8222                          95% CI : (0.6795, 0.92)     No Information Rate : 0.5111             P-Value [Acc > NIR] : 1.465e-05                                                                 Kappa : 0.6464                                                   Mcnemar's Test P-Value : 0.0771                                                              Sensitivity : 0.9545                     Specificity : 0.6957                  Pos Pred Value : 0.7500                  Neg Pred Value : 0.9412                      Prevalence : 0.4889                  Detection Rate : 0.4667            Detection Prevalence : 0.6222               Balanced Accuracy : 0.8251                                                         'Positive' Class : 1"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"binarize-the-output","dir":"Articles","previous_headings":"Steps to create an SDM > fit the model","what":"binarize the output","title":"Species Distribution Models","text":"SDM’s produce continuous surfaces displaying predicted probability suitable habitat across landscape. However binary surfaes (Yes/Suitable), .e. less probable suitable habitat taxon exists particular location (grid cell)? often required ; function PostProcessSDM used binarize predictions. Going continuous surface binary surface loses information. goal binary surface understand species likely found, sample germplasm areas. many ways threshold SDM, many caveats associated process; Frank Harrell written topic post-processing statistics. Historically, assessing probability output 0.5 probability used threshold, probabilities beneath considered ‘Suitable / ’, probabilities classified ‘Suitable / Yes’. works cases, thresholding outside domain statistics realm practice. motto implementing models, slight elaboration George Box’s aphorism, “models wrong, useful - want wrong?”. goal sampling germplasm conservation maximize representation allelic diversity across range species. achieve , need understanding species actual range, hence better predict species present , predict absent actually grows. Accordingly, default thresholding statistic Sensitivity. However, function supports threshold options dismo::threshold. may wondering function named PostProcessSDM rather ThresholdSDM, reason discontinuity function performs additional processes thresholding. Geographic buffers created around intitial occurence points ensure none known occurrence points ‘missing’ output binary map. two edged sword, address notion dispersal limitation, realize suitable habitat occupied habitat. PostProcessSDM function creates cross validation folds, selects training data. fold calculates distance occurrence point ’s nearest neighbor. summarize distances can understand distribution distances quantile. use selected quantile, serve buffer. Area predicted suitable habitat outside buffer become ‘cut ’ (masked) binary raster map, areas within buffer distance known occurrences currently masked reassigned probabilities. theory behind process underdeveloped nascent, come gut analyst. Bradypus data set use 0.25 quantile, saying “Neighbors generally 100km apart, happy risk saying 25km within occurrence occupied suitable habitat”. Increasing value say 1.0 mean suitable habitat removed, decreasing makes maps conservative. cost increasing distances greatly sampling methods may puts grids many areas without populations collect . can compare results applying function side side using output function.","code":"terra::plot(sdModel$RasterPredictions) threshold_rasts <- PostProcessSDM(   rast_cont = sdModel$RasterPredictions,    test = sdModel$TestData,   train = sdModel$TrainData,   planar_proj = planar_proj,   thresh_metric = 'sensitivity',    quant_amt = 0.5   ) 1000 prediction points are sampled from the modeldomain terra::plot(threshold_rasts$FinalRasters)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"rescale-predictor-variables","dir":"Articles","previous_headings":"Steps to create an SDM > fit the model","what":"rescale predictor variables","title":"Species Distribution Models","text":"glmnet used three main reasons, 1) gives directional coefficients documents 1 unit increase independent variable varies response. 2) maintains automated feature selection reducing work analyst needs . 3) glmnet re-scales variables model generation combined beta-coefficients allows partitioning environmental space regions environmentally similar individual species. way raster stack becomes representative model, can use values basis hierarchical cluster later . create copy raster predictions standardized variable, equivalent input glmnet, mulitplied beta-coefficient. also write beta coefficients writeSDMresults function right afterwards.  can see variables ‘close’ scale, work clustering algorithm. layers color (maybe yellow?) means variance, ’s term shrunk model. dealt internally future function. scales variables exact, weighed coefficients model {r Show beta Coefficients model print(rr$BetaCoefficients) can also look coefficients variable. glmnet returns ‘untransformed’ variables, .e. coefficients scale input rasters, calculate BC right afterwards. safeHavens generates kinds things runs functions elasticSDM, PostProcessSDM, RescaleRasters. Given one sampling scheme may followed quite time, best practice save many objects.","code":"rr <- RescaleRasters(   model = sdModel$Model,   predictors = sdModel$Predictors,    training_data = sdModel$TrainData,    pred_mat = sdModel$PredictMatrix)  terra::plot(rr$RescaledPredictors)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"save-results","dir":"Articles","previous_headings":"Steps to create an SDM > fit the model","what":"save results","title":"Species Distribution Models","text":"","code":"bp <- '~/Documents/assoRted/StrategizingGermplasmCollections'  writeSDMresults(   cv_model = sdModel$CVStructure,    pcnm = sdModel$PCNM,    model = sdModel$Model,    cm = sdModel$ConfusionMatrix,    coef_tab = rr$BetaCoefficients,    f_rasts = threshold_rasts$FinalRasters,   thresh = threshold_rasts$Threshold,   file.path(bp, 'results', 'SDM'), 'Bradypus_test')  # we can see that the files were placed here using this.  list.files( file.path(bp, 'results', 'SDM'), recursive = TRUE )"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"wrapping-up","dir":"Articles","previous_headings":"Steps to create an SDM > fit the model","what":"wrapping up","title":"Species Distribution Models","text":", steps make species distribution model - rather get coefficients species distribution model! play around example data set compare buffered distance results end - head next vignette!","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/WorkedExample.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Worked Example","text":"previous tutorials focus individual functions package, little show integrate common workflows develop spatial products sampling. show minimal example curator may load occurrence data species, apply couple sampling approaches data, save results use. use two species native Southwestern United States & Mexico example, eventually narrowing focus discuss one sake brevity. two species example, code setup accomodate many species analyst desires process simultaneously.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/WorkedExample.html","id":"data-prep","dir":"Articles","previous_headings":"Introduction","what":"Data prep","title":"Worked Example","text":"need install rgbif follow along example. rgbif amazing package maintained ROpenSci acess Global Biodiversity Information Facility database within R. Note (free, esay get), GBIF profile required larger data downloads, example need R package. Using rgbif ’ll download occurrence data couple species, just code set mapping multiple species realistic manner. Take quick look data see clear errors. One points location incorrect, ’s latitude great - remove .  tools exist quickly cleaning GBIF data, gatoRs recently published great choice . Map data , keep ggplot around basemap rest vignette.","code":"library(safeHavens) library(sf) # spatial data library(dplyr) # general data handling library(tidyr) # general data handling library(purrr) # mapping functions across lists. library(ggplot2) ## for maps  library(rgbif) # species occurrence data. library(spData) # example cartography data ## small subset of useful columns for example cols = c('decimalLatitude', 'decimalLongitude', 'dateIdentified', 'species', 'acceptedScientificName', 'datasetName',    'coordinateUncertaintyInMeters', 'basisOfRecord', 'institutionCode', 'catalogNumber')  ## download species data using scientificName, can use keys and lookup tables for automating many taxa.  cymu <- rgbif::occ_search(scientificName = \"Vesper multinervatus\", limit = 1000)  ### check to see what CRS are in here, these days usually standardized to wgs84 (epsg:4326) table( cymu[['data']]['geodeticDatum'])  geodeticDatum WGS84    675   ## subset the data to relevant columns  cymu_cols <- cymu[['data']][,cols]  ## repeat this again so a second set of data are on hand  bowa <- rgbif::occ_search(scientificName = \"Bouteloua warnockii\", limit = 1000) bowa_cols <- bowa[['data']][,cols]  ## contrived multispecies example.  spp <- bind_rows(bowa_cols, cymu_cols) |>   drop_na(decimalLatitude, decimalLongitude) |> # any missing coords need dropped.    st_as_sf(coords = c( 'decimalLongitude', 'decimalLatitude'), crs = 4326, remove = F)  rm(cymu, bowa, cols, cymu_cols, bowa_cols) western_states <- spData::us_states |> ## for making a quick basemap.    dplyr::filter(REGION == 'West' & ! NAME %in% c('Montana', 'Washington', 'Idaho', 'Oregon', 'Wyoming') |    NAME %in% c('Oklahoma', 'Texas', 'Kansas')) |>   dplyr::select(NAME, geometry) |>   st_transform(4326)  ggplot() +   geom_sf(data = western_states) +   geom_sf(data = spp, aes(color = species, shape = species)) +   theme_void() +   theme(legend.position = 'bottom') ## check the outlying record.  arrange(spp, by = decimalLatitude, desc=FALSE) |>   head(5) Simple feature collection with 5 features and 10 fields Geometry type: POINT Dimension:     XY Bounding box:  xmin: -102.8461 ymin: 26.2436 xmax: -101.343 ymax: 26.30833 Geodetic CRS:  WGS 84  [38;5;246m# A tibble: 5 × 11 [39m   decimalLatitude decimalLongitude dateIdentified species acceptedScientificName              [3m [38;5;246m<dbl> [39m [23m             [3m [38;5;246m<dbl> [39m [23m  [3m [38;5;246m<chr> [39m [23m           [3m [38;5;246m<chr> [39m [23m    [3m [38;5;246m<chr> [39m [23m                   [38;5;250m1 [39m            26.2            - [31m103 [39m [31m. [39m  [31mNA [39m             Boutel… Bouteloua warnockii G…  [38;5;250m2 [39m            26.2            - [31m103 [39m [31m. [39m  [31mNA [39m             Boutel… Bouteloua warnockii G…  [38;5;250m3 [39m            26.2            - [31m103 [39m [31m. [39m  [31mNA [39m             Boutel… Bouteloua warnockii G…  [38;5;250m4 [39m            26.3            - [31m101 [39m [31m. [39m  [31mNA [39m             Boutel… Bouteloua warnockii G…  [38;5;250m5 [39m            26.3            - [31m101 [39m [31m. [39m  [31mNA [39m             Boutel… Bouteloua warnockii G…  [38;5;246m# ℹ 6 more variables: datasetName <chr>, coordinateUncertaintyInMeters <dbl>, [39m  [38;5;246m#   basisOfRecord <chr>, institutionCode <chr>, catalogNumber <chr>, [39m  [38;5;246m#   geometry <POINT [°]> [39m  ## remove it based on it's latitude.  spp <- filter(spp, decimalLatitude <= 40) bb <- st_transform(spp, 5070) |>   st_buffer(100000) |>   st_transform(4326) |>   st_bbox()  western_states <- st_crop(western_states, bb)  base <- ggplot() +   geom_sf(data = western_states, color = 'white') +   geom_sf(data = spp, aes(color = species, fill = species)) +   theme_void() +   theme(legend.position = 'bottom')  base"},{"path":"https://sagesteppe.github.io/safeHavens/articles/WorkedExample.html","id":"using-safehavens","dir":"Articles","previous_headings":"Introduction","what":"using safeHavens","title":"Worked Example","text":"Now use safeHavens functionality set-environment data GBIF.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/WorkedExample.html","id":"create-species-ranges","dir":"Articles","previous_headings":"Introduction > using safeHavens","what":"Create species ranges","title":"Worked Example","text":"showcase two methods generating species range geometries, used packages functions. rely st_concave_hull function sf. st_concave_hull, ratio parameter can control elasticity hulls, ratio = 1 creating convex hull (akin st_convex_hull function, also sf), ratio = 0.0, creating true concave hull. Results ratio 0.4 look narrowly reduced species. Visualize concave hulls.  Visualize convex hulls.  sake example use concave ranges, ratio 0.4, going forward.","code":"sppL <- split(spp, f = spp$species)  concavities <- function(x, d, rat){    species <- x[['species']][1]   out <- st_transform(x, 5070) |>     st_buffer(dist = d) |>     st_union() |>      st_concave_hull(ratio = rat) |>      st_sf() |>     rename(geometry = 1) |>     mutate(species, .before = geometry)  }  spp_concave <- sppL |>   purrr::map(~ concavities(.x, d = 20000, rat = 0.4)) spp_convex <- sppL |>   purrr::map(~ concavities(.x, d = 20000, rat = 1.0))  rm(concavities) base +   geom_sf(data = spp_concave[[1]], aes(fill = species), alpha = 0.2) +   geom_sf(data = spp_concave[[2]],  aes(fill = species), alpha = 0.2) base +   geom_sf(data = spp_convex[[1]],  aes(fill = species), alpha = 0.2) +   geom_sf(data = spp_convex[[2]], aes(fill = species), alpha = 0.2) rm(spp_convex)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/WorkedExample.html","id":"perform-sampling-for-the-species-ranges","dir":"Articles","previous_headings":"Introduction","what":"Perform sampling for the species ranges","title":"Worked Example","text":"First perform equal area sampling relatively small amount points repetitions.  plot shows results Vesper multinervatus. perform isolation distance (IBD) based sampling.  plot isolation--distance sampling shows individual sample areas Bouteloua warnockii.","code":"eas <- spp_concave |>   purrr::map(~ EqualAreaSample(.x, n = 10, pts = 250, planar_proj = 5070, reps = 25))  base +   geom_sf(data = eas[[2]][['Geometry']], aes(fill = factor(ID)), alpha = 0.2) # create an arbitrary template for example - best to do this over your real range of all species collections # so that it can be recycled across species.  template <- terra::rast(terra::ext(bb), crs = terra::crs(spp), resolution = c(0.1, 0.1)) terra::values(template) <- 0  # the species range now gets 'burned' into the raster template.  spp_concave <- purrr::map(spp_concave, \\(x) {   st_as_sf(x) |>      mutate(Range = 1, .before = geometry) |>     st_transform(4326) |>      terra::rasterize(template, field = 'Range')  })  ## the actual sampling happens here within `map` ibd_samples <- spp_concave |>   purrr::map(~ IBDBasedSample(.x, n = 10, fixedClusters = FALSE, template = template, planar_proj = 5070))  base + ## visualize for a single taxon.    geom_sf(data = ibd_samples[[1]][['Geometry']], aes(fill = factor(ID)), alpha = 0.2) rm(spp_concave)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/WorkedExample.html","id":"prioritize-sample-areas","dir":"Articles","previous_headings":"Introduction > Perform sampling for the species ranges","what":"prioritize sample areas","title":"Worked Example","text":"addition creating spatial geometries can used guide sampling efforts, safeHavens can also help provide visualize guidance general areas germplasm can sampled try maximize distance samples. Please note safeHavens can offer suggestions sample, prioritized suggestions may align reality - consider rules thumbs! made, couple hundred large native seed collections, coordinated couple hundred , many years - argue beggers choosers. Hence basing deliverable metrics purely around data like can difficult - degree reporting autonomy must maintained. function PrioritizeSample offers two levels prioritization. coarse level suggest relative order sample areas sampled , 1:n; function seeks minimize variance ( related measure) sample samples collected. effect tries stratify sampling across species range. second part function almost ‘heamap’ can used show concentric rings around geographic center sample unit. Keeping number rings low allows pragmatic communication desirable/less desirable portions range ideally collect .  map shows possible general order guide prioritization individual sample areas, simplified visuals within sample areas attempt target.","code":"ibd_samples_priority <- ibd_samples %>%   purrr::map(~ st_transform(.x$Geometry, 5070)) |>    purrr::map(~ list(PrioritizeSample(.x, n_breaks = 3)))  base +    geom_sf(data = ibd_samples_priority[[2]][[1]][['Geometry']], aes(fill = factor(Level)), alpha = 0.2)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/WorkedExample.html","id":"wrapping-up","dir":"Articles","previous_headings":"Introduction > Perform sampling for the species ranges","what":"wrapping up","title":"Worked Example","text":"Upon completion using safeHavens results written individual geopackages long term storage. strong benefit geopackage ’s ability hold multiple geometry types (polygons, points, rasters, etc.), can sometimes lost kept separate directories. show write data range species.","code":"## create a directory to hold the outputs p2Collections <- file.path('~', 'Documents', 'WorkedExample_Output') dir.create(p2Collections, showWarnings = FALSE)  ## we will only save the template raster once since it is recycled across taxa.  dir.create(file.path(p2Collections, 'IBD_raster_template'), showWarnings = FALSE) terra::writeRaster(template,    filename = file.path(p2Collections, 'IBD_raster_template', 'IBD_template.tif'), overwrite = FALSE)  ## save each species as a unique geopackage.  for(i in seq_along(sppL)){    fp = file.path(p2Collections, paste0(gsub(' ', '_', sppL[[i]]$species[1]), '.gpkg'))    ### GBIF occurrence points   st_write(sppL[[i]], dsn = fp, layer =  'occurrence_points', quiet = TRUE)    ### results of equal area sampling     st_write(eas[[i]]$Geometry,  dsn = fp, layer =  'equal_area_samples', quiet = TRUE, append = TRUE)    ### ibd based sample (note can be reconstruced from the hulls of ibd samples priority)   st_write(ibd_samples[[i]]$Geometry, dsn = fp, layer =  'ibd_samples', quiet = TRUE, append = TRUE)    ### prioritized information for the IBD samples.    st_write(ibd_samples_priority[[i]][[1]]$Geometry, dsn = fp, layer =  'ibd_sampling_priority', quiet = TRUE, append = TRUE)    message(format(object.size(fp), standard = \"IEC\", units = \"MiB\", digits = 4)) } Warning in rm(spp, sppL, eas, ibd_samples, ibd_samples_priority, base, template, : object 'p2Collections' not found Warning in rm(spp, sppL, eas, ibd_samples, ibd_samples_priority, base, template, : object 'fp' not found Warning in rm(spp, sppL, eas, ibd_samples, ibd_samples_priority, base, template, : object 'i' not found"},{"path":"https://sagesteppe.github.io/safeHavens/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Reed Benkendorf. Author, maintainer.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Benkendorf R (2026). safeHavens: Developing sampling schemas ex situ conservation plant germplasm. R package version 0.0.0.9000, https://sagesteppe.github.io/safeHavens/.","code":"@Manual{,   title = {safeHavens: Developing sampling schemas for ex situ conservation of plant germplasm},   author = {Reed Benkendorf},   year = {2026},   note = {R package version 0.0.0.9000},   url = {https://sagesteppe.github.io/safeHavens/}, }"},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"safehavens-","dir":"","previous_headings":"","what":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","title":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","text":"goal package provide germplasm curators easily referable spatial data sets help prioritize field collection efforts.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","text":"provides functionality seven sampling schemes various curators interested , many likely outperform others certain species areas. package also creates species distribution models, goal germplasm sampling, rather predicting ranges fine resolutions, making inference; interested functionality R several dozen packages tailored purposes.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"description","dir":"","previous_headings":"","what":"Description","title":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","text":"package helps germplasm curators communicate areas interest collection teams collect new material accession. provides seven different sampling approaches curators choose individual taxon hope process.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","text":"safeHavens available github. can installed using remotes devtools like : installed can attached use like package github CRAN","code":"install.packages('devtools') devtools::install_github('sagesteppe/safeHavens')  install.packages('remotes') # remotes is very similar and  a good alternativ for this use case. remotes::install_github('sagesteppe/safeHavens') library(safeHavens)"},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","text":"safeHavens seven user facing functions generating sampling schemes.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"available-sampling-schemes","dir":"","previous_headings":"Usage","what":"Available Sampling Schemes","title":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","text":"following table shows eight sampling approaches available safeHavens, computational complexity (Comp.) environmental data requirements (Envi.): L = Low, M = Medium, H = High. species distribution modelling section couple functions essential achieving EnvironmentalBasedSample design, : elasticSDM, PostProcessSDM, RescaleRasters writeSDMresults.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"acknowledgements","dir":"","previous_headings":"","what":"Acknowledgements","title":"safeHavens: Ex Situ Plant Germplasm Sampling Strategies","text":"Edzer Pebesma, Krzysztof Dyba help seamless installations Ubuntu, getting pkgdown website running.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"function utilizes output elastic net GLM model create weights matrix features relevant species distribution identify clusters throughout range incorporating PCNM/MEM data coordinates implement spatial contiguity.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"","code":"EnvironmentalBasedSample(   pred_rescale,   f_rasts,   lyr = \"Supplemented\",   taxon,   path = \".\",   n = 20,   fixedClusters = TRUE,   n_pts = 500,   planar_proj,   coord_wt = 2.5,   buffer_d = 3,   prop_split = 0.8,   write2disk = FALSE,   method = \"complete\",   min.nc = 3,   max.nc = 20,   ... )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"pred_rescale rasterstack predictor layers rescaled represent beta coefficients elastic net (glmnet::glmnet) modelling process. See ?RescaleRasters implementation functionality. f_rasts rasters output SDM workflow. lyr Character. name layer want use analysis one : 'Threshold', 'Clipped', Supplemented'. missing defaults 'Supplemented'. taxon Character. name taxonomic entity models created. final raster clusters, results KNN classifier trainings, details clustering procedure (fixedClusters=TRUE). path root path output data saved, use WriteSDMresults. Defaults current working directory, check getwd(). n Numeric. number clusters desired. fixedClusters Boolean. Defaults TRUE, create n clusters. False use NbClust::NbClust determine number clusters. n_pts Numeric. number points use generating clusters, randomly sampled within mask area mask. Defaults 500. planar_proj Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. coord_wt Numeric. amount weigh coordinates distance matrix relative important variable identified elastic net regression. Defaults 2.5, setting 1 make value equivalent environmental PCNM variables. metric increases spatial contiguity clusters identified. buffer_d Numeric. using two-stage sampling increase sample size (number points) uncommon clusters, distance buffer individual pts located cells possible sampling. Defaults 3 allows area encompassing around 45-50 raster cells nearby possibly sampled. reasonable default coarsely gridded data (sensible grain type packages use cases), moderate resolution data (e.g. 250m-1km) may require higher values find meaningful differences variables locations. prop_split Numeric. proportion records used training KNN classifier. Defaults 0.8 use 80% records training 20% independent test sample. write2disk Boolean. Whether write results disk . Defaults FALSE. method character passed onto NbClust fixedClusters == TRUE. min.nc = 3 Passed onto NbClust fixedClusters == TRUE, minimum number clusters accept. max.nc = 20 Passed onto NbClust fixedClusters == TRUE, maximum number clusters accept. ... arguments passed NbClust::NbClust optimizing cluster numbers. Defaults using method = 'complete', compare results 20 methods select cluster number commonly generated algorithms. min.nc set default 5, overcome clusters use (easily overwritten supplying argument minc.nc=2, set lower), max.nc = 20 congruence many seed collection endeavors (overwritten max.nc = 10 example).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"Writes four objects disk, returns one object R session (optional).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Create equal area polygons over a geographic range — EqualAreaSample","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"function creates n geographic clusters geographic area (x), typically species range, using kmeans clustering.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"","code":"EqualAreaSample(   x,   n = 20,   pts = 5000,   planar_proj,   returnProjected,   reps = 100,   BS.reps = 9999 )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"x SF object terra spatraster. range generate clusters. n Numeric. number clusters desired. Defaults 20. pts Numeric. number points use generating clusters, placed grid like fashion across x. exact number points used may deviate slightly user submitted value allow equidistant spacing across x. Defaults 5,000. planar_proj Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. returnProjected Boolean. Whether return data set original input CRS (FALSE), new projection (TRUE). Defaults FALSE. reps Numeric. number times rerun voronoi algorithm, set polygons similar sizes, measured using variance areas selected. Defaults 100. BS.reps number bootstrap replicates evaluating results.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"list containing two objects, first results bootstrap simulations. second sf dataframe containing polygons smallest amount variance size.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"","code":"nc <- sf::st_read(system.file(\"shape/nc.shp\", package=\"sf\"), quiet = TRUE) |> dplyr::select(NAME)  set.seed(1) system.time(   zones <- EqualAreaSample(nc, n = 20, pts = 500, planar_proj = 32617, reps = 50) ) #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #>    user  system elapsed  #>   5.192   0.039   5.231   plot(nc, main = 'Counties of North Carolina')  plot(zones$Geometry, main = 'Clusters')  zones$SummaryData #>                  Metric       Value #> 1     variance.observed  9507223610 #> 2        quantile.0.001  9565302650 #> 3             lwr.95.CI  9507223610 #> 4             upr.95.CI 10692510134 #> 5    Voronoi.reps.asked          50 #> 6 Voronoi.reps.received          50 #> 7               BS.reps        9999"},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"Create n seed collection areas based distance geographic (great circle) distance points.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"","code":"IBDBasedSample(   x,   n,   fixedClusters = TRUE,   n_pts = 1000,   template,   prop_split = 0.8,   min.nc = 5,   max.nc = 20,   planar_proj )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"x Raster surface sample points within, e.g. output SDM$Supplemented. n Numeric. number clusters desired. fixedClusters Boolean. Defaults TRUE, create n clusters. False use NbClust::NbClust determine optimal number clusters. n_pts Numeric. number points use generating clusters, randomly sampled within mask area mask. Defaults 1000, generally allows enough points split KNN training. template Raster. raster file can used template plotting. prop_split Numeric. proportion records used training KNN classifier. Defaults 0.8 use 80% records training 20% independent test sample. min.nc Numeric. Minimum number clusters test fixedClusters=FALSE, defaults 5. max.nc Numeric. Maximum number clusters test fixedClusters=FALSE, defaults 20. planar_proj Numeric. Optional. planar projection use sf::st_point_on_surface ensure valid spatial operations.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"simple features (sf) object containing final grids saving computer. See vignette questions saving two main types spatial data models (vector - used , raster).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"","code":"planar_proj <- \"+proj=laea +lat_0=-15 +lon_0=-60 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"  x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326) x_buff <- sf::st_transform(x, planar_proj) |>  sf::st_buffer(125000) |> # we are working in planar metric coordinates, we are  sf::st_as_sfc() |> # buffer by this many / 1000 kilometers.  sf::st_union()  files <- list.files( # note that for this process we need a raster rather than   path = file.path(system.file(package=\"dismo\"), 'ex'), # vector data to accomplish   pattern = 'grd',  full.names=TRUE ) # this we will 'rasterize' the vector using terra predictors <- terra::rast(files) # this can also be done using 'fasterize'. Whenever # we rasterize a product, we will need to provide a template raster that our vector # will inherit the cell size, coordinate system, etc. from  x_buff.sf <- sf::st_as_sf(x_buff) |>   dplyr::mutate(Range = 1) |>   sf::st_transform(terra::crs(predictors))  # and here we specify the field/column with our variable we want to become # an attribute of our raster v <- terra::rasterize(x_buff.sf, predictors, field = 'Range')  # now we run the function demanding 20 areas to make accessions from, ibdbs <- IBDBasedSample(x = v, n = 20, fixedClusters = TRUE,    template = predictors, planar_proj = planar_proj) #> Loading required package: ggplot2 #> Loading required package: lattice plot(ibdbs[['Geometry']])"},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBRSurface.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample a species based on Isolation by Resistance Distance (IBR) — IBRSurface","title":"Sample a species based on Isolation by Resistance Distance (IBR) — IBRSurface","text":"Create n seed collection areas based habitat, geographic, resistance points.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBRSurface.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample a species based on Isolation by Resistance Distance (IBR) — IBRSurface","text":"","code":"IBRSurface(   base_raster,   pop_raster,   resistance_surface,   pts_sf,   ibr_matrix,   fixedClusters = FALSE,   n = NULL,   min.nc = 5,   max.nc = 20,   planar_proj,   max_geo_cells = NULL,   geo_iters = 10,   boundary_width = 2,   distance_method = c(\"haversine\", \"cosine\") )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBRSurface.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample a species based on Isolation by Resistance Distance (IBR) — IBRSurface","text":"base_raster Raster surface sample points within, .... pop_raster Spatiall buffered population raster populationResistance. resistance_surface resistance surface raster  build_resistance_surface populationResistance. pts_sf sf/tibble/dataframe point locations populationResistance $pts_sf ibr_matrix Distance matrix, populationResistance slot $Rmat fixedClusters Boolean. Defaults FALSE, create n clusters. False use NbClust::NbClust determine optimal number clusters. TRUE yet suported. n Numeric. number clusters desired. min.nc Numeric. Minimum number clusters test fixedClusters=FALSE, defaults 5. max.nc Numeric. Maximum number clusters test fixedClusters=FALSE, defaults 20. planar_proj numbering clusters return object. max_geo_cells Numeric. Maximum number cells grow cheap geographic front growth cluster centers . geo_iters Numeric. Maximum number iterations grow hulls . boundary_width Numeric. Maximum boundary width switching final assignment method. distance_method Great circle distance calculation method passed onto ot terra. One 'haversine' (default), 'cosine'.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBRSurface.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample a species based on Isolation by Resistance Distance (IBR) — IBRSurface","text":"","code":"# see package vignette"},{"path":"https://sagesteppe.github.io/safeHavens/reference/KMedoidsBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"K Medoids Based Sample Site Selection Select a subset of sites that maximize spatial dispersion of sites using k-medioids clustering. — KMedoidsBasedSample","title":"K Medoids Based Sample Site Selection Select a subset of sites that maximize spatial dispersion of sites using k-medioids clustering. — KMedoidsBasedSample","text":"function operates individual points - representing populations, rather drawing convex hulls polygons around emulate species range. designed rare species, individual populations relatively scarce, e.g. < 100, decent location data. perform bootstrap re-sampling better estimate true range extent species, well coordinate jittering better address geo-location quality. running n_bootstrap simulations identify individual networks sites (co-location) resilient perturbations, less affected data quality issues. arguments takes known locations populations, solve n priority collection sites. Along process also generate priority ranking sites, indicating naive possible order prioritizing collections; although opportunity never discard site. required input parameter column indicating whether site required. Required sites (1 - many < n_sites) serve fixed parameters optimization scenario greatly speed run time. can represent: existing collections, collections strong chance happenging due funding agency mechanism, otherwise single population closet geographic center species. Notably solve 'around' site, hence solves purely theoretical, linked pragmatic element. One can substitute geographic distance matrix either resistance environmental distance matrix. However, function internally recalculate distances bootstrapped points. See vignette example creating quick environmental distance matrix using simple PCA bioclim variables. Note input data require two boolean (TRUE/FALSE) columns, 'required' 'certain', function run. 'required' notes sites , sampled germplasm collections, sites default required. 'certain' notes user confident taxon hand; default FALSE, meaning sites except 'required' sites dropped simulations.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/KMedoidsBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"K Medoids Based Sample Site Selection Select a subset of sites that maximize spatial dispersion of sites using k-medioids clustering. — KMedoidsBasedSample","text":"","code":"KMedoidsBasedSample(   input_data,   n = 5,   n_bootstrap = 999,   dropout_prob = 0.1,   n_local_search_iter = 100,   n_restarts = 3,   verbose = TRUE,   distance_type = \"geographic\",   min_jitter_dist = 10000 )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/KMedoidsBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"K Medoids Based Sample Site Selection Select a subset of sites that maximize spatial dispersion of sites using k-medioids clustering. — KMedoidsBasedSample","text":"input_data list two elements: 'distances' (distance matrix) 'sites' (data frame site metadata). n number sites want select priority collection. Note results return rank prioritization sites data. n_bootstrap Number bootstrap replicates perform. dropout_prob Probability dropping non-seed sites bootstrap replicate, give sites generally keep 0.2. Set 0 disable dropout. n_local_search_iter Number local search iterations per restart. n_restarts Number random restarts per bootstrap replicate. verbose Whether print progress information. print message run settings, progress bar bootstraps. distance_type Character. Defaults 'geographic', otherwise 'environmental'. geographic coordinate uncertainty greater min_jitter_dist coordinate jittering performed. min_jitter_dist Minimum coordinate uncertainty (meters) initiate jittering site coordinates.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/KMedoidsBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"K Medoids Based Sample Site Selection Select a subset of sites that maximize spatial dispersion of sites using k-medioids clustering. — KMedoidsBasedSample","text":"","code":"if (FALSE) { # \\dontrun{ library(ggplot2)   ### create sample data  n_sites <- 30 # number of known populations  df <- data.frame(    site_id = seq_len(n_sites),    lat = runif(n_sites, 25, 30), # play with these to see elongated results.    lon = runif(n_sites, -125, -120),    required = FALSE,    coord_uncertainty = 0  )  #function can accept a required point, here arbitrarily place near geographic center  dists2c <- greatCircleDistance(    median(df$lat),    median(df$lon),    df$lat,    df$lon  )  df[order(dists2c)[1],'required'] <- TRUE   ## we will simulate coordinate uncertainty on a number of sites.  uncertain_sites <- sample(setdiff(seq_len(n), which(df$required)), size = min(6, n_sites-3))  df$coord_uncertainty[uncertain_sites] <- runif(length(uncertain_sites), 5000, 100000) # meters   # the function can take up to take matrices. the first (required) is a geographic distance  # matrix. calculate this with the `greatCircleDistance` fn from the package for consistency.  # (it will be recalculated during simulations). `sf` gives results in slightly diff units.  dist_mat <- sapply(seq_len(nrow(df)), function(i) {    greatCircleDistance(      df$lat[i], df$lon[i],      df$lat, df$lon    )  })   # the input data is a list, the distance matrix, and the df of actual point locations.  head(df)   test_data <- list(distances = dist_mat, sites = df)  rm(dist_mat, df, n, uncertain_sites, dists2c)   # small quick run    system.time(      res <- maximizeDispersion(  ## reduce some parameters for faster run.        input_data = test_data,        n_bootstrap = 500,        n_local_search_iter = 50,        n_restarts = 2      )    )  ### first selected  ggplot(data = res$input_data,    aes(      x = lon,      y = lat,      shape = required,      size = cooccur_strength,      color = selected      )    ) +    geom_point() +  #  ggrepel::geom_label_repel(aes(label = site_id), size = 4) +    theme_minimal() +    labs(main = 'Priority Selection Status of Sites')  ### order of sampling priority ranking plot.  ggplot(data = res$input_data,    aes(      x = lon,      y = lat,      shape = required,      size = -sample_rank,      color = sample_rank      )    ) +    geom_point() + #   ggrepel::geom_label_repel(aes(label = sample_rank), size = 4) +    theme_minimal() } # }"},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Design additional collections around already existing collections — OpportunisticSample","title":"Design additional collections around already existing collections — OpportunisticSample","text":"function utilizes regular, nearly case existing collections, grid points develop sampling scheme n polygons.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Design additional collections around already existing collections — OpportunisticSample","text":"","code":"OpportunisticSample(polygon, n = 20, collections, reps = 100, BS.reps = 9999)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Design additional collections around already existing collections — OpportunisticSample","text":"polygon input sf polygon, .e. species range administrative unit, sampling desired. n Numeric. total number desired collections. Defaults 20. collections sf point geometry data set existing collections made. reps arguments passed np.boot BS.reps number bootstrap replicates evaluating results.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Design additional collections around already existing collections — OpportunisticSample","text":"list containing two sublists, first 'SummaryData' details number voronoi polygons generated, results bootstrap simulations. second 'Geometry', contains final spatial data products, can written end. See vignette questions saving two main types spatial data models (vector - used , raster).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Design additional collections around already existing collections — OpportunisticSample","text":"","code":"#' Design additional collections around already existing collections ri <- spData::us_states |>   dplyr::select(NAME) |>   dplyr::filter(NAME == 'Rhode Island') |>   sf::st_transform(32617) existing_collections <- sf::st_sample(ri, size = 5) |>   sf::st_as_sf() |>   dplyr::rename(geometry = x)  system.time(   out <- OpportunisticSample(polygon = ri, collections = existing_collections, BS.reps=999) ) #>    user  system elapsed  #>   6.693   0.026   6.723  ggplot2::ggplot() +   ggplot2::geom_sf(data = out$Geometry, ggplot2::aes(fill = ID)) +   ggplot2::geom_sf(data = existing_collections)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"function utilizes regular grid points develop sampling scheme n polygons.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"","code":"PointBasedSample(polygon, n = 20, reps = 100, BS.reps = 9999)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"polygon input sf polygon, .e. species range administrative unit, sampling desired. n Numeric. total number desired collections. Defaults 20. reps arguments passed np.boot BS.reps number bootstrap replicates evaluating results.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"list containing two objects, first results bootstrap simulations. second sf dataframe containing polygons smallest amount variance size.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"","code":"#' Utilize a grid based stratified sample for drawing up polygons ri <- spData::us_states |>   dplyr::select(NAME) |>   dplyr::filter(NAME == 'Rhode Island') |>   sf::st_transform(32617)   system.time(   out <- PointBasedSample(polygon = ri, reps = 10, BS.reps = 10) # set very low for example  ) #>    user  system elapsed  #>   0.597   0.003   0.600  # the function is actually very fast; 150 voronoi reps, with 9999 BS should only take about # 2 seconds per species so not much concern on the speed end of things! head(out$SummaryData) #>                  Metric    Value #> 1     variance.observed 16074215 #> 2        quantile.0.001 16084226 #> 3             lwr.95.CI 16074215 #> 4             upr.95.CI 16084384 #> 5    Voronoi.reps.asked       10 #> 6 Voronoi.reps.received        6 plot(out$Geometry)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PolygonBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample spatial zones within a species range — PolygonBasedSample","title":"Sample spatial zones within a species range — PolygonBasedSample","text":"Intersect vector data file spatial zones (ecoregions, provisional seed transfer zones, spatial partitions) range focal taxon select n zones sample. fewer n zones exist, extra samples allocated using specified method. n zones exist, zones selected using specified method.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PolygonBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample spatial zones within a species range — PolygonBasedSample","text":"","code":"PolygonBasedSample(   x,   zones,   zone_key,   n = 20,   decrease_method = c(\"Largest\", \"Smallest\", \"Most\", \"Assist-warm\", \"Assist-drier\"),   increase_method = c(\"Largest\", \"Smallest\", \"Most\", \"Assist-warm\", \"Assist-drier\"),   warmest_col = NULL,   precip_col = NULL )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PolygonBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample spatial zones within a species range — PolygonBasedSample","text":"x sf object. Species range simple feature. zones sf object. Spatial zones vector data (ecoregions, PSTZs, etc.). zone_key Character. Column name identifying unique zones (required). n Numeric. Desired total number samples. Default = 20. decrease_method Character. Method n < number zones. One : \"Largest\", \"Smallest\", \"\", \"Assist-warm\", \"Assist-drier\". Default = \"Largest\". increase_method Character. Method n > number zones. One : \"Largest\", \"Smallest\", \"\", \"Assist-warm\", \"Assist-drier\". Default = \"Largest\". warmest_col Character. Column name warmest temperature metric (required \"Assist-warm\" method). Higher values column assumed warmer. precip_col Character. Column name precipitation metric (required \"Assist-drier\" method). Lower values column assumed drier.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PolygonBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample spatial zones within a species range — PolygonBasedSample","text":"sf object selected zones/polygons allocation column indicating number samples per polygon.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PolygonBasedSample.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sample spatial zones within a species range — PolygonBasedSample","text":"Simple features can store polygon data 'MULTIPOLYGON' (polygons class stored collectively) 'POLYGON' (individual polygon unique entry). function cast MULTIPOLYGONS POLYGONS needed, pre-casting improve performance. Available methods zone selection: Largest: Select zones total area (descending) Smallest: Select zones total area (ascending) : Select zones polygons (highest fragmentation) Assist-warm: Select warmest zones (requires warmest_col) Assist-drier: Select driest zones (requires precip_col)","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PolygonBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample spatial zones within a species range — PolygonBasedSample","text":"","code":"if (FALSE) { # \\dontrun{ library(tidyverse)  sr_mat <- rbind(   c(0,0), c(10,0), c(10,10), c(0,10), c(0,0) ) sr_poly <- sf::st_polygon(list(sr_mat)) x <- sf::st_sf(id = 1, geometry = sf::st_sfc(sr_poly))  rm(sr_mat, sr_poly)  zone_polys = data.frame(   ## randomly generate some points in XY space.   x = runif(10, min = -2, max = 12),   y = runif(10, min = -2, max = 12) ) |>   # conver to spatial points   sf::st_as_sf(coords = c('x', 'y')) |>   ## allocate XY space to it's nearest point   sf::st_union() |>   sf::st_voronoi() |>   ## extract the contiguous pieces of XY space around points   sf::st_collection_extract('POLYGON') |>   sf::st_as_sf() |>   ## make up seed zones on the fly, assign multiple polygons to some zones.   dplyr::mutate(pstz_key = sample(LETTERS[1:7], size = 10, replace = T)) |>   dplyr::rename('geometry' = x) |>   sf::st_crop(x)  bp <- ggplot2::ggplot(x) +   ggplot2::geom_sf(fill = NA, lwd = 2) +   ggplot2::geom_sf(data = zone_polys, ggplot2::aes(fill = pstz_key))  bp +   ggplot2::geom_sf_label(data = zone_polys, ggplot2::aes(label = pstz_key))  ###################################################################### # example #1: request same numer of samples as zones - all zones returned.  res1 <- PolygonBasedSample(    x = x,    n = length(unique(zone_polys[['pstz_key']])),    zones = zone_polys,    zone_key  = \"pstz_key\",    increase_method = \"Most\"  )  bp +   geom_sf(data = res1, alpha = 0.9) +   geom_sf_label(data = pstz,aes(label = pstz_key))  ## note that we get the largest polygon from EACH group to sample from.  ##################################################################### # Example #2: request fewer samples than zones -> subset by method - choosing largest by area  res2 <- PolygonBasedSample(    x = x, n = 3, zones = zone_polys, zone_key  = \"pstz_key\", increase_method = \"Largest\" )  res2 |>   group_by(pstz_key) |>   mutate(total_area = sum(poly_area)) |>   sf::st_drop_geometry() |>   arrange(-total_area)|>   knitr::kable()  bp + # picks, the three largest   geom_sf(data = res2, alpha = 0.9) +   geom_sf_label(data = zone_polys, aes(label = pstz_key))  ####################################################################### # Example #3: request fewer samples than zones -> subset by method - choosing smallest by area res3 <- PolygonBasedSample(   x = x, n = 3, zones = zone_polys, zone_key = \"pstz_key\", increase_method = \"Smallest\")  res3 |>   group_by(pstz_key) |>   mutate(total_area = sum(poly_area)) |>   sf::st_drop_geometry() |>   arrange(total_area) |>   knitr::kable()  ## returns the largest polygon (poly_area) within the `pstz_key` group, ranked by (total_area)  bp + # picks, the n smallest - too small to see sometimes   geom_sf(data = filter(res3, allocation == 0), alpha = 0.9) +   geom_sf_label(data = zone_polys, aes(label = pstz_key))  #################################################################### # Example #4: request more samples than zones -> allocate extras to Largest pSTZs  ## note that is really a rounding rule - 'Largest' favors giving extra collections to the largest ## polygons while 'smallest' favors giving them smaller polygons. It is really mostly for edge cases ## and the two will generally behave similarly on contrived examples. res4 <- PolygonBasedSample(    x = x, n = 12, zones = zone_polys, zone_key = \"pstz_key\", increase_method = \"Largest\")  res4 |>   group_by(pstz_key) |>   summarize(total_area = sum(poly_area),  Total_Allocation = sum(allocation)) |>   sf::st_drop_geometry() |>   arrange(-total_area) |>   knitr::kable()  bp +   theme(legend.position = 'none') +   geom_sf(data = res4, aes(fill = as.factor(allocation))) +   geom_sf_label(data = res4, aes(label = allocation))  #################################################################### # Example #5: request more samples than zones -> allocate extras to pSTZs with most polygons res5 <- PolygonBasedSample(    x = x, n = 14, zones = zone_polys, zone_key = \"pstz_key\", increase_method = \"Most\")  res5 |>   group_by(pstz_key) |>   summarize(Count = n(), Total_Allocation = sum(allocation)) |>   sf::st_drop_geometry() |>   arrange(-Count) |>   knitr::kable() } # }"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":null,"dir":"Reference","previous_headings":"","what":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"last 'analytical' portion SD modelling process. produce binary (Yes/ NA) rasters species suitable habitat based three step process. first step uses dismo::thresholds determine feature raster want maximuize, case want raster less likely capture presence omit . can subset predicted habitat, dispersed comparing nearest neighbor distances observed points. Finally, can add back areas raster know species observed, hopefully missed original SDM, always suspicious points difficult fit light inertia rest species.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"","code":"PostProcessSDM(   rast_cont,   test,   train,   thresh_metric = \"sensitivity\",   quant_amt = 0.25,   planar_projection )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"rast_cont raw unaltered (except masked) raster predictions (x$RasterPredictions). test test data partition elasticSDM function (x$TestData). train train data partition elasticSDM function (x$TrainData). thresh_metric ?dismo::threshold options, defaults 'sensitivity' quant_amt quantile nearest neighbors distance use steps 2 3. defaults 0.25, using median nearest neighbor distance 10 bootstrapping replicates estimating buffer restrict SDM surface , minimum 10 bootstrap reps adding surface presence points placed binary suitable habitat. planar_projection Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"list containing two options. 1) spatraster 4 layers, ) continuous probabilities suitable habitat feed elasticSDM, B) raster binary format based specified thresholding statistic, C) binary raster B + habitat clipped buffer distances determined measuring nearest neighbor distances thresholding quantile D) binary raster C, adding distance points initially cells classified thresholding suitable habitat. D' general basis future steps, either B, C serve alternatives. 2) threshold statistics calculated dismo dataframe.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PrioritizeSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine which areas in a sample unit should be prioritized — PrioritizeSample","title":"Determine which areas in a sample unit should be prioritized — PrioritizeSample","text":"function offers guidance prioritizing sample locations orders. two parts, first essentially creates simple 'heatmap' tapering geographic center polygon sampled germplasm collection. keep spatial data 'light' , use also hard angled edges (ala vectorization raster), suggest n_breaks per polygon. levels side function relate increasing distances geographic center polygon. cells denoted '1' ideal areas sample polygon maintain well spaced distances across focal taxons range. second part offers loose order prioritizing general order collections. Using user specified metric attempts 'spread' samples across range reduce variance distances samples case desired number samples achieved. within individual sample units returned *Sample function. goal either method avoid collectors teams 'cheat' system repeatedly collecting along border two grid cells. understand many teams may collecting closely due species biology, land management, restrictions, goal function try guide dispersing activity. method used computes geometric centroid region, center falls outside grid, snapped back onto nearest location default. centers cell calculated remaining area grid distances calculated centers locations. final processing n_breaks applied based distances desired cell center partition space different priority collection units. Note submitting data PolygonBasedSample, column n, must maintained.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PrioritizeSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine which areas in a sample unit should be prioritized — PrioritizeSample","text":"","code":"PrioritizeSample(   x,   n_breaks = 3,   verbose = TRUE,   metric = c(\"var\", \"sd\", \"energy\", \"cv\") )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PrioritizeSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine which areas in a sample unit should be prioritized — PrioritizeSample","text":"x sf/tibble/dataframe. set sample grids *Sample functions n_breaks Numeric. number breaks return function, defaults 3. Values beyond 5 questionable utility. verbose Bool. Whether print messages console , defaults TRUE. metric character. metric minimize ordering zones. Options \"var\" (variance), \"sd\" (standard deviation), \"energy\" (sum squared distances), \"cv\" (coefficient variation).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PrioritizeSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine which areas in a sample unit should be prioritized — PrioritizeSample","text":"sf object containing prioritization zones within sample unit, columns: ID, SampleOrder, Level, geometry. ID corresponds sample unit, SampleOrder order prioritize sampling unit, Level priority level within unit (1 highest), geometry spatial geometry prioritization zones.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PrioritizeSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine which areas in a sample unit should be prioritized — PrioritizeSample","text":"","code":"if (FALSE) { # \\dontrun{ nc <- sf::st_read(system.file(\"shape/nc.shp\", package=\"sf\"), quiet = TRUE) |>   dplyr::select(NAME) |>   sf::st_transform(5070) # should be in planar coordinate system.  set.seed(1) zones <- EqualAreaSample(nc, n = 20, pts = 1000, planar_proj = 32617, reps = 100)  # the function requires an input sampling strategy to create the prioritization areas ps <- PrioritizeSample(zones$Geometry, n_breaks = 3, metric = 'energy')  ggplot2::ggplot() +   ggplot2::geom_sf(data = ps[['Geometry']],   ggplot2::aes(fill = factor(Level)), color = 'white', lwd = 1) +   ggplot2::theme_void() +   ggplot2::labs(fill = 'Within Zone Priority:', title = 'Focal areas to center sampling within') +   ggplot2::theme(legend.position= 'bottom')  ps[['Geometry']] |> ### to visualize without the priority zones within.   dplyr::group_by(SampleOrder) |>   dplyr::summarize(geometry = sf::st_union(geometry)) |>    ggplot2::ggplot() +   ggplot2::geom_sf(ggplot2::aes(fill = SampleOrder), color = 'white') +   ggplot2::geom_sf_label(ggplot2::aes(label = SampleOrder), color = 'white', size = 7) +   ggplot2::labs(fill = 'Sample Order', title = 'Priority guidance for sampling order') +   ggplot2::theme_void() +   ggplot2::theme(legend.position= 'bottom') } # }"},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":null,"dir":"Reference","previous_headings":"","what":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"rescaled rasters can used clustering, predicting results cluster analysis back space final product.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"","code":"RescaleRasters(model, predictors, training_data, pred_mat)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"model final output model glmnet elasticSDM predictors raster stack use process elasticSDM training_data data went glmnet model, used calculating variance required scaling process. elasticSDM pred_mat Prediction matrix elasticSDM","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"list two objects. 1) rescaled raster stack. 2) table standardized unstandardized coefficients glmnet model.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/buildResistanceSurface.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a simple, theoretical, raster surface modelling Isolation by Distance. — buildResistanceSurface","title":"Create a simple, theoretical, raster surface modelling Isolation by Distance. — buildResistanceSurface","text":"Used conjunction, preferably populationResistance IBRBasedSample workflow.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/buildResistanceSurface.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a simple, theoretical, raster surface modelling Isolation by Distance. — buildResistanceSurface","text":"","code":"buildResistanceSurface(   base_raster,   resistance_surface = NULL,   oceans = NULL,   lakes = NULL,   rivers = NULL,   tri = NULL,   habitat = NULL,   w_ocean = 100,   w_lakes = 50,   w_rivers = 20,   w_tri = 1,   w_habitat = 1,   addtl_r = NULL,   addtl_w = NULL )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/buildResistanceSurface.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a simple, theoretical, raster surface modelling Isolation by Distance. — buildResistanceSurface","text":"base_raster SpatRaster. Base raster study area. Provides template geometry resolution. resistance_surface SpatRaster. Optional pre-computed resistance raster. provided, raster-building arguments ignored. oceans SpatRaster. Binary (0/1) raster ocean cells. Used increase movement cost. lakes SpatRaster. Binary (0/1) raster lakes. rivers SpatRaster. Binary (0/1) raster rivers. tri SpatRaster. Continuous raster topographic roughness (TRI). Used increase cost mountainous terrain. habitat SpatRaster. Continuous raster habitat suitability. Low values increase cost. w_ocean Numeric. Weight applied oceans (default 2000). w_lakes Numeric. Weight applied lakes (default 200). w_rivers Numeric. Weight applied rivers (default 20). w_tri Numeric. Weight applied TRI (default 1). w_habitat Numeric. Weight applied habitat suitability (default 1). addtl_r SpatRaster, 'raster stack'. Additional layers include resistance surface addtl_w Numeric vector. Must equal length addtl_r exactly. Weights additional rasters layers include resistance surface","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/buildResistanceSurface.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a simple, theoretical, raster surface modelling Isolation by Distance. — buildResistanceSurface","text":"","code":"if (FALSE) { # \\dontrun{ # Prepare resistance raster  # this also can run internally in `population resistance`, # but for time sakes is best to prep ahead of time # especially if treating multiple species in the same domain. res <- buildResistanceSurface(   base_raster = base_rast,   oceans = ocean_r,   lakes = lakes_r,   rivers = rivers_r,   tri = tri_r ) } # }"},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a quick SDM using elastic net regression — elasticSDM","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"function quickly creates SDM using elastic net regression, properly format data downstream use safeHavens workflow. Note elastic net models used couple important reasons: rescale input independent variables modelling, allowing us combine raw data beta coefficients use clustering algorithms downstream. also allow 'shrinking' terms models shrinking terms models able get levels ecological inference prohibited older model selection frameworks.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"","code":"elasticSDM(   x,   predictors,   planar_projection,   domain = NULL,   quantile_v = 0.025,   PCNM = TRUE )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"x (simple feature) sf data set occurrence data species. predictors terra 'rasterstack' variables serve independent predictors. planar_projection Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. domain Numeric, many times larger make entire domain analysis simple bounding box around occurrence data x. quantile_v Numeric, variable used thinning input data, e.g. quantile = 0.05 remove records within lowest 5% distance iteratively, remaining records apart distance . want essentially thinning happen just supply 0.01. Defaults 0.025. PCNM Boolean. Whether include PCNM fitting model. Defaults TRUE use EnvironmentalBasedSample, FALSE used Predictive Provenance type workstreams.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"list 12 objects, subsequently used downstream SDM post processing sequence, think best written disk. actual model prediction raster surface present first list 'RasterPredictions', independent variables used final model present 'Predictors', just global PCNM/MEM raster surfaces 'PCNM'. fit model 'Model', cross validation folds stored 'CVStructure', results single test/train partition 'ConfusionMatrix', two data split 'TrainData' 'TestData' finally 'PredictMatrix' used classifying test data confusion matrix.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"","code":"if (FALSE) { # \\dontrun{   x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv'))  x <- x[,c('lon', 'lat')]  x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)   files <- list.files(    path = file.path(system.file(package=\"dismo\"), 'ex'),    pattern = 'grd',  full.names=TRUE )  predictors <- terra::rast(files)  sdModel <- elasticSDM(    x = x, predictors = predictors, quantile_v = 0.025,    planar_projection =      '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs')   terra::plot(sdModel$RasterPredictions) } # }"},{"path":"https://sagesteppe.github.io/safeHavens/reference/greatCircleDistance.html","id":null,"dir":"Reference","previous_headings":"","what":"Haversine Distance Calculation — greatCircleDistance","title":"Haversine Distance Calculation — greatCircleDistance","text":"Calculate geographic distances geoid. results accurate distance calculations planar system. Function mostly used internally maximize_dispersion","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/greatCircleDistance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Haversine Distance Calculation — greatCircleDistance","text":"","code":"greatCircleDistance(lat1, lon1, lat2, lon2)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/greatCircleDistance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Haversine Distance Calculation — greatCircleDistance","text":"lat1 Double. column holding coords 'focal' population lon1 Double. column holding coords 'focal' population lat2 Double. column holding coords 'non-focal' population lon2 Double. column holding coords 'non-focal' population","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/greatCircleDistance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Haversine Distance Calculation — greatCircleDistance","text":"calculate distances sites (Haversine formula)","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/greatCircleDistance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Haversine Distance Calculation — greatCircleDistance","text":"","code":"n_sites <- 5 # number of known populations  df <- data.frame(    site_id = seq_len(n_sites),    lat = runif(n_sites, 25, 30),    lon = runif(n_sites, -125, -120)  )  dist_mat <- sapply(1:nrow(df), function(i) {    greatCircleDistance(      df$lat[i], df$lon[i],      df$lat, df$lon    )  }) #head(dist_mat)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/order_by_distance_variance.html","id":null,"dir":"Reference","previous_headings":"","what":"Order zones by minimizing distance variance — order_by_distance_variance","title":"Order zones by minimizing distance variance — order_by_distance_variance","text":"Order set spatial zones based minimizing distance variance","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/order_by_distance_variance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Order zones by minimizing distance variance — order_by_distance_variance","text":"","code":"order_by_distance_variance(x, metric = c(\"var\", \"sd\", \"energy\", \"cv\"))"},{"path":"https://sagesteppe.github.io/safeHavens/reference/order_by_distance_variance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Order zones by minimizing distance variance — order_by_distance_variance","text":"x sf object containing zones ordered metric character. metric minimize ordering zones. Options \"var\" (variance), \"sd\" (standard deviation), \"energy\" (sum squared distances), \"cv\" (coefficient variation).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/order_by_distance_variance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Order zones by minimizing distance variance — order_by_distance_variance","text":"numeric vector representing order zones based specified distance variance metric","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/populationResistance.html","id":null,"dir":"Reference","previous_headings":"","what":"Identify clusters of populations least separated by landscape resistance — populationResistance","title":"Identify clusters of populations least separated by landscape resistance — populationResistance","text":"Creates matrix, supporting data structures, Isolation Distance distances using simplified framework. Given raster surface parameterizing resistance movement (e.g., oceans, lakes, rivers, topographic roughness, habitat suitability), function calculates pairwise landscape resistance among populations identifies clusters populations least isolated . function can internally build resistance raster, performance across multiple species, recommended provide pre-computed surfaces. Resistance treated isotropic (slope directionality). Clustering based pairwise IBR distances can visualized used guide sampling schemes.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/populationResistance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Identify clusters of populations least separated by landscape resistance — populationResistance","text":"","code":"populationResistance(   populations_sf,   base_raster,   buffer_dist = 25000,   planar_proj = NULL,   n_points = 150,   resistance_surface = NULL,   oceans = NULL,   lakes = NULL,   rivers = NULL,   tri = NULL,   habitat = NULL,   w_ocean = 100,   w_lakes = 50,   w_rivers = 20,   w_tri = 1,   w_habitat = 1,   graph_method = c(\"complete\", \"delaunay\"),   ibr_method = \"leastcost\" )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/populationResistance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Identify clusters of populations least separated by landscape resistance — populationResistance","text":"populations_sf sf/tibble/data.frame. Coordinates populations clustered. Must contain POINT geometries. base_raster SpatRaster. Base raster study area. Provides template geometry resolution. buffer_dist Numeric. Distance buffer population points assign likely occupied cells (optional; ignored using sample_population_cells directly). planar_proj Numeric. EPSG code used project populations_sf buffering operations. n_points Numeric (defaults 50). Number points sample creating surface. Increasing points drastically increases compute time. resistance_surface SpatRaster. Optional pre-computed resistance raster. provided, raster-building arguments ignored. oceans SpatRaster. Binary (0/1) raster ocean cells. Used increase movement cost. lakes SpatRaster. Binary (0/1) raster lakes. rivers SpatRaster. Binary (0/1) raster rivers. tri SpatRaster. Continuous raster topographic roughness (TRI). Used increase cost mountainous terrain. habitat SpatRaster. Continuous raster habitat suitability. Low values increase cost. w_ocean Numeric. Weight applied oceans (default 2000). w_lakes Numeric. Weight applied lakes (default 200). w_rivers Numeric. Weight applied rivers (default 20). w_tri Numeric. Weight applied TRI (default 1). w_habitat Numeric. Weight applied habitat suitability (default 1). graph_method Character. One \"delaunay\" \"complete\". Defines spatial graph connecting population points. Delauney produces sparser graph can deal higher n, 'complete' computationally intensive n > 175 take considerable time. ibr_method Character. Currently \"leastcost\".","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/populationResistance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Identify clusters of populations least separated by landscape resistance — populationResistance","text":"list following elements: resistance_raster parameterized resistance SpatRaster used IBR calculations. sampled_points sf POINT object population cells used graph calculations. spatial_graph igraph object representing population connectivity graph. edge_list Data.frame graph edges used IBR computation. ibr_matrix Symmetric numeric matrix pairwise landscape resistance populations.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/populationResistance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Identify clusters of populations least separated by landscape resistance — populationResistance","text":"","code":"if (FALSE) { # \\dontrun{ # Prepare resistance raster once  # this also can run internally in `population resistance`, # but for time sakes is best to prep ahead of time # especially if treating multiple species in the same domain. res <- buildResistanceSurface(   base_raster = base_rast,   oceans = ocean_r,   lakes = lakes_r,   rivers = rivers_r,   tri = tri_r )  # Run population resistance clustering out <- populationResistance(   populations_sf = pops_sf,   base_raster = base_rast,   resistance_surface = res,   n_points = 5,   graph_method = \"mst\",   ibr_method = \"leastcost\" )  # Access the results ibr_matrix <- out$ibr_matrix graph <- out$spatial_graph } # }"},{"path":"https://sagesteppe.github.io/safeHavens/reference/projectClusters.html","id":null,"dir":"Reference","previous_headings":"","what":"Project current environmental clusters onto a future climate scenario — projectClusters","title":"Project current environmental clusters onto a future climate scenario — projectClusters","text":"Main entry point future-projection workflow.  steps :","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/projectClusters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Project current environmental clusters onto a future climate scenario — projectClusters","text":"","code":"projectClusters(   eSDM_object,   current_clusters,   future_predictors,   current_predictors,   planar_proj,   coord_wt = 0.001,   mess_threshold = 0,   cluster_novel = TRUE,   n_novel_pts = 500,   n_sample_per_cluster = 50,   nbclust_args = list(),   thresh_metric = \"sensitivity\",   thresholds )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/projectClusters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Project current environmental clusters onto a future climate scenario — projectClusters","text":"eSDM_object output elasticSDM(). current_clusters Output list EnvironmentalBasedSample(). future_predictors SpatRaster future climate. Layer names must match retained current_model. current_predictors SpatRaster current climate (used MESS reference rescaleFuture standardisation). planar_proj EPSG code proj4 string planar projection metres (used current analysis). coord_wt Numeric, default 0.001. Coordinate weighting passed add_weighted_coordinates. mess_threshold MESS values treated novel climate. Default 0. cluster_novel Boolean, defualt TRUE. TRUE novel cells exist, cluster independently NbClust. FALSE novel cells left NA. n_novel_pts Numeric, default 500. Number points sample novel areas clustering. n_sample_per_cluster Number points sample cluster (existing novel) relationship tree.  Default 50. nbclust_args Named list arguments forwarded NbClust::NbClust. Sensible defaults set internally (min.nc = 2, max.nc = 10, method = \"ward.D2\", index = \"\"). thresh_metric Character. Default 'sensitivity', dismo::threshold use cutting future sdm binary surface. thresholds current era thresholds postProcessSDM","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/projectClusters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Project current environmental clusters onto a future climate scenario — projectClusters","text":"named list: clusters_sf sf polygons column ID. suitable_habitat Raster masked suitable habitat future conditions novel_mask SpatRaster — logical, TRUE MESS < threshold. mess SpatRaster — raw MESS scores (minimum across variables). changes data.frame — per-cluster area centroid-shift metrics. novel_similarity data.frame — nearest existing cluster mean silhouette width novel cluster.  Zero-row data frame none.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/projectClusters.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Project current environmental clusters onto a future climate scenario — projectClusters","text":"Rescale future predictors current betas (rescaleFuture). Run dismo::mess identify novel climate cells. Predict known-climate cells existing KNN classifier. novel cells exist cluster_novel = TRUE, cluster independently NbClust (cluster_novel_areas). Sample points every cluster (existing + novel), force onto single tree, extract nearest-existing-cluster relationships via silhouette (analyze_cluster_relationships). Polygonise calculate area / centroid changes.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. dplyr any_of magrittr %>%","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/rescaleFuture.html","id":null,"dir":"Reference","previous_headings":"","what":"Rescale future climate predictors using current model coefficients — rescaleFuture","title":"Rescale future climate predictors using current model coefficients — rescaleFuture","text":"Standardises future predictors using mean SD current climate, applies glmnet beta weights.  weighting applied upstream RescaleRasters — just different raster stack.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/rescaleFuture.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rescale future climate predictors using current model coefficients — rescaleFuture","text":"","code":"rescaleFuture(   model,   future_predictors,   current_predictors,   training_data,   pred_mat )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/rescaleFuture.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rescale future climate predictors using current model coefficients — rescaleFuture","text":"model glmnet model object elasticSDM_noPCNM()$Model. future_predictors SpatRaster future climate variables.  Names must match retained model. current_predictors SpatRaster current climate (provides standardisation parameters). training_data data went glmnet model, used calculating variance required scaling process. elasticSDM pred_mat Prediction matrix elasticSDM","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/rescaleFuture.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rescale future climate predictors using current model coefficients — rescaleFuture","text":"SpatRaster rescaled future predictors (one layer per non-zero coefficient).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/split_cols.html","id":null,"dir":"Reference","previous_headings":"","what":"split and extract the temperature values from Tmin and AHM columns — split_cols","title":"split and extract the temperature values from Tmin and AHM columns — split_cols","text":"Programmed Bower provisional seed zone products, helper function separating recovering values columns data.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/split_cols.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"split and extract the temperature values from Tmin and AHM columns — split_cols","text":"","code":"split_cols(dat, y, sep = \"-\")"},{"path":"https://sagesteppe.github.io/safeHavens/reference/split_cols.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"split and extract the temperature values from Tmin and AHM columns — split_cols","text":"dat data frame columns required split. y character. column name split. sep character. separator values split . Default '-'.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/split_cols.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"split and extract the temperature values from Tmin and AHM columns — split_cols","text":"","code":"df = data.frame(   'Tmin_class' = c('10 - 15 Deg. F.', '15 - 20 Deg. F.', '> 55 Deg. F.' ),   'AHM_class' = c('2 - 3', '6 - 12', '3 - 6') ) split_cols(df, 'Tmin_class') #>   lower upper median range #> 1    10    15   12.5     5 #> 2    15    20   17.5     5 #> 3    55    55   55.0     0 split_cols(df, 'AHM_class') #>   lower upper median range #> 1     2     3    2.5     1 #> 2     6    12    9.0     6 #> 3     3     6    4.5     3"},{"path":"https://sagesteppe.github.io/safeHavens/reference/trainKNN.html","id":null,"dir":"Reference","previous_headings":"","what":"partition data and train a simple KNN model — trainKNN","title":"partition data and train a simple KNN model — trainKNN","text":"Simply use partition test data quickly train simple model","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/trainKNN.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"partition data and train a simple KNN model — trainKNN","text":"","code":"trainKNN(x, split_prop)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/trainKNN.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"partition data and train a simple KNN model — trainKNN","text":"x weighted matrix including class ID column 'ID' split_prop prop data partitions.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":null,"dir":"Reference","previous_headings":"","what":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"function used write wide range values fitPredictOperationalize process. create multiple subdirectories within user specified path. include: 'Rasters' raster stack four final rasters go, 'Fitting' details model fitting caret placed, 'Models' final fit model go, 'Evaluation' evaluation statistics placed, 'Threshold' results form dismo::threshold placed.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"","code":"writeSDMresults(   path,   taxon,   cv_model,   pcnm,   model,   cm,   coef_tab,   f_rasts,   thresh )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"path root path 5 folders created, exist. taxon name taxonomic entity models created. cv_model cross validation data elasticSDM pcnm pcnm/mem rasters elasticSDM model final glmnet model elasticSDM cm confusion matrix elasticSDM coef_tab coefficient table RescaleRasters f_rasts final rasters RescaleRasters thresh threshold statistics PostProcessSDM","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"objects, objects specified, written disk.","code":""}]
