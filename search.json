[{"path":"https://sagesteppe.github.io/safeHavens/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Reed Clark Benkendorf Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"about-safehavens","dir":"Articles","previous_headings":"","what":"about safeHavens","title":"About","text":"package helps germplasm curators communicate areas interest collection teams targeting new accessions. common species, provides seven different sampling approaches curators choose individual taxon hope process. also provides additional sampling approach rare species. package allows easy integration existing workflows, reading accession data databases Excel, comparing occurrence data, writing run results tabular simple spatial data formats, can shared collection teams work handheld electronic devices (e.g. Qfield). allows germplasm curators sophisticated spatial abilities without need GIS experts access fee-based software.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"common-species-sampling-approaches","dir":"Articles","previous_headings":"about safeHavens","what":"‘common’ species sampling approaches","title":"About","text":"approach based fundamental ecological theory. practice, attempt capture geographic environmental variation across species range largely rely Sewall Wright’s concept Isolation Distance (1943), idea populations geographically proximate genetically similar populations apart. Hence, priori approaches, exception EnvironmentalBasedSample function, relies species distribution model (SDM) inform sampling locations based concept Isolation Environment (Wang & Bradburd 2014). none meant supplant genetic diversity-based sampling, recognize data rare, even gathering often miss opportunity make opportunistic seed collections along way. methods various trade-offs terms computational environmental complexity, although designed run standard desktop laptop computer, processing many thousands species overnight possible. table presents currently implemented sampling scheme user-facing function associated . first three functions, GridBasedSample, PointBasedSample, EqualAreaSample, flavors process, try partition species range geographic chunks (polygons) similar sizes. requires minimal computational power features essentially environmental information context beyond species current range restriction. fourth method, OpportunisticSample, special case first three, existing collection records used help guide sampling scheme. OpportunisticSample uses underlying logic PointBasedSample, first ‘removes’ areas around existing collection records, fills remaining gaps new sampling locations first three functions used establishing new collection strategy species, OpportunisticSample used trying augment existing collection strategy. fifth method, IBDBasedSample, largely class ; lieu using continuity geographic space primary method, focuses discontinuity space uses distance matrices clustering determine patches range closer patches. method computationally expensive previous four, incorporates environmental information, explicit way final two methods . EcoregionBasedSample may commonly encountered method North America, various formats, driving two major germplasm banking projects Midwest Southeastern United States, well high level, composing way numerous native seed collection coordinators structured West. method uses environmental variation implicit guide target populations seed collections; , different ecoregions serve stratification agents. broad strokes, general thinking regions represent continuous transitions environment faced species, populations across ranges differently adapted environments. However, relies existing ecoregion maps, may may relevant ecology species. final function, EnvironmentalBasedSample, computationally expensive environmentally explicit. function fits Species Distribution Model (SDM), generated via generalized linear model (glmnet), supported package, cluster populations based environmental variables related observed distributions spatial configuration distance . paper, draws together aspects functions; however empirical testing approach implemented.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"rare-species-sampling-approach","dir":"Articles","previous_headings":"about safeHavens","what":"rare species sampling approach","title":"About","text":"methods designed common species, species 50 100 unique occurrence records. method returns polygon geometry provides guidelines general regions species sampled. However, rare species, occurrence individual populations well tracked, generally collected along maternal lines, requiring considerably field effort gather seed, supplemental approach exists suggests priority individual populations (‘points’) can sampled. maximizeDispersion method utilizes either geographic distances alone geographic distances conjunction distance matrix developed key environmental variables suggest populations prioritized seed collections. method based concept maximizing dispersion selected points geographic environmental spaces best capture overall variation present species range.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/About.html","id":"installation","dir":"Articles","previous_headings":"about safeHavens","what":"installation","title":"About","text":"safeHavens can installed directly github, using either devtools remotes. Note software relies packages may require additional system dependencies, rgeos rgdal. also requires working installation GDAL, PROJ, GEOS system, well Rcpp. Generally, installations go without hitch may require additional attention users. tradeoff free tools enormous power quite commonly used ecological geographical modelling. safeHavens can installed GitHub remotes devtools. users issues devtools windows, remotes tends work well.","code":"remotes::install_github('sagesteppe/safeHavens')   # install.packages('devtools')  # devtools::install_github('sagesteppe/safeHavens')"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"set-up","dir":"Articles","previous_headings":"","what":"set up","title":"Getting Started","text":"safeHavens can installed directly github.","code":"remotes::install_github('sagesteppe/safeHavens') library(safeHavens) library(ggplot2) library(sf) library(terra) library(spData) library(dplyr) library(patchwork) set.seed(23)  planar_proj <- '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs'"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"defining-a-species-range-or-domain-for-sampling","dir":"Articles","previous_headings":"","what":"Defining a Species Range or Domain for Sampling","title":"Getting Started","text":"Central sampling schemes safeHavens species range domain sampling. example, depending goals collection, curator may want sample across entire range species. Alternatively one may interested sampling portion range, e.g. country, state, ecoregion. Either scenarios can accomplished package. show create species range occurrence data, use range run various sampling schemes. use sf simply buffer occurrence points create species range across multiple South American nations.  Alternatives include simple convex hull around species, widespread throughout area, masking binary SDM surface domain.","code":"x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326) x_buff <- sf::st_transform(x, planar_proj) |>   # we are working in planar metric coordinates, we are   # buffer by this many / 1000 kilometers.    sf::st_buffer(125000) |>    sf::st_as_sfc() |>    sf::st_union()  plot(x_buff)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"prep-a-map-background","dir":"Articles","previous_headings":"","what":"Prep a map background","title":"Getting Started","text":"use spData package uses naturalearth data ’s world data suitable creating effective maps variety resolutions.","code":"x_extra_buff <- sf::st_buffer(x_buff, 100000) |> # add a buffer to 'frame' the maps   sf::st_transform(4326)  americas <- spData::world americas <- sf::st_crop(americas, sf::st_bbox(x_extra_buff)) |>   dplyr::select(name_long) #> Warning: attribute variables are assumed to be spatially constant throughout #> all geometries  bb <- sf::st_bbox(x_extra_buff)  map <- ggplot() +    geom_sf(data = americas) +    theme(     legend.position = 'none',      panel.background = element_rect(fill = \"aliceblue\"),      panel.grid.minor.x = element_line(colour = \"red\", linetype = 3, linewidth  = 0.5),      axis.ticks=element_blank(),     axis.text=element_blank(),     plot.background=element_rect(colour=\"steelblue\"),     plot.margin=grid::unit(c(0,0,0,0),\"cm\"),     axis.ticks.length = unit(0, \"pt\"))+    coord_sf(xlim = c(bb[1], bb[3]), ylim = c(bb[2], bb[4]), expand = FALSE)  rm(x_extra_buff, americas)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"running-the-various-sample-design-algorithms","dir":"Articles","previous_headings":"","what":"Running the Various Sample Design Algorithms","title":"Getting Started","text":"Now data can represent species ranges, can run various sampling approaches. table introduction reproduced . Note table ‘Comp.’ ‘Envi.’ refer computational environmental complexity respectively, range low (L) medium high.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"grid-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Grid Based Sample","title":"Getting Started","text":"Grids useful sampling contiguous things. Species ranges often contiguous; however curators analysts geographically grand ecosystems, e.g. steppes, prairies, tundra, taiga might find useful. can look output see grids great type problem. first step grid sampling determining OK number grids try draw starting point, want 20 collections need 20 grids, several merged larger ones. Using aspect ratio simple bounding box around area analyzing, TestGridSizes function determine default number grids (‘Original’) testing. Using defaults create sets grids well, either removing one two grids per direction. Theoretically automate grid selection comparing number grids minimization variance. safe wouldn’t consider configurations generate less 25 initial grids.  Essentially need 20 grids, realistically (albeit limited informal testing) using 25 grids - depending complexity species range - tends effective floor. table plot opt using ‘Smaller’ option, 28 grids generated prompting sf::st_make_grid 7 grids X direction 5 Y direction. can kind think like elbow plot, samples won’t get characteristic shape.","code":"tgs <- TestGridSizes(x_buff) print(tgs) #>       Name Grids  Variance GridNOx GridNOy #> 1 Smallest    34  599.9513       8       6 #> 2  Smaller    28  813.2043       7       5 #> 3 Original    24  862.5458       6       4 #> 4   Larger    18  681.7667       5       3 #> 5  Largest    13 1182.8457       4       2  plot(tgs$Grids, tgs$Variance, xlab = 'Grid Number', ylab = 'Variance',      main = 'Number of grids and areas overlapping species range') text(tgs$Grids, tgs$Variance + 25, labels=tgs$Name, cex= 0.7) abline(v=20, col=\"red\") abline(v=25, col=\"orange\") tgs <- tgs[tgs$Name=='Smaller',] grid_buff <- GridBasedSample(x_buff, planar_proj, gridDimensions = tgs)   gbs.p <- map +    geom_sf(data = grid_buff, aes(fill = factor(ID))) +   # geom_sf_label(data = grid_buff, aes(label = Assigned), alpha = 0.4) +  # on your computer, doesnt work at vignette size   labs(title = 'Grids')  +    coord_sf(expand = F)  gbs.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"point-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Point Based Sample","title":"Getting Started","text":"grids drew pre-specified number grids across species range, merged together required get results. essentially inverse step, rather drawing boundaries - .e. grid cells, draw centers. essentially allows features ‘grow’ little naturally. also think results work little bit better fragmented range, still odd clipping, minor portions section range assigned different grid, general little bit better.","code":"pbs <- PointBasedSample(x_buff) pbs.sf <- pbs$Geometry  st_is_valid(x_buff) #> [1] TRUE  pbs.p <- map +    geom_sf(data = pbs.sf, aes(fill = factor(ID))) +  #  geom_sf_label(data = pbs.sf, aes(label = ID), alpha = 0.4) +    labs(title = 'Point') +    coord_sf(expand = F) pbs.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"equal-area-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Equal Area Sample","title":"Getting Started","text":"Perhaps simplest method offered safeHavens EqualAreaSample. simply creates many points, pts defaulting 5000, within target domain subjects k-means sampling groups specified n, target number collections. individual points assigned group merged polygons ‘take’ geographic space, intersected back species range, area polygon measured. process ran times, defaulting 100 reps, set polygons created reps smallest variance polygon size selected returned. differs point based sampling instance, start regularly spaced points grow , take step back using many points let clusters grow similar sizes.  results look quite similar point based sample.","code":"eas <- EqualAreaSample(x_buff, planar_projection = planar_proj)  #> Warning: did not converge in 10 iterations  eas.p <- map +    geom_sf(data = eas$Geometry, aes(fill = factor(ID))) +  #  geom_sf_label(data = eas.sf, aes(label = ID), alpha = 0.4) +    labs(title = 'Equal Area') +    coord_sf(expand = F) eas.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"opportunistic-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Opportunistic Sample","title":"Getting Started","text":"Many curators interested much can embed existing collections sampling framework. function OpportunisticSample makes minor modifications point based sample maximize existing collection. doesn’t always work exceptionally, especially couple collections close , may beneficial tool belt. observed, three previous sampling schemes end somewhat similar results - took used PointBasedSample framework embedded function . Essentially combines approach point based sampling, forces clusters based around existing accessions. attempts ‘center’ existing collections within clusters, can nearly impossible variety reasons.  , grids aligned around points. can lead oddly shaped clusters, bird hand worth two bush.","code":"exist_pts <- sf::st_sample(x_buff, size = 10) |>     sf::st_as_sf() |> # ^^ just randomly sampling 10 points in the species range    dplyr::rename(geometry = x)  os <- OpportunisticSample(polygon = x_buff, n = 20, collections = exist_pts)  os.p <- map +    geom_sf(data = os$Geometry, aes(fill = factor(ID))) +  #  geom_sf_label(data = os.sf, aes(label = ID), alpha = 0.4) +    geom_sf(data = exist_pts, alpha = 0.4) +    labs(title = 'Opportunistic') +    coord_sf(expand = F)  os.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"isolation-by-distance-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Isolation by Distance Based Sample","title":"Getting Started","text":"Isolation Distance fundamental idea behind package. function explicitly uses IBD develop sampling scheme, obfuscate parameters. Note function requires raster input, rather vector.  data processed raster, lienar edges, representing raster tiles. However, evident borders clusters natural looking previous (future) sampling schemes.","code":"files <- list.files(    path = file.path(system.file(package=\"dismo\"), 'ex'),   pattern = 'grd',  full.names=TRUE )  predictors <- terra::rast(files)   x_buff.sf <- sf::st_as_sf(x_buff) |>    dplyr::mutate(Range = 1) |>    sf::st_transform( terra::crs(predictors))  # and here we specify the field/column with our variable we want to become an attribute of our raster v <- terra::rasterize(x_buff.sf, predictors, field = 'Range')   # now we run the function demanding 20 areas to make accessions from,  ibdbs <- IBDBasedSample(x = v, n = 20, fixedClusters = TRUE, template = predictors)  ibdbs.p <- map +    geom_sf(data = ibdbs, aes(fill = factor(ID))) +  #  geom_sf_label(data = os.sf, aes(label = ID), alpha = 0.4) +    labs(title = 'IBD') +    coord_sf(expand = F) ibdbs.p rm(predictors, files, v, x_buff.sf, exist_pts, os)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"ecoregion-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Ecoregion Based Sample","title":"Getting Started","text":"commonly implemented method guiding native seed collection North America. However, sure exactly practitioners implement , whether formats application consistent among practitioners! reasons different sets options supported user. general usage, two parameters always required x species range sf object, ecoregions, sf object containing ecoregions interest. ecoregions file need subset range x quite yet - function take care . Additional arguments function include usual n specify many accession looking collection. Two additional arguments relate whether using Omernik Level 4 ecoregions data ecoregions (biogeographic regions) another source. OmernikEPA, ecoregion_col, using official EPA release ecoregions optional, however using EPA product supplied - ecoregion_col argument totally necessary. column contain unique names highest resolution level ecoregion want use data set, many data sets, example call ‘neo_eco’ may field ecolevel information!  output differs others see, depicted number collections made per ecoregion. number ecoregions greater requested sample size, return object can take two values - collections, one collection.","code":"neo_eco <- sf::st_read(   file.path(system.file(package=\"safeHavens\"), 'extdata', 'NeoTropicsEcoregions.gpkg'),    quiet = TRUE) |>   dplyr::rename(geometry = geom) head(neo_eco[,c(1, 3, 4, 6, 11)]) #> Simple feature collection with 6 features and 4 fields #> Geometry type: MULTIPOLYGON #> Dimension:     XY #> Bounding box:  xmin: -103.0432 ymin: -31.25308 xmax: -34.79344 ymax: 26.91751 #> Geodetic CRS:  WGS 84 #>                  Provincias      Region      Dominio #> 1 Araucaria Forest province Neotropical       Parana #> 2          Atacama province Neotropical         <NA> #> 3         Atlantic province Neotropical       Parana #> 4           Bahama province Neotropical         <NA> #> 5     Balsas Basin province Neotropical Mesoamerican #> 6         Caatinga province Neotropical      Chacoan #>                        Subregion                       geometry #> 1                        Chacoan MULTIPOLYGON (((-53.58012 -... #> 2 South American Transition Zone MULTIPOLYGON (((-69.42981 -... #> 3                        Chacoan MULTIPOLYGON (((-48.41217 -... #> 4                      Antillean MULTIPOLYGON (((-77.58593 2... #> 5                      Brazilian MULTIPOLYGON (((-97.37265 1... #> 6                        Chacoan MULTIPOLYGON (((-35.56652 -...  x_buff <- sf::st_transform(x_buff, sf::st_crs(neo_eco)) ebs.sf <- EcoregionBasedSample(x_buff, neo_eco, OmernikEPA = FALSE, ecoregion_col = 'Provincias') #> Warning: attribute variables are assumed to be spatially constant throughout #> all geometries #> Warning: attribute variables are assumed to be spatially constant throughout #> all geometries  # for plotting let's crop it to the other objects ebs.sf <- st_crop(ebs.sf, bb) #> Warning: attribute variables are assumed to be spatially constant throughout #> all geometries  ebs.p <- map +    geom_sf(data = ebs.sf, aes(fill = factor(n))) +    labs(title = 'Ecoregion') +    coord_sf(expand = F) ebs.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"environmental-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Environmental Based Sample","title":"Getting Started","text":"environmental based sample can conducted species distribution model data. Included data directory folder objects required run example species. load .","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"load-the-sdm-predictions","dir":"Articles","previous_headings":"","what":"load the SDM predictions","title":"Getting Started","text":"package also SDM prediction saved data can just load couple comparisons.  data loaded R, scale rasters (using RescaleRasters) serve surfaces predict (also done !), run algorithm (EnvironmentalBasedSample). However, run algorithm need create directory (also called ‘folder’), computers save results function EnvironmentalBasedSample. Whereas earlier vignette showcased functions generated species distribution model, us saving results two stage process (e.g. create SDM associated products used: elasticSDM, PostProcessSDM, RescaleRasters, finally saving relevant data writeSDMresults), function produces product writes ancillary data simultaneously. approach chosen function writing four objects: 1) groups vector data, 2) groups raster data, 3) k-nearest neighbors (knn) model used generate clusters, 4) confusion matrix associated testing knn model.  function EnvironmentalBasedSample can take three binary rasters created PostProcessSDM arguments template. showcase different results using .  plots able showcase difference results depending three input rasters utilized. sampling schemes, results vary widely based spatial extents functions applied . Using SDM output undergone thresholding results largest classified area. first glance results may seem different, look central america, largely consistent, near Andes; large differences exist Amazon Basin, even alignment systems evident. Accordingly, surface used species match evaluation criterion. Using threshold raster surface good option want ‘miss’ many areas, whereas clipped supplemented options may better suited scenarios want draw clusters, lack populations can collected .","code":"sdm <- terra::rast(file.path(system.file(package=\"safeHavens\"),  'extdata', 'Bradypus_test.tif')) terra::plot(sdm) sdModel <- readRDS(   file.path(system.file(package=\"safeHavens\"), 'extdata',  'sdModel.rds')   )  sdModel$Predictors <- terra::rast(   file.path(system.file(package=\"safeHavens\"), 'extdata', 'Predictors.tif') ) rr <- RescaleRasters( # you may have already done this!   model = sdModel$Model,   predictors = sdModel$Predictors,    training_data = sdModel$TrainData,    pred_mat = sdModel$PredictMatrix)  # create a directory to hold the results from EBS real quick.  # we will default to placing it in your current working directory.  # If you are a data management freak don't worry too much about this.  # The code to remove the directory will be provided below.  getwd() # this is where the folder is going to located IF YOU DON'T RUN the code below.  #> [1] \"/home/runner/work/safeHavens/safeHavens/vignettes\" p <- file.path('~', 'Documents') # in my case I'll dump it in Documents real quick, this should work on  # Linux and Mac, but I don't think Windows?  # dir.create(file.path(p, 'safeHavens-Vignette')) # now create the directory.   ENVIbs <- EnvironmentalBasedSample(   pred_rescale = rr$RescaledPredictors,    write2disk = FALSE,    path = file.path(p, 'safeHavens-Vignette'), # we are not writing, but showing how to provide argument   taxon = 'Bradypus_test',    f_rasts = sdm, n = 20,    lyr = 'Supplemented',   fixedClusters = TRUE,    n_pts = 500,    planar_projection = planar_proj,   buffer_d = 3, prop_split = 0.8) #> Joining with `by = join_by(x, y)` #> Warning in st_point_on_surface.sfc(st_geometry(x)): st_point_on_surface may not #> give correct results for longitude/latitude data  ENVIbs.p <- map +    geom_sf(data = ENVIbs, aes(fill = factor(ID))) +    #geom_sf_label(data = ENVIbs, aes(label = ID), alpha = 0.4) +    labs(title = 'Environmental') +    coord_sf(expand = FALSE) #> Coordinate system already present. #> ℹ Adding new coordinate system, which will replace the existing one.  ENVIbs.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/GettingStarted.html","id":"comparision-of-different-sampling-schemes","dir":"Articles","previous_headings":"","what":"Comparision of different sampling schemes","title":"Getting Started","text":", ’ve made got maps look ! look relatively similar plotted one another, let’s plot simultaneously see ’s still case.  , top three figures appear quite similar, Opportunistic method deviating slightly form . mind isolation distance (IBD) show biggest different, seems made sense naturally occurring patchiness species range. Ecoregion SEEMS…. Environmental also seems partition feature space quite well. Notably drawing couple clusters Pacific lowlands Northern Andes mountains.","code":"gbs.p + pbs.p + eas.p + os.p  +  ibdbs.p + ebs.p + ENVIbs.p +    plot_layout(ncol = 3)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"prepare-data","dir":"Articles","previous_headings":"","what":"prepare data","title":"Rare Species Sampling Schema","text":"Load required packages. use Bradypus data included dismo package . functions package handle sf objects directly, function actually just use simple data frame sites, simplfy handing data C++ optimization routines. input maximizeDispersion function list two elements: distance matrix, data frame site locations attributes. data frame must contain following columns. second required element, distance matrix, can calculated greatCircleDistance function package. Please use rather st_distance sf consistency, units differ slightly. want use sf::st_distance, make sure convert units match scale greatCircleDistance function, otherwise results incorrect. optimization routine requires least one ‘required’ site specified. select site closest geographic center sites required site. Normally can refer existing accessions, administrative units, preserves helping implement germplasm collection, fortunate enough already samples least guaranteed access. function bootstraps sites simulate true distribution distribution species, also bootstraps coordinate uncertainty site. randomly assign 20% sites coordinate uncertainty 1 km 40 km. Note argument always meters.","code":"library(safeHavens) library(ggplot2) x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326) n_sites <- nrow(x)  df <- data.frame(   site_id = seq_len(n_sites),   required = FALSE,   coord_uncertainty = 0,    lon = sf::st_coordinates(x)[,1],    lat = sf::st_coordinates(x)[,2] )  head(df) #>   site_id required coord_uncertainty      lon      lat #> 1       1    FALSE                 0 -65.4000 -10.3833 #> 2       2    FALSE                 0 -65.3833 -10.3833 #> 3       3    FALSE                 0 -65.1333 -16.8000 #> 4       4    FALSE                 0 -63.6667 -17.4500 #> 5       5    FALSE                 0 -63.8500 -17.4000 #> 6       6    FALSE                 0 -64.4167 -16.0000 dist_mat <- sapply(1:nrow(df), function(i) {    greatCircleDistance(      df$lat[i], df$lon[i],      df$lat, df$lon    )  }) dists2c <- greatCircleDistance(   median(df$lat),    median(df$lon),    df$lat,    df$lon ) df[order(dists2c)[1],'required'] <- TRUE uncertain_sites <- sample(   setdiff(seq_len(n_sites),    which(df$required)),    size = round(n_sites*0.2, 0)   ) df$coord_uncertainty[uncertain_sites] <- runif(length(uncertain_sites), 1000, 40000) # meters"},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"run-dispersion-maximization-based-only-on-geographic-distances","dir":"Articles","previous_headings":"","what":"Run dispersion maximization based only on geographic distances","title":"Rare Species Sampling Schema","text":"input function distance matrix, site data. funtion maximizeDispersion several parameters control optimization routine. function operates relatively quick bootstraps sites, take considerably longer complex scenarios. recommened using least 999 bootstraps real world applications.","code":"test_data <- list(   distances = dist_mat,   sites = df   )  str(test_data) #> List of 2 #>  $ distances: num [1:116, 1:116] 0 1.83 714.09 807.71 797.93 ... #>  $ sites    :'data.frame':   116 obs. of  5 variables: #>   ..$ site_id          : int [1:116] 1 2 3 4 5 6 7 8 9 10 ... #>   ..$ required         : logi [1:116] FALSE FALSE FALSE FALSE FALSE FALSE ... #>   ..$ coord_uncertainty: num [1:116] 0 20455 0 0 37994 ... #>   ..$ lon              : num [1:116] -65.4 -65.4 -65.1 -63.7 -63.9 ... #>   ..$ lat              : num [1:116] -10.4 -10.4 -16.8 -17.4 -17.4 ...  rm(x, n_sites, uncertain_sites, dists2c) st <- system.time( {     geo_res <- maximizeDispersion(  ## reduce some parameters for faster run.        input_data = test_data,       n_sites = 10,       lambda_var = 0.05,       n_bootstrap = 500,       objective = \"sum\",       n_local_search_iter = 50,       n_restarts = 2     )   } ) #> Sites: 116 | Seeds: 1 | Requested: 10 | Coord. Uncertain: 23 | BS Replicates: 500 #>   |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |===                                                                   |   5%  |                                                                              |====                                                                  |   5%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   7%  |                                                                              |=====                                                                 |   8%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |   9%  |                                                                              |=======                                                               |  10%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |==========                                                            |  15%  |                                                                              |===========                                                           |  15%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |============                                                          |  18%  |                                                                              |=============                                                         |  18%  |                                                                              |=============                                                         |  19%  |                                                                              |==============                                                        |  19%  |                                                                              |==============                                                        |  20%  |                                                                              |==============                                                        |  21%  |                                                                              |===============                                                       |  21%  |                                                                              |===============                                                       |  22%  |                                                                              |================                                                      |  22%  |                                                                              |================                                                      |  23%  |                                                                              |=================                                                     |  24%  |                                                                              |=================                                                     |  25%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  27%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |=====================                                                 |  31%  |                                                                              |======================                                                |  31%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |========================                                              |  35%  |                                                                              |=========================                                             |  35%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |==========================                                            |  38%  |                                                                              |===========================                                           |  38%  |                                                                              |===========================                                           |  39%  |                                                                              |============================                                          |  39%  |                                                                              |============================                                          |  40%  |                                                                              |============================                                          |  41%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  44%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |=================================                                     |  48%  |                                                                              |==================================                                    |  48%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |===================================                                   |  51%  |                                                                              |====================================                                  |  51%  |                                                                              |====================================                                  |  52%  |                                                                              |=====================================                                 |  52%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |======================================                                |  55%  |                                                                              |=======================================                               |  55%  |                                                                              |=======================================                               |  56%  |                                                                              |========================================                              |  57%  |                                                                              |========================================                              |  58%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  59%  |                                                                              |==========================================                            |  60%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |=============================================                         |  65%  |                                                                              |==============================================                        |  65%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |===============================================                       |  68%  |                                                                              |================================================                      |  68%  |                                                                              |================================================                      |  69%  |                                                                              |=================================================                     |  69%  |                                                                              |=================================================                     |  70%  |                                                                              |=================================================                     |  71%  |                                                                              |==================================================                    |  71%  |                                                                              |==================================================                    |  72%  |                                                                              |===================================================                   |  72%  |                                                                              |===================================================                   |  73%  |                                                                              |====================================================                  |  74%  |                                                                              |====================================================                  |  75%  |                                                                              |=====================================================                 |  75%  |                                                                              |=====================================================                 |  76%  |                                                                              |======================================================                |  77%  |                                                                              |======================================================                |  78%  |                                                                              |=======================================================               |  78%  |                                                                              |=======================================================               |  79%  |                                                                              |========================================================              |  79%  |                                                                              |========================================================              |  80%  |                                                                              |========================================================              |  81%  |                                                                              |=========================================================             |  81%  |                                                                              |=========================================================             |  82%  |                                                                              |==========================================================            |  82%  |                                                                              |==========================================================            |  83%  |                                                                              |===========================================================           |  84%  |                                                                              |===========================================================           |  85%  |                                                                              |============================================================          |  85%  |                                                                              |============================================================          |  86%  |                                                                              |=============================================================         |  87%  |                                                                              |=============================================================         |  88%  |                                                                              |==============================================================        |  88%  |                                                                              |==============================================================        |  89%  |                                                                              |===============================================================       |  89%  |                                                                              |===============================================================       |  90%  |                                                                              |===============================================================       |  91%  |                                                                              |================================================================      |  91%  |                                                                              |================================================================      |  92%  |                                                                              |=================================================================     |  92%  |                                                                              |=================================================================     |  93%  |                                                                              |==================================================================    |  94%  |                                                                              |==================================================================    |  95%  |                                                                              |===================================================================   |  95%  |                                                                              |===================================================================   |  96%  |                                                                              |====================================================================  |  97%  |                                                                              |====================================================================  |  98%  |                                                                              |===================================================================== |  98%  |                                                                              |===================================================================== |  99%  |                                                                              |======================================================================|  99%  |                                                                              |======================================================================| 100% st #>    user  system elapsed  #>  28.366   0.024  28.393 rm(st)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"return-output-structure","dir":"Articles","previous_headings":"Run dispersion maximization based only on geographic distances","what":"return output structure","title":"Rare Species Sampling Schema","text":"Various elements returned output list. stability score shows often frquently selected network sites selected bootstrapped runs. stability data frame shows often site selected across bootstrap runs. Many users may find combindation input data columns, need carry results. Run parameters saved settings element.","code":"str(geo_res) #> List of 5 #>  $ input_data     :'data.frame': 116 obs. of  9 variables: #>   ..$ site_id          : int [1:116] 47 7 16 109 115 2 20 66 110 88 ... #>   ..$ required         : logi [1:116] TRUE FALSE FALSE FALSE FALSE FALSE ... #>   ..$ coord_uncertainty: num [1:116] 0 0 0 0 0 ... #>   ..$ lon              : num [1:116] -74.3 -63.2 -49.5 -74.5 -66.8 ... #>   ..$ lat              : num [1:116] 4.58 -17.8 -1 -8.38 10.37 ... #>   ..$ cooccur_strength : num [1:116] 4501 4500 4500 4500 4500 ... #>   ..$ is_seed          : logi [1:116] TRUE FALSE FALSE FALSE FALSE FALSE ... #>   ..$ selected         : logi [1:116] TRUE TRUE TRUE TRUE TRUE TRUE ... #>   ..$ sample_rank      : int [1:116] 1 2 2 2 2 3 4 5 6 7 ... #>  $ selected_sites : int [1:10] 2 7 16 20 47 66 88 109 110 115 #>  $ stability_score: num 0.418 #>  $ stability      :'data.frame': 116 obs. of  3 variables: #>   ..$ site_id         : int [1:116] 47 7 16 109 115 2 20 66 110 88 ... #>   ..$ cooccur_strength: num [1:116] 4501 4500 4500 4500 4500 ... #>   ..$ is_seed         : logi [1:116] TRUE FALSE FALSE FALSE FALSE FALSE ... #>  $ settings       :'data.frame': 1 obs. of  6 variables: #>   ..$ n_sites     : num 10 #>   ..$ n_bootstrap : num 500 #>   ..$ objective   : chr \"sum\" #>   ..$ lambda      : num 0.05 #>   ..$ dropout_prob: num 0.15 #>   ..$ n_uncertain : int 23 head(geo_res$stability_score) #> [1] 0.418 head(geo_res$stability) #>     site_id cooccur_strength is_seed #> 47       47             4501    TRUE #> 7         7             4500   FALSE #> 16       16             4500   FALSE #> 109     109             4500   FALSE #> 115     115             4500   FALSE #> 2         2             3942   FALSE head(geo_res$input_data) #>     site_id required coord_uncertainty      lon      lat cooccur_strength #> 47       47     TRUE              0.00 -74.3000   4.5833             4501 #> 7         7    FALSE              0.00 -63.1667 -17.8000             4500 #> 16       16    FALSE              0.00 -49.5000  -1.0000             4500 #> 109     109    FALSE              0.00 -74.5333  -8.3833             4500 #> 115     115    FALSE              0.00 -66.8333  10.3667             4500 #> 2         2    FALSE          20454.98 -65.3833 -10.3833             3942 #>     is_seed selected sample_rank #> 47     TRUE     TRUE           1 #> 7     FALSE     TRUE           2 #> 16    FALSE     TRUE           2 #> 109   FALSE     TRUE           2 #> 115   FALSE     TRUE           2 #> 2     FALSE     TRUE           3 head(geo_res$settings) #>   n_sites n_bootstrap objective lambda dropout_prob n_uncertain #> 1      10         500       sum   0.05         0.15          23"},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"visualize-the-selection-results","dir":"Articles","previous_headings":"Run dispersion maximization based only on geographic distances","what":"visualize the selection results","title":"Rare Species Sampling Schema","text":"","code":"ggplot(data = geo_res$input_data,    aes(     x = lon,      y = lat,      shape = required,      size = cooccur_strength,     color = selected     )   ) +   geom_point() +   # ggrepel::geom_label_repel(aes(label = site_id), size = 4) +    theme_minimal() +    labs(title = 'Priority Selection Status of Sites') ggplot(data = geo_res$input_data,    aes(     x = lon,      y = lat,      shape = required,      size = -sample_rank,     color = sample_rank     )   ) +   geom_point() +   # ggrepel::geom_label_repel(aes(label = sample_rank), size = 4) +   theme_minimal()"},{"path":[]},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"extract-prep-environmental-distances","dir":"Articles","previous_headings":"run dispersion maximization with geographic and environmental distances","what":"extract prep environmental distances","title":"Rare Species Sampling Schema","text":"environmental distances, use PCA transformation environmental variables. simply scrape 100 random points raster layers calculate PCA. predict PCA raster layers across entire study area. take first two layers, calculate environmental distances based two layers.  keep first two PCA layers environmental distance calculation. layers increase dimenstionality, may lead less useful results. Note ’s fine use euclidean distance calculation , values truly position plot.  Adding second distance matrix mix slows things considerable. Note function jittered points, point jittered randomly geographic component, environmental distance matrix also jittered. jitter latter matrix come fitting linear model environmental distance fit function geographic distance. conditional mean used noise alter environmental distance /record. However, unrelated slowdown.","code":"files <- list.files(   path = file.path(system.file(package=\"dismo\"), 'ex'),    pattern = 'grd',  full.names=TRUE ) predictors <- terra::rast(files) # import the independent variables rm(files) pts <- terra::spatSample(predictors, 100, na.rm = TRUE) pts <- pts[, names(pts)!='biome' ] # remove categorical variable for distance calc  pca_results <- stats::prcomp(pts, scale = TRUE) round(pca_results$sdev^2 / sum(pca_results$sdev^2), 2) # variance explained #> [1] 0.49 0.30 0.13 0.06 0.02 0.00 0.00 0.00 pca_raster <- terra::predict(predictors, pca_results)  terra::plot(terra::subset(pca_raster, c(1:2))) # prediction of the pca onto a new raster rm(pts, predictors, pca_results) env_values <- terra::extract(pca_raster,    sf::st_coordinates(     sf::st_as_sf(       df,        coords = c('lon', 'lat'),        crs = 4326     )   ) )[,1:2] plot(env_values, main = 'environmental distance of points from first two PCA axis') env_dist_mat <- as.matrix(     dist(env_values)   )  rm(pca_raster) test_data <- list(   distances = simplify2array(list(dist_mat, env_dist_mat)),   sites = df   )  st <- system.time(    {     env_res <- maximizeDispersion(  ## reduce some parameters for faster run.        input_data = test_data,       n_sites = 10,       lambda_var = 0.2,       weight_1 = 0.1,        weight_2 = 0.9,       n_bootstrap = 99, # considerably slower function!!!       objective = \"sum\",       n_local_search_iter = 50,       n_restarts = 2     )   } ) #> Sites: 116 | Seeds: 1 | Requested: 10 | Coord. Uncertain: 23 | BS Replicates: 99 #>   |                                                                              |                                                                      |   0%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  15%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |=============                                                         |  19%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |================                                                      |  23%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  27%  |                                                                              |====================                                                  |  28%  |                                                                              |=====================                                                 |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  31%  |                                                                              |=======================                                               |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  35%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |============================                                          |  40%  |                                                                              |=============================                                         |  41%  |                                                                              |==============================                                        |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  44%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  48%  |                                                                              |===================================                                   |  49%  |                                                                              |===================================                                   |  51%  |                                                                              |====================================                                  |  52%  |                                                                              |=====================================                                 |  53%  |                                                                              |=====================================                                 |  54%  |                                                                              |======================================                                |  55%  |                                                                              |=======================================                               |  56%  |                                                                              |========================================                              |  57%  |                                                                              |========================================                              |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  60%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |=============================================                         |  65%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |===============================================                       |  68%  |                                                                              |================================================                      |  69%  |                                                                              |=================================================                     |  70%  |                                                                              |=================================================                     |  71%  |                                                                              |==================================================                    |  72%  |                                                                              |===================================================                   |  73%  |                                                                              |====================================================                  |  74%  |                                                                              |====================================================                  |  75%  |                                                                              |=====================================================                 |  76%  |                                                                              |======================================================                |  77%  |                                                                              |======================================================                |  78%  |                                                                              |=======================================================               |  79%  |                                                                              |========================================================              |  80%  |                                                                              |=========================================================             |  81%  |                                                                              |=========================================================             |  82%  |                                                                              |==========================================================            |  83%  |                                                                              |===========================================================           |  84%  |                                                                              |===========================================================           |  85%  |                                                                              |============================================================          |  86%  |                                                                              |=============================================================         |  87%  |                                                                              |==============================================================        |  88%  |                                                                              |==============================================================        |  89%  |                                                                              |===============================================================       |  90%  |                                                                              |================================================================      |  91%  |                                                                              |================================================================      |  92%  |                                                                              |=================================================================     |  93%  |                                                                              |==================================================================    |  94%  |                                                                              |==================================================================    |  95%  |                                                                              |===================================================================   |  96%  |                                                                              |====================================================================  |  97%  |                                                                              |===================================================================== |  98%  |                                                                              |===================================================================== |  99%  |                                                                              |======================================================================| 100%  rm(dist_mat, env_dist_mat) st #>    user  system elapsed  #>  98.727   0.005  98.742 rm(st) head(env_res$stability_score) #> [1] 0.7373737 ggplot(data = env_res$input_data,    aes(     x = lon,      y = lat,      shape = required,      size = cooccur_strength,     color = selected     )   ) +   geom_point() +   # ggrepel::geom_label_repel(aes(label = site_id), size = 4) +    theme_minimal() +    labs(title = 'Priority Selection Status of Sites')"},{"path":[]},{"path":"https://sagesteppe.github.io/safeHavens/articles/RareSpecies.html","id":"bonus-altnerative-methods-for-required-central-points","dir":"Articles","previous_headings":"","what":"Bonus: altnerative methods for required central points","title":"Rare Species Sampling Schema","text":"example use point median geographic center populations. three options can combined feed three center points required sampling locations, providing robust set ‘core’ diversity species. Personally consider ‘pop centered’ population important required site center design . just showcase positions  give results like","code":"dens <- with(df, MASS::kde2d(lon, lat, n = 200)) max_idx <- which(dens$z == max(dens$z), arr.ind = TRUE)[1,] max_point <- c(dens$x[max_idx[1]], dens$y[max_idx[2]])  pops_centre <- sweep(df[c('lon', 'lat')], 2, max_point, \"-\") pop_centered_id <- which.min(rowSums(abs(pops_centre^2)))  rm(dens, max_idx, max_point, pops_centre) env_centered <- sweep(env_values, 2, sapply(env_values, median), \"-\") env_centered_id <- which.min(rowSums(abs(env_centered^2)))  rm(env_values) # geographic centroid was pt 47 centers <- df[ c(env_centered_id, pop_centered_id, 47), ]  centers$type <- c('Environmental', 'Population', 'Geographic')  ggplot(data = df, aes(x = lon, y = lat)) +   geom_point() +    geom_point(data = centers,  col = '#FF1493', size = 4) +    ggrepel::geom_label_repel(data = centers, aes(label = type)) +    theme_minimal() +    labs(title = 'Possbilities for required centers') rm(env_centered_id, env_centered, pop_centered_id) test_data$sites$required[centers$site_id] <- TRUE  env_res <- maximizeDispersion(  ## reduce some parameters for faster run.    input_data = test_data,   n_sites = 10,   lambda_var = 0.1,   weight_1 = 0.5,    weight_2 = 0.5,   n_bootstrap = 99,    objective = \"sum\",   n_local_search_iter = 50,   n_restarts = 2 ) #> Sites: 116 | Seeds: 3 | Requested: 10 | Coord. Uncertain: 23 | BS Replicates: 99 #>   |                                                                              |                                                                      |   0%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  15%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |=============                                                         |  19%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |================                                                      |  23%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  27%  |                                                                              |====================                                                  |  28%  |                                                                              |=====================                                                 |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  31%  |                                                                              |=======================                                               |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  35%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  39%  |                                                                              |============================                                          |  40%  |                                                                              |=============================                                         |  41%  |                                                                              |==============================                                        |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  44%  |                                                                              |================================                                      |  45%  |                                                                              |=================================                                     |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  48%  |                                                                              |===================================                                   |  49%  |                                                                              |===================================                                   |  51%  |                                                                              |====================================                                  |  52%  |                                                                              |=====================================                                 |  53%  |                                                                              |=====================================                                 |  54%  |                                                                              |======================================                                |  55%  |                                                                              |=======================================                               |  56%  |                                                                              |========================================                              |  57%  |                                                                              |========================================                              |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  60%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |=============================================                         |  65%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |===============================================                       |  68%  |                                                                              |================================================                      |  69%  |                                                                              |=================================================                     |  70%  |                                                                              |=================================================                     |  71%  |                                                                              |==================================================                    |  72%  |                                                                              |===================================================                   |  73%  |                                                                              |====================================================                  |  74%  |                                                                              |====================================================                  |  75%  |                                                                              |=====================================================                 |  76%  |                                                                              |======================================================                |  77%  |                                                                              |======================================================                |  78%  |                                                                              |=======================================================               |  79%  |                                                                              |========================================================              |  80%  |                                                                              |=========================================================             |  81%  |                                                                              |=========================================================             |  82%  |                                                                              |==========================================================            |  83%  |                                                                              |===========================================================           |  84%  |                                                                              |===========================================================           |  85%  |                                                                              |============================================================          |  86%  |                                                                              |=============================================================         |  87%  |                                                                              |==============================================================        |  88%  |                                                                              |==============================================================        |  89%  |                                                                              |===============================================================       |  90%  |                                                                              |================================================================      |  91%  |                                                                              |================================================================      |  92%  |                                                                              |=================================================================     |  93%  |                                                                              |==================================================================    |  94%  |                                                                              |==================================================================    |  95%  |                                                                              |===================================================================   |  96%  |                                                                              |====================================================================  |  97%  |                                                                              |===================================================================== |  98%  |                                                                              |===================================================================== |  99%  |                                                                              |======================================================================| 100%  ggplot(data = env_res$input_data,    aes(     x = lon,      y = lat,      shape = required,      size = cooccur_strength,     color = selected     )   ) +   geom_point() +    theme_minimal() +    labs(title = 'Priority Selection Status of Sites') rm(centers)"},{"path":[]},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"background","dir":"Articles","previous_headings":"Species Distribution Modelling","what":"Background","title":"Species Distribution Models","text":"vignette details steps required create Species Distribution Model (SDM) using functions provided safeHavens. SDM required use EnvironmentalBasedSample function, complex sampling scheme provided package. SDM created using elastic net generalized linear model implemented via glmnet package. SDM post-processed create binary raster map suitable unsuitable habitat, used rescale environmental predictor variables according contributions model. rescaled variables used clustering algorithm partition species range environmentally distinct regions germplasm sampling. goal SDM’s create model accurately predicts species located environmental space can predicted geographic space. goal models understand degree direction various environmental features correlate species observed range. model intended replace well-structured thought-SDM; rather, intended provide quick model can used inform sampling strategies.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"about","dir":"Articles","previous_headings":"Species Distribution Modelling","what":"About","title":"Species Distribution Models","text":"authors prone creating many ‘advanced’ SDMs models -house pipelines developing , think juice worth squeeze rely powerhouse R packages heavy lifting. main packages used dismo, caret, glmnet, CAST, terra. dismo package provides user-friendly interface working species occurrence raster data, well helper functions thresholding evaluating models. caret package provides nice interface working machine learning models, glmnet package provides elastic net model. CAST package provides functions spatially informed cross-validation, important SDMs. terra package provides functions working raster data. packages well documented, maintained.","code":""},{"path":[]},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"prep-data","dir":"Articles","previous_headings":"Steps to create an SDM","what":"prep data","title":"Species Distribution Models","text":"First need georeference occurrence data species interest. common species recommened rgbif package download occurrence data GBIF. maintain smaller package size, integrate tutorial dismo use built data set Bradypus variegatus (Brown-throated sloth). also leverage raster data provided dismo well.","code":"library(safeHavens) x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)  planar_proj <- 3857 # Web Mercator for planar distance calcs  files <- list.files(   path = file.path(system.file(package=\"dismo\"), 'ex'),    pattern = 'grd',  full.names=TRUE ) predictors <- terra::rast(files) # import the independent variables"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"fit-the-model","dir":"Articles","previous_headings":"Steps to create an SDM","what":"fit the model","title":"Species Distribution Models","text":"fit SDM using elasticSDM. arguments requires occurrence data x, raster stack predictors, quantile_v offset used create pseudo-absence data, planar_proj used calculate distances occurrence data possible pseudo-absence points. quantile_v used create pseudo-absence data calculating distance occurrence point nearest neighbor, selecting quantile distances create buffer around occurrence point. Points outside buffer used pseudo-absences. hood function uses caret help glmnet, provides output easy explore interact . Elastic net modelS bridge world lasso ridge regression blending alpha parameter. Lasso alpha 0, Ridge alpha 1. Lasso regression perform automated variable selection, can drop (‘shrink’) features model, whereas Ridge keep variables correlated features exist split contributions . elastic net blends propensity drop retain variables whenever used. caret test range alphas determine much ridge lasso regression characteristics best data hand. MAKE NOTE PCNM MORAN EIGNENVECTORS .","code":"sdModel <- elasticSDM(   x = x,   predictors = predictors,   quantile_v = 0.025,   planar_proj = planar_proj   ) #> Warning:  #> Grid searches over lambda (nugget and sill variances) with  minima at the endpoints:  #>   (GCV) Generalized Cross-Validation  #>    minimum at  right endpoint  lambda  =  1.656291e-07 (eff. df= 174.8 )"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"explore-the-output","dir":"Articles","previous_headings":"Steps to create an SDM > fit the model","what":"explore the output","title":"Species Distribution Models","text":"can see elastic net decided top model. used Accuracy rather Kappa main criterion model selection. can see selected model works predicting state test data. Note accuracy results slightly higher CV folds. bug, CV folds testing holdouts, brand new set holdouts. main reason confusion matrix results likely higher due spatial auto-correlation address typical ‘random split’ test train data. order minimize effects spatial-autocorrelation model use CAST hood allows spatially informed cross validation. Consider output CVStructure bit realistic.","code":"sdModel$CVStructure #> glmnet  #>  #> 184 samples #>   6 predictor #>   2 classes: '0', '1'  #>  #> No pre-processing #> Resampling: Bootstrapped (25 reps)  #> Summary of sample sizes: 184, 184, 184, 184, 184, 184, ...  #> Resampling results across tuning parameters: #>  #>   alpha  lambda        Accuracy   Kappa     #>   0.10   0.0006204077  0.8360016  0.6712790 #>   0.10   0.0062040769  0.8282214  0.6560044 #>   0.10   0.0620407692  0.8279379  0.6559269 #>   0.55   0.0006204077  0.8365349  0.6722483 #>   0.55   0.0062040769  0.8287096  0.6571131 #>   0.55   0.0620407692  0.8182452  0.6371623 #>   1.00   0.0006204077  0.8376016  0.6743248 #>   1.00   0.0062040769  0.8293442  0.6582937 #>   1.00   0.0620407692  0.8107098  0.6222415 #>  #> Accuracy was used to select the optimal model using the largest value. #> The final values used for the model were alpha = 1 and lambda = 0.0006204077.  coef(sdModel$Model, s = \"lambda.min\") #> 7 x 1 sparse Matrix of class \"dgCMatrix\" #>              s=lambda.min #> (Intercept)  -4.138796551 #> bio17        -0.007649011 #> biome        -0.176893842 #> bio1          0.023751253 #> bio8         -0.005606085 #> bio12         0.001275749 #> PCNM1       -18.787653011 sdModel$ConfusionMatrix #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1 #>          0 16  0 #>          1  7 22 #>                                            #>                Accuracy : 0.8444           #>                  95% CI : (0.7054, 0.9351) #>     No Information Rate : 0.5111           #>     P-Value [Acc > NIR] : 3.102e-06        #>                                            #>                   Kappa : 0.6909           #>                                            #>  Mcnemar's Test P-Value : 0.02334          #>                                            #>             Sensitivity : 1.0000           #>             Specificity : 0.6957           #>          Pos Pred Value : 0.7586           #>          Neg Pred Value : 1.0000           #>              Prevalence : 0.4889           #>          Detection Rate : 0.4889           #>    Detection Prevalence : 0.6444           #>       Balanced Accuracy : 0.8478           #>                                            #>        'Positive' Class : 1                #>"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"binarize-the-output","dir":"Articles","previous_headings":"Steps to create an SDM > fit the model","what":"binarize the output","title":"Species Distribution Models","text":"SDM’s produce continuous surfaces displaying predicted probability suitable habitat across landscape. However binary surfaes (Yes/Suitable), .e. less probable suitable habitat taxon exists particular location (grid cell)? often required ; function PostProcessSDM used binarize predictions. Going continuous surface binary surface loses information. goal binary surface understand species likely found, sample germplasm areas. many ways threshold SDM, many caveats associated process; Frank Harrell written topic post-processing statistics. Historically, assessing probability output 0.5 probability used threshold, probabilities beneath considered ‘Suitable / ’, probabilities classified ‘Suitable / Yes’. works cases, thresholding outside domain statistics realm practice. motto implementing models, slight elaboration George Box’s aphorism, “models wrong, useful - want wrong?”. goal sampling germplasm conservation maximize representation allelic diversity across range species. achieve , need understanding species actual range, hence better predict species present , predict absent actually grows. Accordingly, default thresholding statistic Sensitivity. However, function supports threshold options dismo::threshold. may wondering function named PostProcessSDM rather ThresholdSDM, reason discontinuity function performs additional processes thresholding. Geographic buffers created around intitial occurence points ensure none known occurrence points ‘missing’ output binary map. two edged sword, address notion dispersal limitation, realize suitable habitat occupied habitat. PostProcessSDM function creates cross validation folds, selects training data. fold calculates distance occurrence point ’s nearest neighbor. summarize distances can understand distribution distances quantile. use selected quantile, serve buffer. Area predicted suitable habitat outside buffer become ‘cut ’ (masked) binary raster map, areas within buffer distance known occurrences currently masked reassigned probabilities. theory behind process underdeveloped nascent, come gut analyst. Bradypus data set use 0.25 quantile, saying “Neighbors generally 100km apart, happy risk saying 25km within occurrence occupied suitable habitat”. Increasing value say 1.0 mean suitable habitat removed, decreasing makes maps conservative. cost increasing distances greatly sampling methods may puts grids many areas without populations collect . can compare results applying function side side using output function.","code":"terra::plot(sdModel$RasterPredictions) threshold_rasts <- PostProcessSDM(   rast_cont = sdModel$RasterPredictions,    test = sdModel$TestData,   planar_proj = planar_proj,   thresh_metric = 'sensitivity',    quant_amt = 0.5   ) #> 1000 prediction points are sampled from the modeldomain terra::plot(threshold_rasts$FinalRasters)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"rescale-predictor-variables","dir":"Articles","previous_headings":"Steps to create an SDM > fit the model","what":"rescale predictor variables","title":"Species Distribution Models","text":"glmnet used three main reasons, 1) gives directional coefficients documents 1 unit increase independent variable varies response. 2) maintains automated feature selection reducing work analyst needs . 3) glmnet re-scales variables model generation combined beta-coefficients allows partitioning environmental space regions environmentally similar individual species. way raster stack becomes representative model, can use values basis hierarchical cluster later .  can see variables ‘close’ scale, work clustering algorithm. layers color (maybe yellow?) means variance, ’s term shrunk model. dealt internally future function. scales variables exact, weighed coefficients model {r Show beta Coefficients model print(rr$BetaCoefficients) can also look coefficients variable. glmnet returns ‘untransformed’ variables, .e. coefficients scale input rasters, calculate BC right afterwards. safeHavens generates kinds things runs functions elasticSDM, PostProcessSDM, RescaleRasters. Given one sampling scheme may followed quite time, best practice save many objects.","code":"# CREATE A COPY OF THE RASTER PREDICTORS WHERE WE HAVE  # STANDARDIZED EACH VARIABLE - SO IT IS EQUIVALENT TO THE INPUT TO THE GLMNET # FUNCTION, AND THEN MULTIPLIED IT BY IT'S BETA COEFFICIENT FROM THE FIT MODEL # we will also write out the beta coefficients using writeSDMresults right after # this.   rr <- RescaleRasters(   model = sdModel$Model,   predictors = sdModel$Predictors,    training_data = sdModel$TrainData,    pred_mat = sdModel$PredictMatrix)  terra::plot(rr$RescaledPredictors)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"save-results","dir":"Articles","previous_headings":"Steps to create an SDM > fit the model","what":"save results","title":"Species Distribution Models","text":"","code":"bp <- '~/Documents/assoRted/StrategizingGermplasmCollections'  writeSDMresults(   cv_model = sdModel$CVStructure,    pcnm = sdModel$PCNM,    model = sdModel$Model,    cm = sdModel$ConfusionMatrix,    coef_tab = rr$BetaCoefficients,    f_rasts = threshold_rasts$FinalRasters,   thresh = threshold_rasts$Threshold,   file.path(bp, 'results', 'SDM'), 'Bradypus_test')  # we can see that the files were placed here using this.  list.files( file.path(bp, 'results', 'SDM'), recursive = TRUE )"},{"path":"https://sagesteppe.github.io/safeHavens/articles/SpeciesDistributionModels.html","id":"wrapping-up","dir":"Articles","previous_headings":"Steps to create an SDM > fit the model","what":"wrapping up","title":"Species Distribution Models","text":", steps make species distribution model - rather get coefficients species distribution model! play around example data set compare buffered distance results end - head next vignette!","code":""},{"path":"https://sagesteppe.github.io/safeHavens/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Reed Benkendorf. Author, maintainer.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Benkendorf R (2025). safeHavens: Developing sampling schemas ex situ conservation plant germplasm. R package version 0.0.0.9000, https://sagesteppe.github.io/safeHavens/.","code":"@Manual{,   title = {safeHavens: Developing sampling schemas for ex situ conservation of plant germplasm},   author = {Reed Benkendorf},   year = {2025},   note = {R package version 0.0.0.9000},   url = {https://sagesteppe.github.io/safeHavens/}, }"},{"path":[]},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"description","dir":"","previous_headings":"","what":"Description","title":"Developing sampling schemas for ex situ conservation of plant germplasm","text":"goal package provide germplasm curators easily referable spatial data sets help prioritize field collection efforts.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Developing sampling schemas for ex situ conservation of plant germplasm","text":"provides functionality seven sampling schemes various curators interested , many likely outperform others certain species areas. package also creates species distribution models, goal germplasm sampling, rather predicting ranges fine resolutions, making inference; interested functionality R several dozen packages tailored purposes.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"description-1","dir":"","previous_headings":"","what":"Description","title":"Developing sampling schemas for ex situ conservation of plant germplasm","text":"package helps germplasm curators communicate areas interest collection teams collect new material accession. provides seven different sampling approaches curators choose individual taxon hope process.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Developing sampling schemas for ex situ conservation of plant germplasm","text":"safeHavens available github. can installed using remotes devtools like : installed can attached use like package github CRAN","code":"install.packages('devtools') devtools::install_github('sagesteppe/safeHavens')  install.packages('remotes') # remotes is very similar and  a good alternativ for this use case. remotes::install_github('sagesteppe/safeHavens') library(safeHavens)"},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Developing sampling schemas for ex situ conservation of plant germplasm","text":"safeHavens seven user facing functions generating sampling schemes. species distribution modelling section couple functions essential achieving EnvironmentalBasedSample design, : elasticSDM, PostProcessSDM, RescaleRasters writeSDMresults.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"acknowledgements","dir":"","previous_headings":"","what":"Acknowledgements","title":"Developing sampling schemas for ex situ conservation of plant germplasm","text":"Edzer Pebesma, Krzysztof Dyba help seamless installations Ubuntu, getting pkgdown website running.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EcoregionBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine the sample size for n ecoregions in an area — EcoregionBasedSample","title":"Determine the sample size for n ecoregions in an area — EcoregionBasedSample","text":"Intersect vector data file ecoregions (ecoregions) range focal taxon (x) selects n ecoregions sample. fewer n ecoregions exist range species, extra samples added largest ecoregions area proportions meet n. function adequate support official Omernik L4 shapefiles created EPA, worked minimal experimentation vector data sources. ecoregions exist across range species n (common),  select method determine sample sizes (instance 1 sample per ecoregion) selected. Methods include sampling n largest, orsmallest ecoregions number, ecoregion disconnected habitat (measured soley number polygons). three methods returns largest polygon within criterion. note 'polygons'. Simple features able store polygon data two main formats, 'MULTIPOLYGON', individual polygons composing class stored collectively, 'POLYGONS' individual polygon unique entry within class. 'Polygons' generally used two areas class discontinuous, analyst wants easily analyze separately. 'MULTIPOLYGONS' generally created analyst interested understanding properties entire class. EPA Omernik spatial data set comes 'POLYGONS' 'MULTIPOLYGONS', used somewhat extensively believe creators struck happy balance creating many small polygons, e.g.  areas like coastal reef island (MULTIPOLYGON use case), big polygons. modify , rare occasion (essentially islands), refer 'polygon' may technically multipolygon.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EcoregionBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine the sample size for n ecoregions in an area — EcoregionBasedSample","text":"","code":"EcoregionBasedSample(   x,   ecoregions,   OmernikEPA,   n,   ecoregion_col,   increase_method,   decrease_method )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/EcoregionBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine the sample size for n ecoregions in an area — EcoregionBasedSample","text":"x range species simple feature (sf) object. ecoregions ecoregion vector data file (~shapefile) sf. OmernikEPA Boolean. TRUE indicates data US EPA minimally modified, FALSE several input required ensure function maps appropriately. left blank default method scan exact match standard Omernik ecoregion field (column) names, matched dispatched Omernik module, else fail unless associated columns specified (SEE ). n Numeric. desired total number samples across range ecoregion_col Character. Name column contain finest resolution data used analysis. Omernik L4 file defaults relevant columns automatically, different type file submitted, specified function fails. increase_method Character. Method implement number L4 ecoregions less n. decrease_method Character. Method implement number L4 ecoregions greater n. 'Largest' (default) select n largest ecoregions total area, select largest single polygon within classes. 'Smallest' select n smallest ecoregions total area, select largest single polygon within classes. '' select n ecoregions polygons, select largest polygon .","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EcoregionBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine the sample size for n ecoregions in an area — EcoregionBasedSample","text":"sf object, length input data set, finest resolution eco level, geometry fields retained, new column 'n' indicating many accession gather ecoregion.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EcoregionBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine the sample size for n ecoregions in an area — EcoregionBasedSample","text":"","code":"# First example is using a subset (and with simplified geometry) Omernik L4 # ecoregion shapefile from the EPA. Which as of the time of writing were  # available, at no cost, at the following URL # https://www.epa.gov/eco-research/level-iii-and-iv-ecoregions-continental-united-states  polygon <- spData::us_states |> dplyr::select(NAME) |>    dplyr::filter(NAME == 'California') |>    sf::st_transform(4326)     Weco <- sf::st_read(   file.path(system.file(package=\"safeHavens\"), 'extdata', 'WesternEcoregions.gpkg'),    quiet = TRUE) head(Weco) #> Simple feature collection with 6 features and 17 fields #> Geometry type: MULTIPOLYGON #> Dimension:     XY #> Bounding box:  xmin: -120.6736 ymin: 36.50122 xmax: -118.0553 ymax: 40.4275 #> Geodetic CRS:  WGS 84 #>      NA_L2NAME NA_L2CODE                      L3_KEY               NA_L3NAME #> 1 COLD DESERTS      10.1 13  Central Basin and Range Central Basin and Range #> 2 COLD DESERTS      10.1 13  Central Basin and Range Central Basin and Range #> 3 COLD DESERTS      10.1 13  Central Basin and Range Central Basin and Range #> 4 COLD DESERTS      10.1 13  Central Basin and Range Central Basin and Range #> 5 COLD DESERTS      10.1 13  Central Basin and Range Central Basin and Range #> 6 COLD DESERTS      10.1 13  Central Basin and Range Central Basin and Range #>   Shape_Leng             L2_KEY US_L3CODE NA_L1CODE NA_L3CODE #> 1   94308.62 10.1  COLD DESERTS        13        10    10.1.5 #> 2   49617.99 10.1  COLD DESERTS        13        10    10.1.5 #> 3 2106033.22 10.1  COLD DESERTS        13        10    10.1.5 #> 4  165631.31 10.1  COLD DESERTS        13        10    10.1.5 #> 5  510158.56 10.1  COLD DESERTS        13        10    10.1.5 #> 6   87086.56 10.1  COLD DESERTS        13        10    10.1.5 #>                NA_L1NAME                                          US_L4NAME #> 1 NORTH AMERICAN DESERTS Sierra Nevada-Influenced Semiarid Hills and Basins #> 2 NORTH AMERICAN DESERTS Sierra Nevada-Influenced Semiarid Hills and Basins #> 3 NORTH AMERICAN DESERTS Sierra Nevada-Influenced Semiarid Hills and Basins #> 4 NORTH AMERICAN DESERTS                                      Sierra Valley #> 5 NORTH AMERICAN DESERTS                                 Upper Owens Valley #> 6 NORTH AMERICAN DESERTS                                 Mono-Adobe Valleys #>                 US_L3NAME #> 1 Central Basin and Range #> 2 Central Basin and Range #> 3 Central Basin and Range #> 4 Central Basin and Range #> 5 Central Basin and Range #> 6 Central Basin and Range #>                                                     L4_KEY US_L4CODE #> 1 13aa  Sierra Nevada-Influenced Semiarid Hills and Basins      13aa #> 2 13aa  Sierra Nevada-Influenced Semiarid Hills and Basins      13aa #> 3 13aa  Sierra Nevada-Influenced Semiarid Hills and Basins      13aa #> 4                                      13ab  Sierra Valley      13ab #> 5                                 13ac  Upper Owens Valley      13ac #> 6                                 13ad  Mono-Adobe Valleys      13ad #>                       L1_KEY Shape_Area       NAME #> 1 10  NORTH AMERICAN DESERTS  285751793 California #> 2 10  NORTH AMERICAN DESERTS  103496425 California #> 3 10  NORTH AMERICAN DESERTS 6613879510 California #> 4 10  NORTH AMERICAN DESERTS  449164266 California #> 5 10  NORTH AMERICAN DESERTS 1763113790 California #> 6 10  NORTH AMERICAN DESERTS  162895833 California #>                             geom #> 1 MULTIPOLYGON (((-118.694 37... #> 2 MULTIPOLYGON (((-119.323 38... #> 3 MULTIPOLYGON (((-119.845 38... #> 4 MULTIPOLYGON (((-120.1395 3... #> 5 MULTIPOLYGON (((-118.463 37... #> 6 MULTIPOLYGON (((-118.7649 3...  out <- EcoregionBasedSample(polygon, Weco) #> Warning: attribute variables are assumed to be spatially constant throughout all geometries #> Warning: attribute variables are assumed to be spatially constant throughout all geometries sum(out$n) #> [1] 20  ggplot2::ggplot() +     ggplot2::geom_sf(data = out, ggplot2::aes(fill = factor(n)))     # This second example is from a recent publication by Morreno et al. 2022 and  # presents biogeographic regions of the Neotropics and is available from a  # google drive linked in the publication describing there creation located at # https://www.scielo.br/j/aabc/a/hPft4CK6RV8QBr8nP7bxhRQ/?lang=en#  # Essentially we showcase how a user can maintain this functions utility # while catering to data in a format differing from the Omernik L4 distribution.   neo_eco <- sf::st_read(    file.path(system.file(package=\"safeHavens\"), 'extdata', 'NeoTropicsEcoregions.gpkg'),     quiet = TRUE) sp_range <- sf::st_polygon( # complete  list(    rbind(      c(-80,-5), c(-80,10), c(-60,10), c(-55,5),      c(-60,-5), c(-80,-5)     )  ) ) |>  sf::st_sfc() |>   sf::st_as_sf() |>  sf::st_set_crs(4326) |>  dplyr::rename(geometry = x) |>  dplyr::mutate(Species = 'Da species')  out <- EcoregionBasedSample(sp_range, neo_eco, ecoregion_col = 'Provincias') #> Warning: attribute variables are assumed to be spatially constant throughout all geometries #> Warning: attribute variables are assumed to be spatially constant throughout all geometries sum(out$n) #> [1] 19  ggplot2::ggplot() +   ggplot2::geom_sf(data = neo_eco) +   ggplot2::geom_sf(data = out, ggplot2::aes(fill = factor(n))) +   ggplot2::geom_sf(data = sp_range, fill = NA, color = 'Red')    # Note that both of the files of the above ecoregions have had their geometry # simplified, i.e. made less complex - you should notice they look slightly angular # like an old cartoon such as Rugrats or so. We do this to reduce the file size # to make it easier to install the package, and reduce the run time of the functions # for these simple examples."},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"function utilizes output elastic net GLM model create weights matrix features relevant species distribution identify clusters throughout range incorporating PCNM/MEM data coordinates implement spatial contiguity.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"","code":"EnvironmentalBasedSample(   pred_rescale,   f_rasts,   lyr,   taxon,   path,   n,   fixedClusters,   n_pts,   planar_projection,   coord_wt,   buffer_d,   prop_split,   write2disk,   ... )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"pred_rescale rasterstack predictor layers rescaled represent beta coefficients elastic net (glmnet::glmnet) modelling process. See ?RescaleRasters implementation functionality. f_rasts rasters output SDM workflow. lyr Character. name layer want use analysis one : 'Threshold', 'Clipped', Supplemented'. missing defaults 'Supplemented'. taxon Character. name taxonomic entity models created. final raster clusters, results KNN classifier trainings, details clustering procedure (fixedClusters=TRUE). path root path output data saved, use WriteSDMresults. Defaults current working directory, check getwd(). n Numeric. number clusters desired. fixedClusters Boolean. Defaults TRUE, create n clusters. False use NbClust::NbClust determine number clusters. n_pts Numeric. number points use generating clusters, randomly sampled within mask area mask. Defaults 500. planar_projection Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. coord_wt Numeric. amount weigh coordinates distance matrix relative important variable identified elastic net regression. Defaults 2.5, setting 1 make value equivalent environmental PCNM variables. metric increases spatial contiguity clusters identified. buffer_d Numeric. using two-stage sampling increase sample size (number points) uncommon clusters, distance buffer individual pts located cells possible sampling. Defaults 3 allows area encompassing around 45-50 raster cells nearby possibly sampled. reasonable default coarsely gridded data (sensible grain type packages use cases), moderate resolution data (e.g. 250m-1km) may require higher values find meaningful differences variables locations. prop_split Numeric. proportion records used training KNN classifier. Defaults 0.8 use 80% records training 20% independent test sample. write2disk Boolean. Whether write results disk . Defaults FALSE. ... arguments passed NbClust::NbClust optimizing cluster numbers. Defaults using method = 'complete', compare results 20 methods select cluster number commonly generated algorithms. min.nc set default 5, overcome clusters use (easily overwritten supplying argument minc.nc=2, set lower), max.nc = 20 congruence many seed collection endeavors (overwritten max.nc = 10 example).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"Writes four objects disk, returns one object R session (optional).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Create equal area polygons over a geographic range — EqualAreaSample","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"function creates n geographic clusters geographic area (x), typically species range, using kmeans clustering.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"","code":"EqualAreaSample(   x,   n = 20,   pts = 5000,   planar_projection,   returnProjected,   reps = 100,   BS.reps = 9999 )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"x SF object terra spatraster. range generate clusters. n Numeric. number clusters desired. Defaults 20. pts Numeric. number points use generating clusters, placed grid like fashion across x. exact number points used may deviate slightly user submitted value allow equidistant spacing across x. Defaults 5,000. planar_projection Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. returnProjected Boolean. Whether return data set original input CRS (FALSE), new projection (TRUE). Defaults FALSE. reps Numeric. number times rerun voronoi algorithm, set polygons similar sizes, measured using variance areas selected. Defaults 100. BS.reps number bootstrap replicates evaluating results.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"list containing two objects, first results bootstrap simulations. second sf dataframe containing polygons smallest amount variance size.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"","code":"nc <- sf::st_read(system.file(\"shape/nc.shp\", package=\"sf\"), quiet = TRUE) |> dplyr::select(NAME)  set.seed(1) system.time(   zones <- EqualAreaSample(nc, n = 20, pts = 500, planar_projection = 32617, reps = 50) ) #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #>    user  system elapsed  #>   5.189   0.016   5.206   plot(nc, main = 'Counties of North Carolina')  plot(zones$Geometry, main = 'Clusters')  zones$SummaryData #>                  Metric       Value #> 1     variance.observed  9507223610 #> 2        quantile.0.001  9565302650 #> 3             lwr.95.CI  9507223610 #> 4             upr.95.CI 10692510134 #> 5    Voronoi.reps.asked          50 #> 6 Voronoi.reps.received          50 #> 7               BS.reps        9999"},{"path":"https://sagesteppe.github.io/safeHavens/reference/GridBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","title":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","text":"function creates 20 grid cells geographic area (x), typically species range.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/GridBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","text":"","code":"GridBasedSample(x, planar_projection, gridDimensions)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/GridBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","text":"x SF object terra spatraster. range generate clusters. planar_projection Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. gridDimensions single row form ouput TestGridSizes optimal number grids generate.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/GridBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","text":"simple features (sf) object containing final grids saving computer. See vignette questions saving two main types spatial data models (vector - used , raster).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/GridBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","text":"","code":"if (FALSE)  # not ran to bypass CRAN check time limits. ~6 seconds to treat Rhode Island.  ri <- spData::us_states |>  dplyr::filter(NAME == 'Rhode Island') |>   sf::st_transform(32615)  sizeOptions <- TestGridSizes(ri) #> Error: object 'ri' not found head(sizeOptions) # in this case let's shoot for 33 and see what happens #> Error: object 'sizeOptions' not found sizeOptions <- sizeOptions[sizeOptions$Name == 'Original',] #> Error: object 'sizeOptions' not found  output <- GridBasedSample(ri, 5070, gridDimensions = sizeOptions) #> Error: object 'ri' not found plot(output) #> Error: object 'output' not found  # \\dontrun{}"},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"Create n seed collection areas based distance geographic (great circle) distance points.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"","code":"IBDBasedSample(   x,   n,   fixedClusters,   n_pts,   template,   prop_split,   min.nc,   max.nc )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"x Raster surface sample points within, e.g. output SDM$Supplemented. n Numeric. number clusters desired. fixedClusters Boolean. Defaults TRUE, create n clusters. False use NbClust::NbClust determine optimal number clusters. n_pts Numeric. number points use generating clusters, randomly sampled within mask area mask. Defaults 1000, generally allows enough points split KNN training. template Raster. raster file can used template plotting. prop_split Numeric. proportion records used training KNN classifier. Defaults 0.8 use 80% records training 20% independent test sample. min.nc Numeric. Minimum number clusters test fixedClusters=FALSE, defaults 5. max.nc Numeric. Maximum number clusters test fixedClusters=FALSE, defaults 20.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"simple features (sf) object containing final grids saving computer. See vignette questions saving two main types spatial data models (vector - used , raster).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"","code":"planar_proj = '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs'  x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326) x_buff <- sf::st_transform(x, planar_proj) |>  sf::st_buffer(125000) |> # we are working in planar metric coordinates, we are  sf::st_as_sfc() |> # buffer by this many / 1000 kilometers.   sf::st_union()  files <- list.files( # note that for this process we need a raster rather than    path = file.path(system.file(package=\"dismo\"), 'ex'), # vector data to accomplish   pattern = 'grd',  full.names=TRUE ) # this we will 'rasterize' the vector using terra predictors <- terra::rast(files) # this can also be done using 'fasterize'. Whenever # we rasterize a product, we will need to provide a template raster that our vector # will inherit the cell size, coordinate system, etc. from   x_buff.sf <- sf::st_as_sf(x_buff) |>    dplyr::mutate(Range = 1) |>    sf::st_transform(terra::crs(predictors))  # and here we specify the field/column with our variable we want to become  # an attribute of our raster v <- terra::rasterize(x_buff.sf, predictors, field = 'Range')   # now we run the function demanding 20 areas to make accessions from,  ibdbs <- IBDBasedSample(x = v, n = 20, fixedClusters = TRUE, template = predictors) #> Warning: coordinate ranges not computed along great circles; install package lwgeom to get rid of this warning #> Loading required package: ggplot2 #> Loading required package: lattice #> Warning: st_point_on_surface may not give correct results for longitude/latitude data plot(ibdbs)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Design additional collections around already existing collections — OpportunisticSample","title":"Design additional collections around already existing collections — OpportunisticSample","text":"function utilizes regular, nearly case existing collections, grid points develop sampling scheme n polygons.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Design additional collections around already existing collections — OpportunisticSample","text":"","code":"OpportunisticSample(polygon, n, collections, reps, BS.reps)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Design additional collections around already existing collections — OpportunisticSample","text":"polygon input sf polygon, .e. species range administrative unit, sampling desired. n Numeric. total number desired collections. Defaults 20. collections sf point geometry data set existing collections made. reps arguments passed np.boot BS.reps number bootstrap replicates evaluating results.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Design additional collections around already existing collections — OpportunisticSample","text":"list containing two sublists, first 'SummaryData' details number voronoi polygons generated, results bootstrap simulations. second 'Geometry', contains final spatial data products, can written end. See vignette questions saving two main types spatial data models (vector - used , raster).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Design additional collections around already existing collections — OpportunisticSample","text":"","code":"#' Design additional collections around already existing collections ri <- spData::us_states |>   dplyr::select(NAME) |>   dplyr::filter(NAME == 'Rhode Island') |>   sf::st_transform(32617) existing_collections <- sf::st_sample(ri, size = 5) |>   sf::st_as_sf() |>   dplyr::rename(geometry = x)  system.time(   out <- OpportunisticSample(polygon = ri, BS.reps=4999)  ) # set very low for example #>    user  system elapsed  #>   6.619   0.024   6.648  # the function is actually very fast; 150 voronoi reps, with 9999 BS should only take about # 7 seconds per species so not much concern on the speed end of things. ggplot2::ggplot() +    ggplot2::geom_sf(data = out$Geometry, ggplot2::aes(fill = ID)) +    ggplot2::geom_sf(data = existing_collections)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"function utilizes regular, nearly case existing collections, grid points develop sampling scheme n polygons.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"","code":"PointBasedSample(polygon, n = 20, collections, reps = 100, BS.reps = 9999)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"polygon input sf polygon, .e. species range administrative unit, sampling desired. n Numeric. total number desired collections. Defaults 20. collections sf point geometry data set existing collections made. reps arguments passed np.boot BS.reps number bootstrap replicates evaluating results.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"list containing two objects, first results bootstrap simulations. second sf dataframe containing polygons smallest amount variance size.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"","code":"#' Utilize a grid based stratified sample for drawing up polygons ri <- spData::us_states |>   dplyr::select(NAME) |>   dplyr::filter(NAME == 'Rhode Island') |>   sf::st_transform(32617)     system.time(   out <- PointBasedSample(polygon = ri, reps = 10, BS.reps = 10) # set very low for example  ) #>    user  system elapsed  #>   0.594   0.002   0.595  # the function is actually very fast; 150 voronoi reps, with 9999 BS should only take about # 2 seconds per species so not much concern on the speed end of things! head(out$SummaryData) #>                  Metric    Value #> 1     variance.observed 16538241 #> 2        quantile.0.001 16541620 #> 3             lwr.95.CI 16538241 #> 4             upr.95.CI 17219398 #> 5    Voronoi.reps.asked       10 #> 6 Voronoi.reps.received        6 plot(out$Geometry)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":null,"dir":"Reference","previous_headings":"","what":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"last 'analytical' portion SD modelling process. produce binary (Yes/ NA) rasters species suitable habitat based three step process. first step uses dismo::thresholds determine feature raster want maximuize, case want raster less likely capture presence omit . can subset predicted habitat, dispersed comparing nearest neighbor distances observed points. Finally, can add back areas raster know species observed, hopefully missed original SDM, always suspicious points difficult fit light inertia rest species.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"","code":"PostProcessSDM(   rast_cont,   test,   train,   thresh_metric,   quant_amt,   planar_projection )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"rast_cont raw unaltered (except masked) raster predictions (x$RasterPredictions). test test data partition elasticSDM function (x$TestData). train train data partition elasticSDM function (x$TrainData). thresh_metric ?dismo::threshold options, defaults 'sensitivity' quant_amt quantile nearest neighbors distance use steps 2 3. defaults 0.25, using median nearest neighbor distance 10 bootstrapping replicates estimating buffer restrict SDM surface , minimum 10 bootstrap reps adding surface presence points placed binary suitable habitat. planar_projection Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"list containing two options. 1) spatraster 4 layers, ) continuous probabilities suitable habitat feed elasticSDM, B) raster binary format based specified thresholding statistic, C) binary raster B + habitat clipped buffer distances determined measuring nearest neighbor distances thresholding quantile D) binary raster C, adding distance points initially cells classified thresholding suitable habitat. D' general basis future steps, either B, C serve alternatives. 2) threshold statistics calculated dismo dataframe.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PrioritizeSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine which areas of a sample unit should be prioritized — PrioritizeSample","title":"Determine which areas of a sample unit should be prioritized — PrioritizeSample","text":"function offers two ways enforce prioritization within individual sample units returned *Sample function. goal either method avoid collectors teams 'cheat' system repeatedly collecting along border two grid cells. understand many teams may collecting closely due species biology, land management, restrictions, goal function try guide dispersing activity. method used computes geometric centroid region, center falls outside grid, snapped back onto nearest location default. centers cell calculated remaining area grid distances calculated centers locations. final processing n_breaks applied based distances desired cell center partition space different priority collection units. Note submitting data ecoregion based sample, column n, must maintained.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PrioritizeSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine which areas of a sample unit should be prioritized — PrioritizeSample","text":"","code":"PrioritizeSample(x, method, reps, n_breaks, verbose = TRUE)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PrioritizeSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine which areas of a sample unit should be prioritized — PrioritizeSample","text":"x sf/tibble/dataframe. set sample grids *Sample functions method Character. method use prioritization. Currently 'centered' implemented. reps Numeric. number repetitions used sampling design. used messaging purposes time. n_breaks Numeric. number breaks return function, defaults 3. Values much higher untested, beyond 5 questionable utility. verbose Bool. Whether print messages console , defaults TRUE.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PrioritizeSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine which areas of a sample unit should be prioritized — PrioritizeSample","text":"","code":"if (FALSE) { # \\dontrun{ nc <- sf::st_read(system.file(\"shape/nc.shp\", package=\"sf\"), quiet = TRUE) |> dplyr::select(NAME) |>   sf::st_transform(5070) # should be in planar coordinate system.   set.seed(1) zones <- EqualAreaSample(nc, n = 20, pts = 1000, planar_projection = 32617, reps = 100)  # the function requires an input sampling strategy to create the prioritization areas ps <- PrioritizeSample(zones$Geometry, method = 'centered', n_breaks = 3)  ggplot2::ggplot() +   ggplot2::geom_sf(data = ps,  aes(fill = factor(Level))) +  ggplot2::geom_sf(data = zones$Geometry, color = 'red', fill = NA, linewidth = 1)  } # }"},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":null,"dir":"Reference","previous_headings":"","what":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"rescaled rasters can used clustering, predicting results cluster analysis back space final product.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"","code":"RescaleRasters(model, predictors, training_data, pred_mat)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"model final output model glmnet elasticSDM predictors raster stack use process elasticSDM training_data data went glmnet model, used calculating variance required scaling process. elasticSDM pred_mat Prediction matrix elasticSDM","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"list two objects. 1) rescaled raster stack. 2) table standardized unstandardized coefficients glmnet model.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/TestGridSizes.html","id":null,"dir":"Reference","previous_headings":"","what":"Get an estimate for how many grids to draw over a species range — TestGridSizes","title":"Get an estimate for how many grids to draw over a species range — TestGridSizes","text":"function uses dimensions species grid estimate many grids need added x y directions cover 20 grid cells roughly equal areas.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/TestGridSizes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get an estimate for how many grids to draw over a species range — TestGridSizes","text":"","code":"TestGridSizes(target)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/TestGridSizes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get an estimate for how many grids to draw over a species range — TestGridSizes","text":"target species range simple feature (sf) object.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/TestGridSizes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get an estimate for how many grids to draw over a species range — TestGridSizes","text":"dataframe testing results grid combination. user needs select optimal grid size based tradeoff minimizing variance, without creating many grids need erased. Rhode Island example use 'Original' option asks 4 x grids 7 y grids.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/TestGridSizes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get an estimate for how many grids to draw over a species range — TestGridSizes","text":"","code":"ri <- spData::us_states |> dplyr::select(NAME) |>    dplyr::filter(NAME == 'Rhode Island') |>    sf::st_transform(32617)  sizeOptions <- TestGridSizes(ri) head(sizeOptions) #>       Name Grids    Variance GridNOx GridNOy #> 1 Smallest    49    9.242133       6       9 #> 2  Smaller    37  355.799568       5       8 #> 3 Original    25 1038.567100       4       7 #> 4   Larger    16 1245.553777       3       6 #> 5  Largest    11 1322.092849       2       5"},{"path":"https://sagesteppe.github.io/safeHavens/reference/VoronoiSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a voronoi sample of an area n times — VoronoiSampler","title":"Make a voronoi sample of an area n times — VoronoiSampler","text":"Split area n polygons roughly equal area, optionally removing default points replacing existing collections build future collections around.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/VoronoiSampler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a voronoi sample of an area n times — VoronoiSampler","text":"","code":"VoronoiSampler(polygon, n, collections, reps)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/VoronoiSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a voronoi sample of an area n times — VoronoiSampler","text":"polygon input sf polygon, .e. species range administrative unit, sampling desired. n Numeric. total number desired collections. Defaults 20. collections sf point geometry data set existing collections made. reps Numeric. number times rerun voronoi algorithm, set polygons similar sizes, measured using variance areas selected. Defaults 150, may accomplish around 100 succesful iterations.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/WesternEcoregions.html","id":null,"dir":"Reference","previous_headings":"","what":"Omernik level 4 ecoregions of California and Oregon — WesternEcoregions","title":"Omernik level 4 ecoregions of California and Oregon — WesternEcoregions","text":"vector data set Omernik level 4 ecoregions clipped California Oregon. Downloaded https://www.epa.gov/eco-research/level-iii--iv-ecoregions-continental-united-states simplified using mapshaper. vector data set Bioregions developed Morrone et al. 2022 Downloaded https://neotropicalmap.atlasbiogeografico.com/ simplified using mapshaper.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/WesternEcoregions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Omernik level 4 ecoregions of California and Oregon — WesternEcoregions","text":"","code":"data(WesternEcoregions)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/WesternEcoregions.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Omernik level 4 ecoregions of California and Oregon — WesternEcoregions","text":"spatial vector data set. spatial vector data set.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/WesternEcoregions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Omernik level 4 ecoregions of California and Oregon — WesternEcoregions","text":"L4_KEY. Full name codes Level 4 ecoregions US_L4CODE. Codes level 4 ecoregions US_L4NAME. Names level 4 ecoregions Provincias Full name bioregion","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/assignGrid_pts.html","id":null,"dir":"Reference","previous_headings":"","what":"place random points in the polygon which will be dissolved with the larger polygons — assignGrid_pts","title":"place random points in the polygon which will be dissolved with the larger polygons — assignGrid_pts","text":"function ran within GridBasedSample place points throughout polygon geometries merged larger polygons assign neighboring polygons based much area want grow polygons .","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/assignGrid_pts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"place random points in the polygon which will be dissolved with the larger polygons — assignGrid_pts","text":"","code":"assignGrid_pts(neighb_grid, focal_grid, props, nf_pct)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/assignGrid_pts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"place random points in the polygon which will be dissolved with the larger polygons — assignGrid_pts","text":"neighb_grid  focal_grid  props  nf_pct","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/createPCNM_fitModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Create global/regional PCNM surfaces and fit elastic net regression to all covariates — createPCNM_fitModel","title":"Create global/regional PCNM surfaces and fit elastic net regression to all covariates — createPCNM_fitModel","text":"function lifting SDM workflow. create PCNM/MEM surfaces subset local/global maps. can use Thin plate regression predict onto actual raster surface can used prediction downstream.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/createPCNM_fitModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create global/regional PCNM surfaces and fit elastic net regression to all covariates — createPCNM_fitModel","text":"","code":"createPCNM_fitModel(x, planar_proj, ctrl, indices_knndm, sub, test)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/createPCNM_fitModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create global/regional PCNM surfaces and fit elastic net regression to all covariates — createPCNM_fitModel","text":"x training data sf/tibble/dataframe planar_proj Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. ctrl control object created character SDM function. indices_knndm sdm function sub subset predictors elasticSDM test test data partition elasticSDM","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/createPCNM_fitModel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create global/regional PCNM surfaces and fit elastic net regression to all covariates — createPCNM_fitModel","text":"use cross validation determine suitable glmnet model alpha lambda, fit using glmnet. returns three objects spit environment, 1) pcnm, surfaces eigenvectors used glmnet model (including shrunk ), 2) glmnet model 3) fitting information carets process.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a quick SDM using elastic net regression — elasticSDM","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"function quickly creates SDM using elastic net regression, properly format data downstream use safeHavens workflow. Note elastic net models used couple important reasons: rescale input independent variables modelling, allowing us combine raw data beta coefficients use clustering algorithms downstream. also allowing 'shrinking' terms models shrinking terms models able get levels ecological inference prohibited older model selection frameworks.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"","code":"elasticSDM(x, predictors, planar_projection, domain, quantile_v = 0.025)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"x (simple feature) sf data set occurrence data species. predictors terra 'rasterstack' variables serve indepedent predictors. planar_projection Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. domain Numeric, many times larger make entire domain analysis simple bounding box around occurrence data x. quantile_v Numeric, variable used thinning input data, e.g. quantile = 0.05 remove records within lowest 5% distance iteratively, remaining records apart distance . want essentially thinning happen just supply 0.01. Defaults 0.025.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"list 12 objects, subsequently used downstream  SDM Post processing sequence, think best written disk. actual model prediction raster surface present first list 'RasterPredictions', indepedent variables used final model present 'Predictors, just global PCNM/MEM raster surfaces 'PCNM'. fit model 'Model', cross validation folds stored 'CVStructure', results single test/train partition 'ConfusionMatrix', two data split 'TrainData' 'TestData' finally 'PredictMatrix' used classifying test data confusion matrix.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"","code":"if (FALSE) { # \\dontrun{   x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv'))  x <- x[,c('lon', 'lat')]  x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)   files <- list.files(    path = file.path(system.file(package=\"dismo\"), 'ex'),     pattern = 'grd',  full.names=TRUE )  predictors <- terra::rast(files)  sdModel <- elasticSDM(    x = x, predictors = predictors, quantile_v = 0.025,    planar_projection =      '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs')        terra::plot(sdModel$RasterPredictions) } # }"},{"path":"https://sagesteppe.github.io/safeHavens/reference/get_elements.html","id":null,"dir":"Reference","previous_headings":"","what":"Recursively grab a named component of a list. — get_elements","title":"Recursively grab a named component of a list. — get_elements","text":"Recursively grab named component list.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/get_elements.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recursively grab a named component of a list. — get_elements","text":"","code":"get_elements(x, element)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/get_elements.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recursively grab a named component of a list. — get_elements","text":"x list lists element quoted name list element extract.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/greatCircleDistance.html","id":null,"dir":"Reference","previous_headings":"","what":"Haversine Distance Calculation — greatCircleDistance","title":"Haversine Distance Calculation — greatCircleDistance","text":"Calculate geographic distances geoid. results accurate distance calculations planar system. Function mostly used internally maximize_dispersion","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/greatCircleDistance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Haversine Distance Calculation — greatCircleDistance","text":"","code":"greatCircleDistance(lat1, lon1, lat2, lon2)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/greatCircleDistance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Haversine Distance Calculation — greatCircleDistance","text":"lat1 Double. column holding coords 'focal' population lon1 Double. column holding coords 'focal' population lat2 Double. column holding coords 'non-focal' population lon2 Double. column holding coords 'non-focal' population","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/greatCircleDistance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Haversine Distance Calculation — greatCircleDistance","text":"calculate distances sites (Haversine formula)","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/greatCircleDistance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Haversine Distance Calculation — greatCircleDistance","text":"","code":"n_sites <- 5 # number of known populations  df <- data.frame(    site_id = seq_len(n_sites),    lat = runif(n_sites, 25, 30),     lon = runif(n_sites, -125, -120)  )  dist_mat <- sapply(1:nrow(df), function(i) {    greatCircleDistance(      df$lat[i], df$lon[i],      df$lat, df$lon    )  }) #head(dist_mat)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/healPolygons.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean up unioned geometries - part 1 — healPolygons","title":"Clean up unioned geometries - part 1 — healPolygons","text":"function uses sf::st_snap remove small lines artifacts associated unioning polygons. ran within snapGrids","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/healPolygons.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean up unioned geometries - part 1 — healPolygons","text":"","code":"healPolygons(x)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/healPolygons.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean up unioned geometries - part 1 — healPolygons","text":"x output snapgrids","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/maximizeDispersion.html","id":null,"dir":"Reference","previous_headings":"","what":"Maximize Dispersion Site Selection — maximizeDispersion","title":"Maximize Dispersion Site Selection — maximizeDispersion","text":"function operates individual points, rather drawing convex hulls polygons around . designed rare species, individual populations relatively scarce, e.g. < 100, decent location data. perform bootstrap re-sampling better estimate true range extent species, well coordinate jittering better address geo-location quality. running n simulations identify individual networks sites (co-location) resilient perturbations, less affected data quality issues. particular point function, relative grid based approaches package, treats populations individuals, allows curators focus  'edges' species ranges. arguments takes known locations populations, solve n priority collection sites. Along process also generate priority ranking sites, indicating naive possible order prioritizing collections; although opportunity never discard site. required input parameter column indicating whether site required. Required sites (1 - many < n_sites) serve fixed parameters optimization scenario greatly speed run time. can represent: existing collections, collections strong chance happenging due funding agency mechanism, otherwise single population closet geographic center species. Notably solve 'around' site, hence solves purely theoretical, linked pragmatic element.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/maximizeDispersion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maximize Dispersion Site Selection — maximizeDispersion","text":"","code":"maximizeDispersion(   input_data,   lambda_var = 0.15,   n_sites = 5,   weight_1 = 1,   weight_2 = 0,   n_bootstrap = 999,   dropout_prob = 0.15,   objective = c(\"sum\", \"maxmin\"),   n_local_search_iter = 100,   n_restarts = 3,   seed = NULL,   verbose = TRUE )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/maximizeDispersion.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maximize Dispersion Site Selection — maximizeDispersion","text":"input_data list two elements: 'distances' (distance matrix array) 'sites' (data frame site metadata). lambda_var Essentially smoothing parameter controls trade-maximizing dispersion minimizing variance pairwise distances among selected sites. higher values prioritize variance reduction strongly. recommend checking stops 0.05 0.3 see works best data. Also check 0.5+, getting started, get feel strong variance reduction penalization can become. n_sites number sites want select priority collection. Note results return rank prioritization sites data. weight_1 Weights combining multiple distance matrices (provided). weight_1 geographic distance matrix weight_2 Weights combining multiple distance matrices (provided). weight_2 climatic distance matrix (provided). n_bootstrap Number bootstrap replicates perform. dropout_prob Probability dropping non-seed sites bootstrap replicate. objective Objective function optimize: \"sum\" (dispersion sum variance penalty) \"maxmin\" (maximize minimum distance). n_local_search_iter Number local search iterations per restart. n_restarts Number random restarts per bootstrap replicate. seed Random seed reproducibility. verbose Whether print progress information. print message run settings, progress bar bootstraps.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/maximizeDispersion.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Maximize Dispersion Site Selection — maximizeDispersion","text":"Select subset sites maximize spatial dispersion using bootstrapped local search algorithm, combining maximizing distance minimizing variance.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/maximizeDispersion.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Maximize Dispersion Site Selection — maximizeDispersion","text":"","code":"library(ggplot2)   n_sites <- 30 # number of known populations  df <- data.frame(    site_id = seq_len(n_sites),    lat = runif(n_sites, 25, 30), # play with these to see elongated results.     lon = runif(n_sites, -125, -120),    required = FALSE,    coord_uncertainty = 0  )  #function relies on at least one required point. here arbitrarily place near geographic center  dists2c <- greatCircleDistance(    median(df$lat),     median(df$lon),     df$lat,     df$lon  )  df[order(dists2c)[1],'required'] <- TRUE    ## we will simulate coordinate uncertainty on a number of sites.    uncertain_sites <- sample(setdiff(seq_len(n_sites), which(df$required)), size = min(6, n_sites-3))  df$coord_uncertainty[uncertain_sites] <- runif(length(uncertain_sites), 1000, 10000) # meters    # the function can take up to take matrices. the first (required) is a geographic distance  # matrix. calculate this with the `greatCircleDistance` fn from the package for consistency.   # (it will be recalculated during simulations). `sf` gives results in slightly diff units.   dist_mat <- sapply(1:nrow(df), function(i) {    greatCircleDistance(      df$lat[i], df$lon[i],      df$lat, df$lon    )  })   # the input data is a list, the distance matrix, and the df of actual point locations.   head(df) #>   site_id      lat       lon required coord_uncertainty #> 1       1 27.40703 -124.3706    FALSE             0.000 #> 2       2 27.02518 -121.7400    FALSE             0.000 #> 3       3 29.35227 -121.5366    FALSE             0.000 #> 4       4 29.95740 -122.7790    FALSE          7782.969 #> 5       5 29.24809 -124.1661    FALSE             0.000 #> 6       6 28.05629 -124.1151    FALSE             0.000   test_data <- list(distances = dist_mat, sites = df)  rm(dist_mat, df, n_sites, uncertain_sites, dists2c)   # small quick run (fast); timing for R package / CRAN status.     system.time(       res <- maximizeDispersion(  ## reduce some parameters for faster run.         input_data = test_data,        lambda_var = 0.2,        n_bootstrap = 500,        objective = \"maxmin\",        n_local_search_iter = 50,        n_restarts = 2      )    ) #> Sites: 30 | Seeds: 1 | Requested: 5 | Coord. Uncertain: 6 | BS Replicates: 500 #>    |                                                                               |                                                                      |   0%   |                                                                               |                                                                      |   1%   |                                                                               |=                                                                     |   1%   |                                                                               |=                                                                     |   2%   |                                                                               |==                                                                    |   2%   |                                                                               |==                                                                    |   3%   |                                                                               |===                                                                   |   4%   |                                                                               |===                                                                   |   5%   |                                                                               |====                                                                  |   5%   |                                                                               |====                                                                  |   6%   |                                                                               |=====                                                                 |   7%   |                                                                               |=====                                                                 |   8%   |                                                                               |======                                                                |   8%   |                                                                               |======                                                                |   9%   |                                                                               |=======                                                               |   9%   |                                                                               |=======                                                               |  10%   |                                                                               |=======                                                               |  11%   |                                                                               |========                                                              |  11%   |                                                                               |========                                                              |  12%   |                                                                               |=========                                                             |  12%   |                                                                               |=========                                                             |  13%   |                                                                               |==========                                                            |  14%   |                                                                               |==========                                                            |  15%   |                                                                               |===========                                                           |  15%   |                                                                               |===========                                                           |  16%   |                                                                               |============                                                          |  17%   |                                                                               |============                                                          |  18%   |                                                                               |=============                                                         |  18%   |                                                                               |=============                                                         |  19%   |                                                                               |==============                                                        |  19%   |                                                                               |==============                                                        |  20%   |                                                                               |==============                                                        |  21%   |                                                                               |===============                                                       |  21%   |                                                                               |===============                                                       |  22%   |                                                                               |================                                                      |  22%   |                                                                               |================                                                      |  23%   |                                                                               |=================                                                     |  24%   |                                                                               |=================                                                     |  25%   |                                                                               |==================                                                    |  25%   |                                                                               |==================                                                    |  26%   |                                                                               |===================                                                   |  27%   |                                                                               |===================                                                   |  28%   |                                                                               |====================                                                  |  28%   |                                                                               |====================                                                  |  29%   |                                                                               |=====================                                                 |  29%   |                                                                               |=====================                                                 |  30%   |                                                                               |=====================                                                 |  31%   |                                                                               |======================                                                |  31%   |                                                                               |======================                                                |  32%   |                                                                               |=======================                                               |  32%   |                                                                               |=======================                                               |  33%   |                                                                               |========================                                              |  34%   |                                                                               |========================                                              |  35%   |                                                                               |=========================                                             |  35%   |                                                                               |=========================                                             |  36%   |                                                                               |==========================                                            |  37%   |                                                                               |==========================                                            |  38%   |                                                                               |===========================                                           |  38%   |                                                                               |===========================                                           |  39%   |                                                                               |============================                                          |  39%   |                                                                               |============================                                          |  40%   |                                                                               |============================                                          |  41%   |                                                                               |=============================                                         |  41%   |                                                                               |=============================                                         |  42%   |                                                                               |==============================                                        |  42%   |                                                                               |==============================                                        |  43%   |                                                                               |===============================                                       |  44%   |                                                                               |===============================                                       |  45%   |                                                                               |================================                                      |  45%   |                                                                               |================================                                      |  46%   |                                                                               |=================================                                     |  47%   |                                                                               |=================================                                     |  48%   |                                                                               |==================================                                    |  48%   |                                                                               |==================================                                    |  49%   |                                                                               |===================================                                   |  49%   |                                                                               |===================================                                   |  50%   |                                                                               |===================================                                   |  51%   |                                                                               |====================================                                  |  51%   |                                                                               |====================================                                  |  52%   |                                                                               |=====================================                                 |  52%   |                                                                               |=====================================                                 |  53%   |                                                                               |======================================                                |  54%   |                                                                               |======================================                                |  55%   |                                                                               |=======================================                               |  55%   |                                                                               |=======================================                               |  56%   |                                                                               |========================================                              |  57%   |                                                                               |========================================                              |  58%   |                                                                               |=========================================                             |  58%   |                                                                               |=========================================                             |  59%   |                                                                               |==========================================                            |  59%   |                                                                               |==========================================                            |  60%   |                                                                               |==========================================                            |  61%   |                                                                               |===========================================                           |  61%   |                                                                               |===========================================                           |  62%   |                                                                               |============================================                          |  62%   |                                                                               |============================================                          |  63%   |                                                                               |=============================================                         |  64%   |                                                                               |=============================================                         |  65%   |                                                                               |==============================================                        |  65%   |                                                                               |==============================================                        |  66%   |                                                                               |===============================================                       |  67%   |                                                                               |===============================================                       |  68%   |                                                                               |================================================                      |  68%   |                                                                               |================================================                      |  69%   |                                                                               |=================================================                     |  69%   |                                                                               |=================================================                     |  70%   |                                                                               |=================================================                     |  71%   |                                                                               |==================================================                    |  71%   |                                                                               |==================================================                    |  72%   |                                                                               |===================================================                   |  72%   |                                                                               |===================================================                   |  73%   |                                                                               |====================================================                  |  74%   |                                                                               |====================================================                  |  75%   |                                                                               |=====================================================                 |  75%   |                                                                               |=====================================================                 |  76%   |                                                                               |======================================================                |  77%   |                                                                               |======================================================                |  78%   |                                                                               |=======================================================               |  78%   |                                                                               |=======================================================               |  79%   |                                                                               |========================================================              |  79%   |                                                                               |========================================================              |  80%   |                                                                               |========================================================              |  81%   |                                                                               |=========================================================             |  81%   |                                                                               |=========================================================             |  82%   |                                                                               |==========================================================            |  82%   |                                                                               |==========================================================            |  83%   |                                                                               |===========================================================           |  84%   |                                                                               |===========================================================           |  85%   |                                                                               |============================================================          |  85%   |                                                                               |============================================================          |  86%   |                                                                               |=============================================================         |  87%   |                                                                               |=============================================================         |  88%   |                                                                               |==============================================================        |  88%   |                                                                               |==============================================================        |  89%   |                                                                               |===============================================================       |  89%   |                                                                               |===============================================================       |  90%   |                                                                               |===============================================================       |  91%   |                                                                               |================================================================      |  91%   |                                                                               |================================================================      |  92%   |                                                                               |=================================================================     |  92%   |                                                                               |=================================================================     |  93%   |                                                                               |==================================================================    |  94%   |                                                                               |==================================================================    |  95%   |                                                                               |===================================================================   |  95%   |                                                                               |===================================================================   |  96%   |                                                                               |====================================================================  |  97%   |                                                                               |====================================================================  |  98%   |                                                                               |===================================================================== |  98%   |                                                                               |===================================================================== |  99%   |                                                                               |======================================================================|  99%   |                                                                               |======================================================================| 100% #>    user  system elapsed  #>   2.373   0.003   2.378   ### first selected   ggplot(data = res$input_data,     aes(      x = lon,       y = lat,       shape = required,       size = cooccur_strength,      color = selected      )    ) +    geom_point() +   #  ggrepel::geom_label_repel(aes(label = site_id), size = 4) +     theme_minimal() +     labs(main = 'Priority Selection Status of Sites')  #> Ignoring unknown labels: #> • main : \"Priority Selection Status of Sites\"   ### order of sampling priority ranking plot.  ggplot(data = res$input_data,     aes(      x = lon,       y = lat,       shape = required,       size = -sample_rank,      color = sample_rank      )    ) +    geom_point() +  #   ggrepel::geom_label_repel(aes(label = sample_rank), size = 4) +    theme_minimal()"},{"path":"https://sagesteppe.github.io/safeHavens/reference/reduceFinalGrids.html","id":null,"dir":"Reference","previous_headings":"","what":"More sliver fixing — reduceFinalGrids","title":"More sliver fixing — reduceFinalGrids","text":"instances tiny little grids tagged along processing. just give arbitrary nearest feature. annoying chance (... millionth area...) arbitrary random point missed, areas tend remarkable inconsequential, worth randomly reassigning neighbor","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/reduceFinalGrids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"More sliver fixing — reduceFinalGrids","text":"","code":"reduceFinalGrids(final_grids)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/reduceFinalGrids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"More sliver fixing — reduceFinalGrids","text":"final_grids truthfully nearly final point.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. dplyr any_of magrittr %>%","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/snapGrids.html","id":null,"dir":"Reference","previous_headings":"","what":"turn the point grid suggestions made by assignGrid_pts into polygons — snapGrids","title":"turn the point grid suggestions made by assignGrid_pts into polygons — snapGrids","text":"function part grid based sampling process turn small grid cells, broken , larger existing grid cells.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/snapGrids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"turn the point grid suggestions made by assignGrid_pts into polygons — snapGrids","text":"","code":"snapGrids(x, neighb_grid, focal_grid)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/snapGrids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"turn the point grid suggestions made by assignGrid_pts into polygons — snapGrids","text":"x output assignGrid_pts neighb_grid neighboring grid options. focal_grid grid reassign area .","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/trainKNN.html","id":null,"dir":"Reference","previous_headings":"","what":"partition data and train a simple KNN model — trainKNN","title":"partition data and train a simple KNN model — trainKNN","text":"Simply use partition test data quickly train simple model","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/trainKNN.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"partition data and train a simple KNN model — trainKNN","text":"","code":"trainKNN(x, split_prop)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/trainKNN.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"partition data and train a simple KNN model — trainKNN","text":"x weighted matrix including class ID column 'ID' split_prop prop data partitions.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":null,"dir":"Reference","previous_headings":"","what":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"function used write wide range values fitPredictOperationalize process. create multiple subdirectories within user specified path. include: 'Rasters' raster stack four final rasters go, 'Fitting' details model fitting caret placed, 'Models' final fit model go, 'Evaluation' evaluation statistics placed, 'Threshold' results form dismo::threshold placed.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"","code":"writeSDMresults(   path,   taxon,   cv_model,   pcnm,   model,   cm,   coef_tab,   f_rasts,   thresh )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"path root path 5 folders created, exist. taxon name taxonomic entity models created. cv_model cross validation data elasticSDM pcnm pcnm/mem rasters elasticSDM model final glmnet model elasticSDM cm confusion matrix elasticSDM coef_tab coefficient table RescaleRasters f_rasts final rasters RescaleRasters thresh threshold statistics PostProcessSDM","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"objects, objects specified, written disk.","code":""}]
