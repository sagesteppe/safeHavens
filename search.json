[{"path":"https://sagesteppe.github.io/safeHavens/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Reed Benkendorf Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/safeHavens.html","id":"notes-about-safehavens","dir":"Articles","previous_headings":"","what":"Notes about safeHavens","title":"Showcasing Sampling Schemes with Sloths","text":"package helps germplasm curators communicate areas interest collection teams targeting new accessions. provides seven different sampling approaches curators choose , individual taxon hope process. also allows easy integration existing workflows put required spatial data share collection teams. approaches based standard practices ecology, reflect basic tenets population genetics, Tobler’s first law geography. methods various trade offs terms computational environmental complexity. table presents currently implemented sampling scheme user facing function associated . mind first four functions really flavors process, one whereby try partition species range geographic chunks similar sizes. However, often case four things seem similar may enormously different results implementation. fifth method IBDBasedSample largely class ’s , lieu using continuity geographic space ’s primary method, focuses discontinuity space using distance matrices clustering determine patches range close patches. impetus behind method course Sewall Wrights Isolation Distance (1943). EcoregionBasedSample may commonly encountered method North America various formats driving two major germplasm banking projects Midwest Southeastern United States, well high level, composing way numerous native seed collection coordinators structured West. method using environmental variation implicit guide targeting populations seed collections, .e. different ecoregions serve stratification agent. broad strokes, general thinking regions represent continuous transitions environment faced species, populations across ranges differently adapted environments. Given ’s relative popularity implementation, function arguments ’s counterparts, discussed . final function EnvironmentalBasedSample computationally expensive, environmentally explicit. function rely Species Distribution Model, generated via generalized linear model, supported package, cluster populations based environmental variables related observed distributions spatial configuration distance . paper, draws together aspects functions, however testing approach implemented. discussed depth. Note table ‘Comp.’ ‘Envi.’ refer computational environmental complexity respectively, range low (L) medium high.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/safeHavens.html","id":"general-notes-about-this-vignette","dir":"Articles","previous_headings":"","what":"General notes about this vignette","title":"Showcasing Sampling Schemes with Sloths","text":"package strongly focused plants, disrespect animals, just never occurred development. However, make animal people, use Sloth, think Bradypus variegatus looks maniacal mischievous, like inspiration many Jim Henson puppets. However, treat Bradypus like plant species discuss sampling schemes - assume focused organisms seldom move great distances dispersal processes. can access data Bradypus dismo package - presumably short ‘Distribution Modelling’, created Robert Hijman others. use couple dismo functions package, utilize spatial data , ’s good package familiar . want read dismo distribution modelling general awesome bookdown resource written Hijman Elith - aspects vignette definitely based . use Species Distribution Models (SDMs) input variable many arguments function - don’t get caught details making . days can get good SDM results essentially entirely automated pipelines, ’ll truncated process outlined chunk, honestly argue ignore section first times read document, go back details (discussed link ) decide pursue route EnvironmentalBasedSample. may optimistic thinking one schemes give adequate results sample design, may better certain species projects. Future users need obtain occurrence records using package. r package rbgif fantastic way , well documented article linking several vignettes package .","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/safeHavens.html","id":"species-distribution-modelling-skip-me-your-first-few-read-throughs","dir":"Articles","previous_headings":"","what":"Species Distribution Modelling (skip me your first few read throughs?!)","title":"Showcasing Sampling Schemes with Sloths","text":"interested visualizing sampling scheme safeHavens complex function EnvironmentalBasedSample requires, can explore section understand costs required achieve process. goes , recommend checking section last. meantime, still want see results EnvironmentalBasedSample compared functions, can load results disk, distributed package. goal SDM’s create model accurately predicts species located environmental space, hence geographic space. goal models understand degree, direction, various environmental features correlate species observed range. Accordingly, optimized use making typical decisions associated species distribution models, .e. conservation area planning. Rather models supported package solely focused estimating effects various environmental parameters species distribution. use caret package help glmnet modelling, ’s unnecessary, provides output easy explore interact . much caret functionality improved tidymodels associated packages Max Kuhn (also lead caret author), caret pretty stable serve purposes fine. makes elastic net model interesting able bridge worlds lasso ridge regression blending alpha parameter. Lasso alpha 0, Ridge alpha 1. Lasso regression perform automated variable selection, can actually drop (‘shrink’) model, Ridge regression keep variables correlated features present give contributions . elastic net blends propensity drop retain variables whenever used. caret test range alphas accomplish . Note using don’t exactly get feel variable really correlated event, least get sense set variables affects range. Infer risk! Inference SDM’s something recommend except special circumstances anyways. can see elastic net decided top model. used Accuracy rather Kappa main criterion model selection. can see selected model works predicting state test data. Note accuracy results slightly higher CV folds. bug, CV folds testing holdouts, brand new set holdouts. main reason confusion matrix results likely higher due spatial auto-correlation address typical ‘random split’ test train data. order minimize effects spatial-autocorrelation model use CAST hood allows spatially informed cross validation. Consider output CVStructure bit realistic. SDM’s produce surfaces display probability suitable habitat across landscape. Many people want binary prediction surface , .e. less probable suitable habitat taxon exists particular location (grid cell)?. Going continuous probability surface binary surface loses lot information, many use cases essential reduce computational complexity. interested caveats associated Frank Harrell written extensively topic. use binary surfaces implementing sampling procedures across species range. function PostProcessSDM used purpose. Historically, assessing probability output 0.5 probability used threshold, probabilities beneath considered ‘suitable’, probabilities classified ‘Suitable’. works well many use cases, argue thresholding outside domain statistics realm practice. motto implementing models, slight elaboration George Box’s aphorism, “models wrong, useful - want wrong?”. goal sampling germplasm conservation maximize representation allelic diversity across range species. order , need good understanding species actual range , hence happy predict species present , predict absent actually grows. Hence preferred thresholding statistic Sensitivity, metric weighs false predicted presences. argument free vary supports threshold values calculated dismo::threshold, explore better understand ’s options. now, may wondering function achieves named PostProcessSDM rather ThresholdSDM, reason perceived discontinuity function another process ’s second portion. Using initial occurrence data, sets went training test data developing statistical model, create ‘buffers’ around points ensure none known occurrence points ‘missing’ output binary map. two edged sword, address notion dispersal limitation, realize suitable habitat occupied habitat. PostProcessSDM function creates cross validation folds, selects training data. fold calculates distance occurrence point ’s nearest neighbor. summarize distances can understand distribution distances quantile. use selected quantile, serve buffer. Area predicted suitable habitat outside buffer become ‘cut ’ (masked) binary raster map, areas within buffer distance known occurrences currently masked reassigned probabilities. theory behind process underdeveloped nascent, come gut analyst. Bradypus data set use 0.25 quantile, saying “Neighbors generally 100km apart, happy risk saying 25km within occurrence occupied suitable habitat”. Increasing value say 1.0 mean suitable habitat removed, decreasing makes maps conservative. cost increasing distances greatly sampling methods may puts grids many areas without populations collect . can compare results applying function side side using output function. glmnet used three main reasons, 1) gives directional coefficients feel 1 unit increase independent variable predicts response, mind big improvement ‘Variable Importance Factors’ just know certain variables contributed model others. 2) maintains degree automated selection reducing work analyst needs , .e. can process many species without spending much time single one. 3) glmnet actually re-scales variables model generation, suppose can implemented models, use re-scaling glmnet transform independent variables raster stack multiply beta-coefficients. way raster stack becomes representative model, can use values basis hierarchical cluster later . can see variables ‘close’ scale, work clustering algorithm. layers color (maybe yellow?) means variance, ’s term shrunk model. dealt internally future function. scales variables exact, weighed coefficients model can also look coefficients variable. glmnet returns ‘untransformed’ variables, .e. coefficients scale input rasters, calculate BC right afterwards. safeHavens generates kinds things runs functions elasticSDM, PostProcessSDM, RescaleRasters. Given one sampling scheme may followed quite time, think best practice save many objects. Yes, take storage space, storage virtually free days anyways. write following items? test write directory exists project associated creation R package, just save somewhere delete . test files tiny anyways. , steps make species distribution model - rather get coefficients species distribution model! play around example data set compare buffered distance results end.","code":"x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)  files <- list.files(   path = file.path(system.file(package=\"dismo\"), 'ex'),    pattern = 'grd',  full.names=TRUE ) predictors <- terra::rast(files) # import the independent variables sdModel <- elasticSDM(   x = x, predictors = predictors, quantile_v = 0.025,   planar_proj = planar_proj) sdModel$CVStructure sdModel$ConfusionMatrix terra::plot(sdModel$RasterPredictions) threshold_rasts <- PostProcessSDM(   rast_cont = sdModel$RasterPredictions,    test = sdModel$TestData,   planar_proj = planar_proj,   thresh_metric = 'sensitivity', quant_amt = 0.5) terra::plot(threshold_rasts$FinalRasters) # CREATE A COPY OF THE RASTER PREDICTORS WHERE WE HAVE  # STANDARDIZED EACH VARIABLE - SO IT IS EQUIVALENT TO THE INPUT TO THE GLMNET # FUNCTION, AND THEN MULTIPLIED IT BY IT'S BETA COEFFICIENT FROM THE FIT MODEL # we will also write out the beta coefficients using writeSDMresults right after # this.   rr <- RescaleRasters(   model = sdModel$Model,   predictors = sdModel$Predictors,    training_data = sdModel$TrainData,    pred_mat = sdModel$PredictMatrix)  terra::plot(rr$RescaledPredictors) print(rr$BetaCoefficients) bp <- '~/Documents/assoRted/StrategizingGermplasmCollections'  writeSDMresults(   cv_model = sdModel$CVStructure,    pcnm = sdModel$PCNM,    model = sdModel$Model,    cm = sdModel$ConfusionMatrix,    coef_tab = rr$BetaCoefficients,    f_rasts = threshold_rasts$FinalRasters,   thresh = threshold_rasts$Threshold,   file.path(bp, 'results', 'SDM'), 'Bradypus_test')  # we can see that the files were placed here using this.  list.files( file.path(bp, 'results', 'SDM'), recursive = TRUE )"},{"path":"https://sagesteppe.github.io/safeHavens/articles/safeHavens.html","id":"alternative-to-sdm-just-buffer-start-reading-here-your-first-few-times","dir":"Articles","previous_headings":"","what":"Alternative to SDM, just buffer! (start reading here your first few times!)","title":"Showcasing Sampling Schemes with Sloths","text":"Creating SDMs whole process, users may want go road. provide approach give us results can work , computationally cheap. relies one input, occurrence data , simple process - drawing circle specific radius around observation point! showing second two major packages package depends , sf. sf relies simple feature geometries now workhorse vector data analysis R. well documented tested, unfamiliar consider dplyr spatial data ecosystem R. vignette use :: notation can get idea functions coming , ’ll notice lot sf::st_*.  Voila! ’s whole process. Just play around distances get something looks OK. define ‘OK’?.. feeling gut boots tingling. Note buffered polygons may look little nebulous now, ’ll maps context just couple steps.","code":"planar_proj =     '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs'  x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326) x_buff <- sf::st_transform(x, planar_proj) |>   sf::st_buffer(125000) |> # we are working in planar metric coordinates, we are   sf::st_as_sfc() |> # buffer by this many / 1000 kilometers.    sf::st_union()  plot(x_buff)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/safeHavens.html","id":"just-load-the-sdm","dir":"Articles","previous_headings":"","what":"Just load the SDM!","title":"Showcasing Sampling Schemes with Sloths","text":"package also SDM prediction saved data can just load couple comparisons.","code":"sdm <- terra::rast(file.path(system.file(package=\"safeHavens\"),  'extdata', 'Bradypus_test.tif')) terra::plot(sdm)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/safeHavens.html","id":"now-prep-some-data-for-visualizing-the-results","dir":"Articles","previous_headings":"","what":"Now prep some data for visualizing the results","title":"Showcasing Sampling Schemes with Sloths","text":"necessary, going add context maps help interpret results various functions package. use spData package uses naturalearth data ’s world data suitable creating effective maps variety resolutions.","code":"x_extra_buff <- sf::st_buffer(x_buff, 100000) |> # add a buffer to 'frame' the maps   sf::st_transform(4326)  americas <- spData::world americas <- sf::st_crop(americas, sf::st_bbox(x_extra_buff)) |>   dplyr::select(name_long) #> Warning: attribute variables are assumed to be spatially constant throughout #> all geometries  bb <- sf::st_bbox(x_extra_buff)  map <- ggplot() +    geom_sf(data = americas) +    theme(     legend.position = 'none',      panel.background = element_rect(fill = \"aliceblue\"),      panel.grid.minor.x = element_line(colour = \"red\", linetype = 3, linewidth  = 0.5),      axis.ticks=element_blank(),     axis.text=element_blank(),     plot.background=element_rect(colour=\"steelblue\"),     plot.margin=grid::unit(c(0,0,0,0),\"cm\"),     axis.ticks.length = unit(0, \"pt\"))+    coord_sf(xlim = c(bb[1], bb[3]), ylim = c(bb[2], bb[4]), expand = FALSE)  rm(x_extra_buff, americas)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/safeHavens.html","id":"running-the-various-sample-design-algorithms","dir":"Articles","previous_headings":"","what":"Running the Various Sample Design Algorithms","title":"Showcasing Sampling Schemes with Sloths","text":"Now data can represent species ranges, can run various sampling approaches. table introduction reproduced . Note table ‘Comp.’ ‘Envi.’ refer computational environmental complexity respectively, range low (L) medium high.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/articles/safeHavens.html","id":"grid-based-and-point-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Grid Based and Point Based Sample","title":"Showcasing Sampling Schemes with Sloths","text":"Ecologists love grids. us taught love grids sampling. Grids useful sampling contiguous things. Species ranges often contiguous; curators analysts geographically grand ecosystems, e.g. steppes, prairies, tundra, taiga might find useful. can look output see grids great type problem. first step grid sampling determining OK number grids try draw starting point, want 20 collections need 20 grids, several merged larger ones. Using aspect ratio simple bounding box around area analyzing, function determine default number grids (‘Original’) testing. Using defaults create sets grids well, either removing one two grids per direction. Theoretically automate grid selection comparing number grids minimization variance. safe wouldn’t consider configurations generate less 25 initial grids.  Essentially need 20 grids, realistically (albeit limited informal testing) using 25 grids - depending complexity species range - tends effective floor. table plot opt using ‘Smaller’ option, 28 grids generated prompting sf::st_make_grid 7 grids X direction 5 Y direction. can kind think like elbow plot, samples won’t get characteristic shape.  grids drew pre-specified number grids across species range, merged together required get results. essentially inverse step, rather drawing boundaries - .e. grid cells, draw centers. essentially allows features ‘grow’ little naturally. also think results work little bit better fragmented range, still odd clipping, minor portions section range assigned different grid, general little bit better.","code":"tgs <- TestGridSizes(x_buff) print(tgs) #>       Name Grids  Variance GridNOx GridNOy #> 1 Smallest    34  599.9513       8       6 #> 2  Smaller    28  813.2043       7       5 #> 3 Original    24  862.5458       6       4 #> 4   Larger    18  681.7667       5       3 #> 5  Largest    13 1182.8457       4       2  plot(tgs$Grids, tgs$Variance, xlab = 'Grid Number', ylab = 'Variance',      main = 'Number of grids and areas overlapping species range') text(tgs$Grids, tgs$Variance + 25, labels=tgs$Name, cex= 0.7) abline(v=20, col=\"red\") abline(v=25, col=\"orange\") tgs <- tgs[tgs$Name=='Smaller',] grid_buff <- GridBasedSample(x_buff, planar_proj, gridDimensions = tgs)   gbs.p <- map +    geom_sf(data = grid_buff, aes(fill = factor(ID))) +   # geom_sf_label(data = grid_buff, aes(label = Assigned), alpha = 0.4) +  # on your computer, doesnt work at vignette size   labs(title = 'Grids')  +    coord_sf(expand = F)  gbs.p pbs <- PointBasedSample(x_buff) pbs.sf <- pbs$Geometry  pbs.p <- map +    geom_sf(data = pbs.sf, aes(fill = factor(ID))) +  #  geom_sf_label(data = pbs.sf, aes(label = ID), alpha = 0.4) +    labs(title = 'Point') +    coord_sf(expand = F) pbs.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/safeHavens.html","id":"equal-area-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Equal Area Sample","title":"Showcasing Sampling Schemes with Sloths","text":"Perhaps simplest method offered safeHavens EqualAreaSample. simply creates many points, pts defaults 5000, within target polygon subjects k-means sampling groups specified n target number collections. individual points assigned groups polygons ‘take’ map space developed, intersected back species range, area polygon measured. process ran times, defaulting 100 reps, set polygons created reps smallest variance polygon size selected returned function. differs point based sampling instance, start regularly spaced points grow , take step back using many points let clusters grow similar sizes. results look quite similar point based sample.","code":"eas <- EqualAreaSample(x_buff, planar_projection = planar_proj)  #> Warning: did not converge in 10 iterations  eas.p <- map +    geom_sf(data = eas$Geometry, aes(fill = factor(ID))) +  #  geom_sf_label(data = eas.sf, aes(label = ID), alpha = 0.4) +    labs(title = 'Equal Area') +    coord_sf(expand = F) eas.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/safeHavens.html","id":"opportunistic-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Opportunistic Sample","title":"Showcasing Sampling Schemes with Sloths","text":"blunt, many new players germplasm conservation table (thrilled !), many existing collections largely grown opportunity (still happy ). Many Curators may interested much can embed existing collections sampling framework. function OpportunisticSample makes minor modifications point based sample try maximize existing collection. doesn’t always work exceptionally, especially couple collections close , may beneficial tool belt. observed, three previous sampling schemes end somewhat similar results - took used PointBasedSample framework embedded function - also easiest function work ! Essentially combines approach point based sampling, forces clusters based around existing accessions. attempts ‘center’ existing collections within clusters, can nearly impossible variety reasons. see , grids aligned around points. can lead funky clusters, bird hand worth two bush.","code":"exist_pts <- sf::st_sample(x_buff, size = 10) |>     sf::st_as_sf() |> # ^^ just randomly sampling 10 points in the species range    dplyr::rename(geometry = x)  os <- OpportunisticSample(polygon = x_buff, n = 20, collections = exist_pts)  os.p <- map +    geom_sf(data = os$Geometry, aes(fill = factor(ID))) +  #  geom_sf_label(data = os.sf, aes(label = ID), alpha = 0.4) +    geom_sf(data = exist_pts, alpha = 0.4) +    labs(title = 'Opportunistic') +    coord_sf(expand = F)  os.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/safeHavens.html","id":"isolation-by-distance-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Isolation by Distance Based Sample","title":"Showcasing Sampling Schemes with Sloths","text":"Isolation Distance, much lesser extent Toblers first law geography, fundamental ideas driving package. sampling schemes implicitly based around former idea, latter essential germplasm conservation. function explicitly uses IBD develop sampling scheme, obfuscate parameters.  data processed raster, sharp edges, representing raster tiles. However, immediately evident borders clusters natural looking previous (future) sampling schemes.","code":"files <- list.files( # note that for this process we need a raster rather than    path = file.path(system.file(package=\"dismo\"), 'ex'), # vector data to accomplish   pattern = 'grd',  full.names=TRUE ) # this we will 'rasterize' the vector using terra predictors <- terra::rast(files) # this can also be done using 'fasterize'. Whenever # we rasterize a product, we will need to provide a template raster that our vector # will inherit the cell size, coordinate system, etc. from   x_buff.sf <- sf::st_as_sf(x_buff) |>    dplyr::mutate(Range = 1) |>    sf::st_transform( terra::crs(predictors))  # and here we specify the field/column with our variable we want to become an attribute of our raster v <- terra::rasterize(x_buff.sf, predictors, field = 'Range')   # now we run the function demanding 20 areas to make accessions from,  ibdbs <- IBDBasedSample(x = v, n = 20, fixedClusters = TRUE, template = predictors)  ibdbs.p <- map +    geom_sf(data = ibdbs, aes(fill = factor(ID))) +  #  geom_sf_label(data = os.sf, aes(label = ID), alpha = 0.4) +    labs(title = 'IBD') +    coord_sf(expand = F) ibdbs.p rm(predictors, files, v, x_buff.sf, exist_pts, os)"},{"path":"https://sagesteppe.github.io/safeHavens/articles/safeHavens.html","id":"ecoregion-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Ecoregion Based Sample","title":"Showcasing Sampling Schemes with Sloths","text":"mentioned far commonly implemented method guiding native seed collection. However, sure exactly practitioners implement , whether formats application consistent among practitioners! reasons different sets options supported user. general usage, two parameters always required x species range sf object, ecoregions, sf object containing ecoregions interest. ecoregions file need subset range x quite yet - function take care . Additional arguments function include usual n specify many accession looking collection. Two additional arguments relate whether using Omernik Level 4 ecoregions data ecoregions (biogeographic regions) another source. OmernikEPA, ecoregion_col, using official EPA release ecoregions optional, however using EPA product supplied - ecoregion_col argument totally necessary. column contain unique names highest resolution level ecoregion want use data set, many data sets, example call ‘neo_eco’ may field ecolevel information!  output differs others see, depicted number collections made per ecoregion. number ecoregions greater requested sample size, return object can take two values - collections, one collection.","code":"neo_eco <- sf::st_read(   file.path(system.file(package=\"safeHavens\"), 'extdata', 'NeoTropicsEcoregions.gpkg'),    quiet = TRUE) |>   dplyr::rename(geometry = geom) head(neo_eco[,c(1, 3, 4, 6, 11)]) #> Simple feature collection with 6 features and 4 fields #> Geometry type: MULTIPOLYGON #> Dimension:     XY #> Bounding box:  xmin: -103.0432 ymin: -31.25308 xmax: -34.79344 ymax: 26.91751 #> Geodetic CRS:  WGS 84 #>                  Provincias      Region      Dominio #> 1 Araucaria Forest province Neotropical       Parana #> 2          Atacama province Neotropical         <NA> #> 3         Atlantic province Neotropical       Parana #> 4           Bahama province Neotropical         <NA> #> 5     Balsas Basin province Neotropical Mesoamerican #> 6         Caatinga province Neotropical      Chacoan #>                        Subregion                       geometry #> 1                        Chacoan MULTIPOLYGON (((-53.58012 -... #> 2 South American Transition Zone MULTIPOLYGON (((-69.42981 -... #> 3                        Chacoan MULTIPOLYGON (((-48.41217 -... #> 4                      Antillean MULTIPOLYGON (((-77.58593 2... #> 5                      Brazilian MULTIPOLYGON (((-97.37265 1... #> 6                        Chacoan MULTIPOLYGON (((-35.56652 -...  x_buff <- sf::st_transform(x_buff, sf::st_crs(neo_eco)) ebs.sf <- EcoregionBasedSample(x_buff, neo_eco, OmernikEPA = FALSE, ecoregion_col = 'Provincias') #> Warning: attribute variables are assumed to be spatially constant throughout #> all geometries #> Warning: attribute variables are assumed to be spatially constant throughout #> all geometries  # for plotting let's crop it to the other objects ebs.sf <- st_crop(ebs.sf, bb) #> Warning: attribute variables are assumed to be spatially constant throughout #> all geometries  ebs.p <- map +    geom_sf(data = ebs.sf, aes(fill = factor(n))) +    labs(title = 'Ecoregion') +    coord_sf(expand = F) ebs.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/safeHavens.html","id":"environmental-based-sample","dir":"Articles","previous_headings":"Running the Various Sample Design Algorithms","what":"Environmental Based Sample","title":"Showcasing Sampling Schemes with Sloths","text":"environmental based sample can conducted species distribution model data. Included data directory folder objects required run example species. load . data loaded R, scale rasters (using RescaleRasters) serve surfaces predict (also done !), run algorithm (EnvironmentalBasedSample). However, run algorithm need create directory (also called ‘folder’), computers save results function EnvironmentalBasedSample. Whereas earlier vignette showcased functions generated species distribution model, us saving results two stage process (e.g. create SDM associated products used: elasticSDM, PostProcessSDM, RescaleRasters, finally saving relevant data writeSDMresults), function produces product writes ancillary data simultaneously. approach chosen function writing four objects: 1) groups vector data, 2) groups raster data, 3) k-nearest neighbors (knn) model used generate clusters, 4) confusion matrix associated testing knn model.  function EnvironmentalBasedSample can take three binary rasters created PostProcessSDM arguments template. showcase different results using .  plots able showcase difference results depending three input rasters utilized. sampling schemes, results vary widely based spatial extents functions applied . Using SDM output undergone thresholding results largest classified area. first glance results may seem different, look central america, largely consistent, near Andes; large differences exist Amazon Basin, even alignment systems evident. Accordingly, surface used species match evaluation criterion. Using threshold raster surface good option want ‘miss’ many areas, whereas clipped supplemented options may better suited scenarios want draw clusters, lack populations can collected .","code":"sdModel <- readRDS(   file.path(system.file(package=\"safeHavens\"), 'extdata',  'sdModel.rds')   )  sdModel$Predictors <- terra::rast(   file.path(system.file(package=\"safeHavens\"), 'extdata', 'Predictors.tif') ) rr <- RescaleRasters( # you may have already done this!   model = sdModel$Model,   predictors = sdModel$Predictors,    training_data = sdModel$TrainData,    pred_mat = sdModel$PredictMatrix)  # create a directory to hold the results from EBS real quick.  # we will default to placing it in your current working directory.  # If you are a data management freak don't worry too much about this.  # The code to remove the directory will be provided below.  getwd() # this is where the folder is going to located IF YOU DON'T RUN the code below.  #> [1] \"/home/runner/work/safeHavens/safeHavens/vignettes\" p <- file.path('~', 'Documents') # in my case I'll dump it in Documents real quick, this should work on  # Linux and Mac, but I don't think Windows?  # dir.create(file.path(p, 'safeHavens-Vignette')) # now create the directory.   ENVIbs <- EnvironmentalBasedSample(   pred_rescale = rr$RescaledPredictors,    write2disk = FALSE,    path = file.path(p, 'safeHavens-Vignette'), # we are not writing, but showing how to provide argument   taxon = 'Bradypus_test',    f_rasts = sdm, n = 20,    lyr = 'Supplemented',   fixedClusters = TRUE,    n_pts = 500,    planar_projection = planar_proj,   buffer_d = 3, prop_split = 0.8) #> Joining with `by = join_by(x, y)` #> Warning in st_point_on_surface.sfc(st_geometry(x)): st_point_on_surface may not #> give correct results for longitude/latitude data  ENVIbs.p <- map +    geom_sf(data = ENVIbs, aes(fill = factor(ID))) +    #geom_sf_label(data = ENVIbs, aes(label = ID), alpha = 0.4) +    labs(title = 'Environmental') +    coord_sf(expand = FALSE) #> Coordinate system already present. Adding new coordinate system, which will #> replace the existing one.  ENVIbs.p"},{"path":"https://sagesteppe.github.io/safeHavens/articles/safeHavens.html","id":"comparision-of-different-sampling-schemes","dir":"Articles","previous_headings":"","what":"Comparision of different sampling schemes","title":"Showcasing Sampling Schemes with Sloths","text":", ’ve made got maps look ! look relatively similar plotted one another, let’s plot simultaneously see ’s still case.  , top three figures appear quite similar, Opportunistic method deviating slightly form . mind isolation distance (IBD) show biggest different, seems made sense naturally occurring patchiness species range. Ecoregion SEEMS…. Environmental also seems partition feature space quite well. Notably drawing couple clusters Pacific lowlands Northern Andes mountains.","code":"gbs.p + pbs.p + eas.p + os.p  +  ibdbs.p + ebs.p + ENVIbs.p +    plot_layout(ncol = 3)"},{"path":"https://sagesteppe.github.io/safeHavens/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Reed Benkendorf. Author, maintainer.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Benkendorf R (2024). safeHavens: Developing sampling schemas ex situ conservation plant germplasm. R package version 0.0.0.9000, https://sagesteppe.github.io/safeHavens/.","code":"@Manual{,   title = {safeHavens: Developing sampling schemas for ex situ conservation of plant germplasm},   author = {Reed Benkendorf},   year = {2024},   note = {R package version 0.0.0.9000},   url = {https://sagesteppe.github.io/safeHavens/}, }"},{"path":[]},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"description","dir":"","previous_headings":"","what":"Description","title":"Developing sampling schemas for ex situ conservation of plant germplasm","text":"goal package provide germplasm curators easily referable spatial data sets help prioritize field collection efforts.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Developing sampling schemas for ex situ conservation of plant germplasm","text":"provides functionality seven sampling schemes various curators interested , many likely outperform others certain species areas. package also creates species distribution models, goal germplasm sampling, rather predicting ranges fine resolutions, making inference; interested functionality R several dozen packages tailored purposes.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"description-1","dir":"","previous_headings":"","what":"Description","title":"Developing sampling schemas for ex situ conservation of plant germplasm","text":"package helps germplasm curators communicate areas interest collection teams collect new material accession. provides seven different sampling approaches curators choose individual taxon hope process.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Developing sampling schemas for ex situ conservation of plant germplasm","text":"safeHavens available github. can installed using remotes devtools like : installed can attached use like package github CRAN","code":"install.packages('devtools') devtools::install_github('sagesteppe/safeHavens')  install.packages('remotes') # remotes is very similar and  a good alternativ for this use case. remotes::install_github('sagesteppe/safeHavens') library(safeHavens)"},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Developing sampling schemas for ex situ conservation of plant germplasm","text":"safeHavens seven user facing functions generating sampling schemes. species distribution modelling section couple functions essential achieving EnvironmentalBasedSample design, : elasticSDM, PostProcessSDM, RescaleRasters writeSDMresults.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/index.html","id":"acknowledgements","dir":"","previous_headings":"","what":"Acknowledgements","title":"Developing sampling schemas for ex situ conservation of plant germplasm","text":"Edzer Pebesma, Krzysztof Dyba help seamless installations Ubuntu, getting pkgdown website running.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EcoregionBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine the sample size for n ecoregions in an area — EcoregionBasedSample","title":"Determine the sample size for n ecoregions in an area — EcoregionBasedSample","text":"Intersect vector data file ecoregions (ecoregions) range focal taxon (x) selects n ecoregions sample. fewer n ecoregions exist range species, extra samples added largest ecoregions area proportions meet n. function adequate support official Omernik L4 shapefiles created EPA, worked minimal experimentation vector data sources. ecoregions exist across range species n (common),  select method determine sample sizes (instance 1 sample per ecoregion) selected. Methods include sampling n largest, orsmallest ecoregions number, ecoregion disconnected habitat (measured soley number polygons). three methods returns largest polygon within criterion. note 'polygons'. Simple features able store polygon data two main formats, 'MULTIPOLYGON', individual polygons composing class stored collectively, 'POLYGONS' individual polygon unique entry within class. 'Polygons' generally used two areas class discontinuous, analyst wants easily analyze separately. 'MULTIPOLYGONS' generally created analyst interested understanding properties entire class. EPA Omernik spatial data set comes 'POLYGONS' 'MULTIPOLYGONS', used somewhat extensively believe creators struck happy balance creating many small polygons, e.g.  areas like coastal reef island (MULTIPOLYGON use case), big polygons. modify , rare occasion (essentially islands), refer 'polygon' may technically multipolygon.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EcoregionBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine the sample size for n ecoregions in an area — EcoregionBasedSample","text":"","code":"EcoregionBasedSample(   x,   ecoregions,   OmernikEPA,   n,   ecoregion_col,   increase_method,   decrease_method )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/EcoregionBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine the sample size for n ecoregions in an area — EcoregionBasedSample","text":"x range species simple feature (sf) object. ecoregions ecoregion vector data file (~shapefile) sf. OmernikEPA Boolean. TRUE indicates data US EPA minimally modified, FALSE several input required ensure function maps appropriately. left blank default method scan exact match standard Omernik ecoregion field (column) names, matched dispatched Omernik module, else fail unless associated columns specified (SEE ). n Numeric. desired total number samples across range ecoregion_col Character. Name column contain finest resolution data used analysis. Omernik L4 file defaults relevant columns automatically, different type file submitted, specified function fails. increase_method Character. Method implement number L4 ecoregions less n. decrease_method Character. Method implement number L4 ecoregions greater n. 'Largest' (default) select n largest ecoregions total area, select largest single polygon within classes. 'Smallest' select n smallest ecoregions total area, select largest single polygon within classes. '' select n ecoregions polygons, select largest polygon .","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EcoregionBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine the sample size for n ecoregions in an area — EcoregionBasedSample","text":"sf object, length input data set, finest resolution eco level, geometry fields retained, new column 'n' indicating many accession gather ecoregion.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EcoregionBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine the sample size for n ecoregions in an area — EcoregionBasedSample","text":"","code":"# First example is using a subset (and with simplified geometry) Omernik L4 # ecoregion shapefile from the EPA. Which as of the time of writing were  # available, at no cost, at the following URL # https://www.epa.gov/eco-research/level-iii-and-iv-ecoregions-continental-united-states  polygon <- spData::us_states |> dplyr::select(NAME) |>    dplyr::filter(NAME == 'California') |>    sf::st_transform(4326)     Weco <- sf::st_read(   file.path(system.file(package=\"safeHavens\"), 'extdata', 'WesternEcoregions.gpkg'),    quiet = TRUE) head(Weco) #> Simple feature collection with 6 features and 17 fields #> Geometry type: MULTIPOLYGON #> Dimension:     XY #> Bounding box:  xmin: -120.6736 ymin: 36.50122 xmax: -118.0553 ymax: 40.4275 #> Geodetic CRS:  WGS 84 #>      NA_L2NAME NA_L2CODE                      L3_KEY               NA_L3NAME #> 1 COLD DESERTS      10.1 13  Central Basin and Range Central Basin and Range #> 2 COLD DESERTS      10.1 13  Central Basin and Range Central Basin and Range #> 3 COLD DESERTS      10.1 13  Central Basin and Range Central Basin and Range #> 4 COLD DESERTS      10.1 13  Central Basin and Range Central Basin and Range #> 5 COLD DESERTS      10.1 13  Central Basin and Range Central Basin and Range #> 6 COLD DESERTS      10.1 13  Central Basin and Range Central Basin and Range #>   Shape_Leng             L2_KEY US_L3CODE NA_L1CODE NA_L3CODE #> 1   94308.62 10.1  COLD DESERTS        13        10    10.1.5 #> 2   49617.99 10.1  COLD DESERTS        13        10    10.1.5 #> 3 2106033.22 10.1  COLD DESERTS        13        10    10.1.5 #> 4  165631.31 10.1  COLD DESERTS        13        10    10.1.5 #> 5  510158.56 10.1  COLD DESERTS        13        10    10.1.5 #> 6   87086.56 10.1  COLD DESERTS        13        10    10.1.5 #>                NA_L1NAME                                          US_L4NAME #> 1 NORTH AMERICAN DESERTS Sierra Nevada-Influenced Semiarid Hills and Basins #> 2 NORTH AMERICAN DESERTS Sierra Nevada-Influenced Semiarid Hills and Basins #> 3 NORTH AMERICAN DESERTS Sierra Nevada-Influenced Semiarid Hills and Basins #> 4 NORTH AMERICAN DESERTS                                      Sierra Valley #> 5 NORTH AMERICAN DESERTS                                 Upper Owens Valley #> 6 NORTH AMERICAN DESERTS                                 Mono-Adobe Valleys #>                 US_L3NAME #> 1 Central Basin and Range #> 2 Central Basin and Range #> 3 Central Basin and Range #> 4 Central Basin and Range #> 5 Central Basin and Range #> 6 Central Basin and Range #>                                                     L4_KEY US_L4CODE #> 1 13aa  Sierra Nevada-Influenced Semiarid Hills and Basins      13aa #> 2 13aa  Sierra Nevada-Influenced Semiarid Hills and Basins      13aa #> 3 13aa  Sierra Nevada-Influenced Semiarid Hills and Basins      13aa #> 4                                      13ab  Sierra Valley      13ab #> 5                                 13ac  Upper Owens Valley      13ac #> 6                                 13ad  Mono-Adobe Valleys      13ad #>                       L1_KEY Shape_Area       NAME #> 1 10  NORTH AMERICAN DESERTS  285751793 California #> 2 10  NORTH AMERICAN DESERTS  103496425 California #> 3 10  NORTH AMERICAN DESERTS 6613879510 California #> 4 10  NORTH AMERICAN DESERTS  449164266 California #> 5 10  NORTH AMERICAN DESERTS 1763113790 California #> 6 10  NORTH AMERICAN DESERTS  162895833 California #>                             geom #> 1 MULTIPOLYGON (((-118.694 37... #> 2 MULTIPOLYGON (((-119.323 38... #> 3 MULTIPOLYGON (((-119.845 38... #> 4 MULTIPOLYGON (((-120.1395 3... #> 5 MULTIPOLYGON (((-118.463 37... #> 6 MULTIPOLYGON (((-118.7649 3...  out <- EcoregionBasedSample(polygon, Weco) #> Warning: attribute variables are assumed to be spatially constant throughout all geometries #> Warning: attribute variables are assumed to be spatially constant throughout all geometries sum(out$n) #> [1] 20  ggplot2::ggplot() +     ggplot2::geom_sf(data = out, ggplot2::aes(fill = factor(n)))     # This second example is from a recent publication by Morreno et al. 2022 and  # presents biogeographic regions of the Neotropics and is available from a  # google drive linked in the publication describing there creation located at # https://www.scielo.br/j/aabc/a/hPft4CK6RV8QBr8nP7bxhRQ/?lang=en#  # Essentially we showcase how a user can maintain this functions utility # while catering to data in a format differing from the Omernik L4 distribution.   neo_eco <- sf::st_read(    file.path(system.file(package=\"safeHavens\"), 'extdata', 'NeoTropicsEcoregions.gpkg'),     quiet = TRUE) sp_range <- sf::st_polygon( # complete  list(    rbind(      c(-80,-5), c(-80,10), c(-60,10), c(-55,5),      c(-60,-5), c(-80,-5)     )  ) ) |>  sf::st_sfc() |>   sf::st_as_sf() |>  sf::st_set_crs(4326) |>  dplyr::rename(geometry = x) |>  dplyr::mutate(Species = 'Da species')  out <- EcoregionBasedSample(sp_range, neo_eco, ecoregion_col = 'Provincias') #> Warning: attribute variables are assumed to be spatially constant throughout all geometries #> Warning: attribute variables are assumed to be spatially constant throughout all geometries sum(out$n) #> [1] 19  ggplot2::ggplot() +   ggplot2::geom_sf(data = neo_eco) +   ggplot2::geom_sf(data = out, ggplot2::aes(fill = factor(n))) +   ggplot2::geom_sf(data = sp_range, fill = NA, color = 'Red')    # Note that both of the files of the above ecoregions have had their geometry # simplified, i.e. made less complex - you should notice they look slightly angular # like an old cartoon such as Rugrats or so. We do this to reduce the file size # to make it easier to install the package, and reduce the run time of the functions # for these simple examples."},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"function utilizes output elastic net GLM model create weights matrix features relevant species distribution identify clusters throughout range incorporating PCNM/MEM data coordinates implement spatial contiguity.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"","code":"EnvironmentalBasedSample(   pred_rescale,   f_rasts,   lyr,   taxon,   path,   n,   fixedClusters,   n_pts,   planar_projection,   coord_wt,   buffer_d,   prop_split,   write2disk,   ... )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"pred_rescale rasterstack predictor layers rescaled represent beta coefficients elastic net (glmnet::glmnet) modelling process. See ?RescaleRasters implementation functionality. f_rasts rasters output SDM workflow. lyr Character. name layer want use analysis one : 'Threshold', 'Clipped', Supplemented'. missing defaults 'Supplemented'. taxon Character. name taxonomic entity models created. final raster clusters, results KNN classifier trainings, details clustering procedure (fixedClusters=TRUE). path root path output data saved, use WriteSDMresults. Defaults current working directory, check getwd(). n Numeric. number clusters desired. fixedClusters Boolean. Defaults TRUE, create n clusters. False use NbClust::NbClust determine number clusters. n_pts Numeric. number points use generating clusters, randomly sampled within mask area mask. Defaults 500. planar_projection Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. coord_wt Numeric. amount weigh coordinates distance matrix relative important variable identified elastic net regression. Defaults 2.5, setting 1 make value equivalent environmental PCNM variables. metric increases spatial contiguity clusters identified. buffer_d Numeric. using two-stage sampling increase sample size (number points) uncommon clusters, distance buffer individual pts located cells possible sampling. Defaults 3 allows area encompassing around 45-50 raster cells nearby possibly sampled. reasonable default coarsely gridded data (sensible grain type packages use cases), moderate resolution data (e.g. 250m-1km) may require higher values find meaningful differences variables locations. prop_split Numeric. proportion records used training KNN classifier. Defaults 0.8 use 80% records training 20% independent test sample. write2disk Boolean. Whether write results disk . Defaults FALSE. ... arguments passed NbClust::NbClust optimizing cluster numbers. Defaults using method = 'complete', compare results 20 methods select cluster number commonly generated algorithms. min.nc set default 5, overcome clusters use (easily overwritten supplying argument minc.nc=2, set lower), max.nc = 20 congruence many seed collection endeavors (overwritten max.nc = 10 example).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EnvironmentalBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create environmental and spatial clusters for targeting collection areas — EnvironmentalBasedSample","text":"Writes four objects disk, returns one object R session (optional).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Create equal area polygons over a geographic range — EqualAreaSample","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"function creates n geographic clusters geographic area (x), typically species range, using kmeans clustering.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"","code":"EqualAreaSample(x, n, pts, planar_projection, returnProjected, reps, BS.reps)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"x SF object terra spatraster. range generate clusters. n Numeric. number clusters desired. Defaults 20. pts Numeric. number points use generating clusters, placed grid like fashion across x. exact number points used may deviate slightly user submitted value allow equidistant spacing across x. Defaults 5,000. planar_projection Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. returnProjected Boolean. Whether return data set original input CRS (FALSE), new projection (TRUE). Defaults FALSE. reps Numeric. number times rerun voronoi algorithm, set polygons similar sizes, measured using variance areas selected. Defaults 100. BS.reps number bootstrap replicates evaluating results.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"list containing two objects, first results bootstrap simulations. second sf dataframe containing polygons smallest amount variance size.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/EqualAreaSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create equal area polygons over a geographic range — EqualAreaSample","text":"","code":"nc <- sf::st_read(system.file(\"shape/nc.shp\", package=\"sf\"), quiet = TRUE) |> dplyr::select(NAME)  set.seed(1) system.time(   zones <- EqualAreaSample(nc, n = 20, pts = 1000, planar_projection = 32617, reps = 100) ) #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #> Warning: did not converge in 10 iterations #>    user  system elapsed  #>  12.694   0.136  12.831   plot(nc, main = 'Counties of North Carolina')  plot(zones$Geometry, main = 'Clusters')  zones$SummaryData #>                  Metric       Value #> 1     variance.observed  8561240787 #> 2        quantile.0.001  8713321235 #> 3             lwr.95.CI  8561240787 #> 4             upr.95.CI 10097406923 #> 5    Voronoi.reps.asked         100 #> 6 Voronoi.reps.received         100 #> 7               BS.reps        9999"},{"path":"https://sagesteppe.github.io/safeHavens/reference/GridBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","title":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","text":"function creates 20 grid cells geographic area (x), typically species range.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/GridBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","text":"","code":"GridBasedSample(x, planar_projection, gridDimensions)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/GridBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","text":"x SF object terra spatraster. range generate clusters. planar_projection Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. gridDimensions single row form ouput TestGridSizes optimal number grids generate.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/GridBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","text":"simple features (sf) object containing final grids saving computer. See vignette questions saving two main types spatial data models (vector - used , raster).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/GridBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create hexagonal grid based polygons over a geographic range — GridBasedSample","text":"","code":"ri <- spData::us_states |>  dplyr::filter(NAME == 'Rhode Island') |>   sf::st_transform(32615)  sizeOptions <- TestGridSizes(ri) head(sizeOptions) # in this case let's shoot for 33 and see what happens #>       Name Grids  Variance GridNOx GridNOy #> 1 Smallest    41  146.7971       6       8 #> 2  Smaller    33  679.0576       5       7 #> 3 Original    23 1396.9884       4       6 #> 4   Larger    14 1296.7012       3       5 #> 5  Largest    10 1340.4689       2       4 sizeOptions <- sizeOptions[sizeOptions$Name == 'Original',]  output <- GridBasedSample(ri, 5070, gridDimensions = sizeOptions) #> Warning: attribute variables are assumed to be spatially constant throughout all geometries #> Warning: attribute variables are assumed to be spatially constant throughout all geometries #> Warning: attribute variables are assumed to be spatially constant throughout all geometries #> Warning: st_point_on_surface assumes attributes are constant over geometries plot(output)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"Create n seed collection areas based distance geographic (great circle) distance points.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"","code":"IBDBasedSample(   x,   n,   fixedClusters,   n_pts,   template,   prop_split,   min.nc,   max.nc )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"x Raster surface sample points within, e.g. output SDM$Supplemented. n Numeric. number clusters desired. fixedClusters Boolean. Defaults TRUE, create n clusters. False use NbClust::NbClust determine optimal number clusters. n_pts Numeric. number points use generating clusters, randomly sampled within mask area mask. Defaults 1000, generally allows enough points split KNN training. template Raster. raster file can used template plotting. prop_split Numeric. proportion records used training KNN classifier. Defaults 0.8 use 80% records training 20% independent test sample. min.nc Numeric. Minimum number clusters test fixedClusters=FALSE, defaults 5. max.nc Numeric. Maximum number clusters test fixedClusters=FALSE, defaults 20.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"simple features (sf) object containing final grids saving computer. See vignette questions saving two main types spatial data models (vector - used , raster).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/IBDBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample a species based on Isolation by Geographic Distance (IBD) — IBDBasedSample","text":"","code":"planar_proj = '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs'  x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv')) x <- x[,c('lon', 'lat')] x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326) x_buff <- sf::st_transform(x, planar_proj) |>  sf::st_buffer(125000) |> # we are working in planar metric coordinates, we are  sf::st_as_sfc() |> # buffer by this many / 1000 kilometers.   sf::st_union()  files <- list.files( # note that for this process we need a raster rather than    path = file.path(system.file(package=\"dismo\"), 'ex'), # vector data to accomplish   pattern = 'grd',  full.names=TRUE ) # this we will 'rasterize' the vector using terra predictors <- terra::rast(files) # this can also be done using 'fasterize'. Whenever # we rasterize a product, we will need to provide a template raster that our vector # will inherit the cell size, coordinate system, etc. from   x_buff.sf <- sf::st_as_sf(x_buff) |>    dplyr::mutate(Range = 1) |>    sf::st_transform(terra::crs(predictors))  # and here we specify the field/column with our variable we want to become  # an attribute of our raster v <- terra::rasterize(x_buff.sf, predictors, field = 'Range')   # now we run the function demanding 20 areas to make accessions from,  ibdbs <- IBDBasedSample(x = v, n = 20, fixedClusters = TRUE, template = predictors) #> Warning: coordinate ranges not computed along great circles; install package lwgeom to get rid of this warning #> Warning: coordinate ranges not computed along great circles; install package lwgeom to get rid of this warning #> Loading required package: ggplot2 #> Loading required package: lattice #> Warning: st_point_on_surface may not give correct results for longitude/latitude data plot(ibdbs)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Design additional collections around already existing collections — OpportunisticSample","title":"Design additional collections around already existing collections — OpportunisticSample","text":"function utilizes regular, nearly case existing collections, grid points develop sampling scheme n polygons.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Design additional collections around already existing collections — OpportunisticSample","text":"","code":"OpportunisticSample(polygon, n, collections, reps, BS.reps)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Design additional collections around already existing collections — OpportunisticSample","text":"polygon input sf polygon, .e. species range administrative unit, sampling desired. n Numeric. total number desired collections. Defaults 20. collections sf point geometry data set existing collections made. reps arguments passed np.boot BS.reps number bootstrap replicates evaluating results.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Design additional collections around already existing collections — OpportunisticSample","text":"list containing two sublists, first 'SummaryData' details number voronoi polygons generated, results bootstrap simulations. second 'Geometry', contains final spatial data products, can written end. See vignette questions saving two main types spatial data models (vector - used , raster).","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/OpportunisticSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Design additional collections around already existing collections — OpportunisticSample","text":"","code":"#' Design additional collections around already existing collections ri <- spData::us_states |>   dplyr::select(NAME) |>   dplyr::filter(NAME == 'Rhode Island') |>   sf::st_transform(32617) existing_collections <- sf::st_sample(ri, size = 5) |>   sf::st_as_sf() |>   dplyr::rename(geometry = x)  system.time(   out <- OpportunisticSample(polygon = ri, reps = 150)  ) # set very low for example #>    user  system elapsed  #>  13.965   0.083  14.050  # the function is actually very fast; 150 voronoi reps, with 9999 BS should only take about # 7 seconds per species so not much concern on the speed end of things. ggplot2::ggplot() +    ggplot2::geom_sf(data = out$Geometry, ggplot2::aes(fill = ID)) +    ggplot2::geom_sf(data = existing_collections)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"function utilizes regular, nearly case existing collections, grid points develop sampling scheme n polygons.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"","code":"PointBasedSample(polygon, n, collections, reps, BS.reps)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"polygon input sf polygon, .e. species range administrative unit, sampling desired. n Numeric. total number desired collections. Defaults 20. collections sf point geometry data set existing collections made. reps arguments passed np.boot BS.reps number bootstrap replicates evaluating results.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"list containing two objects, first results bootstrap simulations. second sf dataframe containing polygons smallest amount variance size.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PointBasedSample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a sampling grid based off of regularly sampled points across the species range. — PointBasedSample","text":"","code":"#' Utilize a grid based stratified sample for drawing up polygons ri <- spData::us_states |>   dplyr::select(NAME) |>   dplyr::filter(NAME == 'Rhode Island') |>   sf::st_transform(32617)     system.time(   out <- PointBasedSample(polygon = ri, reps = 10, BS.reps = 10) # set very low for example  ) #>    user  system elapsed  #>   0.876   0.004   0.880  # the function is actually very fast; 150 voronoi reps, with 9999 BS should only take about # 2 seconds per species so not much concern on the speed end of things! head(out$SummaryData) #>                  Metric    Value #> 1     variance.observed 12358061 #> 2        quantile.0.001 12360106 #> 3             lwr.95.CI 12358061 #> 4             upr.95.CI 12573411 #> 5    Voronoi.reps.asked       10 #> 6 Voronoi.reps.received        7 plot(out$Geometry)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":null,"dir":"Reference","previous_headings":"","what":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"last 'analytical' portion SD modelling process. produce binary (Yes/ NA) rasters species suitable habitat based three step process. first step uses dismo::thresholds determine feature raster want maximuize, case want raster less likely capture presence omit . can subset predicted habitat, dispersed comparing nearest neighbor distances observed points. Finally, can add back areas raster know species observed, hopefully missed original SDM, always suspicious points difficult fit light inertia rest species.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"","code":"PostProcessSDM(   rast_cont,   test,   train,   thresh_metric,   quant_amt,   planar_projection )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"rast_cont raw unaltered (except masked) raster predictions (x$RasterPredictions). test test data partition elasticSDM function (x$TestData). train train data partition elasticSDM function (x$TrainData). thresh_metric ?dismo::threshold options, defaults 'sensitivity' quant_amt quantile nearest neighbors distance use steps 2 3. defaults 0.25, using median nearest neighbor distance 10 bootstrapping replicates estimating buffer restrict SDM surface , minimum 10 bootstrap reps adding surface presence points placed binary suitable habitat. planar_projection Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/PostProcessSDM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Modify the output rasters from the SDM process to better match target goals — PostProcessSDM","text":"list containing two options. 1) spatraster 4 layers, ) continuous probabilities suitable habitat feed elasticSDM, B) raster binary format based specified thresholding statistic, C) binary raster B + habitat clipped buffer distances determined measuring nearest neighbor distances thresholding quantile D) binary raster C, adding distance points initially cells classified thresholding suitable habitat. D' general basis future steps, either B, C serve alternatives. 2) threshold statistics calculated dismo dataframe.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":null,"dir":"Reference","previous_headings":"","what":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"rescaled rasters can used clustering, predicting results cluster analysis back space final product.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"","code":"RescaleRasters(model, predictors, training_data, pred_mat)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"model final output model glmnet elasticSDM predictors raster stack use process elasticSDM training_data data went glmnet model, used calculating variance required scaling process. elasticSDM pred_mat Prediction matrix elasticSDM","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/RescaleRasters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rescale a raster stack to reflect the beta coefficients from a glmnet model — RescaleRasters","text":"list two objects. 1) rescaled raster stack. 2) table standardized unstandardized coefficients glmnet model.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/TestGridSizes.html","id":null,"dir":"Reference","previous_headings":"","what":"Get an estimate for how many grids to draw over a species range — TestGridSizes","title":"Get an estimate for how many grids to draw over a species range — TestGridSizes","text":"function uses dimensions species grid estimate many grids need added x y directions cover 20 grid cells roughly equal areas.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/TestGridSizes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get an estimate for how many grids to draw over a species range — TestGridSizes","text":"","code":"TestGridSizes(target)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/TestGridSizes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get an estimate for how many grids to draw over a species range — TestGridSizes","text":"target species range simple feature (sf) object.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/TestGridSizes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get an estimate for how many grids to draw over a species range — TestGridSizes","text":"dataframe testing results grid combination. user needs select optimal grid size based tradeoff minimizing variance, without creating many grids need erased. Rhode Island example use 'Original' option asks 4 x grids 7 y grids.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/TestGridSizes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get an estimate for how many grids to draw over a species range — TestGridSizes","text":"","code":"ri <- spData::us_states |> dplyr::select(NAME) |>    dplyr::filter(NAME == 'Rhode Island') |>    sf::st_transform(32617)  sizeOptions <- TestGridSizes(ri) head(sizeOptions) #>       Name Grids    Variance GridNOx GridNOy #> 1 Smallest    49    9.242133       6       9 #> 2  Smaller    37  355.799568       5       8 #> 3 Original    25 1038.567100       4       7 #> 4   Larger    16 1245.553777       3       6 #> 5  Largest    11 1322.092849       2       5"},{"path":"https://sagesteppe.github.io/safeHavens/reference/VoronoiSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a voronoi sample of an area n times — VoronoiSampler","title":"Make a voronoi sample of an area n times — VoronoiSampler","text":"Split area n polygons roughly equal area, optionally removing default points replacing existing collections build future collections around. Split area n polygons roughly equal area, optionally removing default points replacing existing collections build future collections around.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/VoronoiSampler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a voronoi sample of an area n times — VoronoiSampler","text":"","code":"VoronoiSampler(polygon, n, collections, reps)  VoronoiSampler(polygon, n, collections, reps)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/VoronoiSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a voronoi sample of an area n times — VoronoiSampler","text":"polygon input sf polygon, .e. species range administrative unit, sampling desired. n Numeric. total number desired collections. Defaults 20. collections sf point geometry data set existing collections made. reps Numeric. number times rerun voronoi algorithm, set polygons similar sizes, measured using variance areas selected. Defaults 150, may accomplish around 100 succesful iterations.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/WesternEcoregions.html","id":null,"dir":"Reference","previous_headings":"","what":"Omernik level 4 ecoregions of California and Oregon — WesternEcoregions","title":"Omernik level 4 ecoregions of California and Oregon — WesternEcoregions","text":"vector data set Omernik level 4 ecoregions clipped California Oregon. Downloaded https://www.epa.gov/eco-research/level-iii--iv-ecoregions-continental-united-states simplified using mapshaper. vector data set Bioregions developed Morrone et al. 2022 Downloaded https://neotropicalmap.atlasbiogeografico.com/ simplified using mapshaper.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/WesternEcoregions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Omernik level 4 ecoregions of California and Oregon — WesternEcoregions","text":"","code":"data(WesternEcoregions)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/WesternEcoregions.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Omernik level 4 ecoregions of California and Oregon — WesternEcoregions","text":"spatial vector data set. spatial vector data set.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/WesternEcoregions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Omernik level 4 ecoregions of California and Oregon — WesternEcoregions","text":"L4_KEY. Full name codes Level 4 ecoregions US_L4CODE. Codes level 4 ecoregions US_L4NAME. Names level 4 ecoregions Provincias Full name bioregion","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/assignGrid_pts.html","id":null,"dir":"Reference","previous_headings":"","what":"place random points in the polygon which will be dissolved with the larger polygons — assignGrid_pts","title":"place random points in the polygon which will be dissolved with the larger polygons — assignGrid_pts","text":"function ran within GridBasedSample place points throughout polygon geometries merged larger polygons assign neighboring polygons based much area want grow polygons .","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/assignGrid_pts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"place random points in the polygon which will be dissolved with the larger polygons — assignGrid_pts","text":"","code":"assignGrid_pts(neighb_grid, focal_grid, props, nf_pct)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/assignGrid_pts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"place random points in the polygon which will be dissolved with the larger polygons — assignGrid_pts","text":"neighb_grid  focal_grid  props  nf_pct","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/assign_pts_frst.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign randomly sampled points to their nearest grid based on the difference between the polygons original and desired size. — assign_pts_frst","title":"Assign randomly sampled points to their nearest grid based on the difference between the polygons original and desired size. — assign_pts_frst","text":"Assign randomly sampled points nearest grid based difference polygons original desired size.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/assign_pts_frst.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign randomly sampled points to their nearest grid based on the difference between the polygons original and desired size. — assign_pts_frst","text":"","code":"assign_pts_frst(x, props, nf_pct)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/assign_pts_frst.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign randomly sampled points to their nearest grid based on the difference between the polygons original and desired size. — assign_pts_frst","text":"x distance matrix points potential neighbors polygons props existing proportional area potential neighbor polygons nf_pct desired proportional area potential neighbor polygon","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/createPCNM_fitModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Create global/regional PCNM surfaces and fit elastic net regression to all covariates — createPCNM_fitModel","title":"Create global/regional PCNM surfaces and fit elastic net regression to all covariates — createPCNM_fitModel","text":"function lifting SDM workflow. create PCNM/MEM surfaces subset local/global maps. can use Thin plate regression predict onto actual raster surface can used prediction downstream.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/createPCNM_fitModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create global/regional PCNM surfaces and fit elastic net regression to all covariates — createPCNM_fitModel","text":"","code":"createPCNM_fitModel(x, planar_proj, ctrl, indices_knndm, sub, test)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/createPCNM_fitModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create global/regional PCNM surfaces and fit elastic net regression to all covariates — createPCNM_fitModel","text":"x training data sf/tibble/dataframe planar_proj Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. ctrl control object created character SDM function. indices_knndm sdm function sub subset predictors elasticSDM test test data partition elasticSDM","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/createPCNM_fitModel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create global/regional PCNM surfaces and fit elastic net regression to all covariates — createPCNM_fitModel","text":"use cross validation determine suitable glmnet model alpha lambda, fit using glmnet. returns three objects spit environment, 1) pcnm, surfaces eigenvectors used glmnet model (including shrunk ), 2) glmnet model 3) fitting information carets process.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a quick SDM using elastic net regression — elasticSDM","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"function quickly creates SDM using elastic net regression, properly format data downstream use safeHavens workflow. Note elastic net models used couple important reasons: rescale input independent variables modelling, allowing us combine raw data beta coefficients use clustering algorithms downstream. also allowing 'shrinking' terms models shrinking terms models able get levels ecological inference prohibited older model selection frameworks.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"","code":"elasticSDM(x, predictors, planar_projection, domain, quantile_v)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"x (simple feature) sf data set occurrence data species. predictors terra 'rasterstack' variables serve indepedent predictors. planar_projection Numeric, character vector. EPSG code, proj4 string, planar coordinate projection, meters, use function. species narrow ranges UTM zone may best (e.g. 32611 WGS84 zone 11 north, 29611 NAD83 zone 11 north). Otherwise continental scale projection like 5070 See https://projectionwizard.org/ information CRS. value simply passed sf::st_transform need experiment. domain Numeric, many times larger make entire domain analysis simple bounding box around occurrence data x. quantile_v Numeric, variable used thinning input data, e.g. quantile = 0.05 remove records within lowest 5% distance iteratively, remaining records apart distance . want essentially thinning happen just supply 0.01. Defaults 0.025.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"list 12 objects, subsequently used downstream  SDM Post processing sequence, think best written disk. actual model prediction raster surface present first list 'RasterPredictions', indepedent variables used final model present 'Predictors, just global PCNM/MEM raster surfaces 'PCNM'. fit model 'Model', cross validation folds stored 'CVStructure', results single test/train partition 'ConfusionMatrix', two data split 'TrainData' 'TestData' finally 'PredictMatrix' used classifying test data confusion matrix.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/elasticSDM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a quick SDM using elastic net regression — elasticSDM","text":"","code":"if (FALSE) { # \\dontrun{   x <- read.csv(file.path(system.file(package=\"dismo\"), 'ex', 'bradypus.csv'))  x <- x[,c('lon', 'lat')]  x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)   files <- list.files(    path = file.path(system.file(package=\"dismo\"), 'ex'),     pattern = 'grd',  full.names=TRUE )  predictors <- terra::rast(files)  sdModel <- elasticSDM(    x = x, predictors = predictors, quantile_v = 0.025,    planar_projection =      '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs')        terra::plot(sdModel$RasterPredictions) } # }"},{"path":"https://sagesteppe.github.io/safeHavens/reference/first_neigh_directions.html","id":null,"dir":"Reference","previous_headings":"","what":"We use spdep::knearneigh to ensure we obtain 4 of the nearest neighbors to a polygon — first_neigh_directions","title":"We use spdep::knearneigh to ensure we obtain 4 of the nearest neighbors to a polygon — first_neigh_directions","text":"However, system limitations, island ringed land, work adequately. island coast, may return neighbors 'behind' another neighbor. draw lines focal portion grid combining neighbor, lines cross neighbor discarded.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/first_neigh_directions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"We use spdep::knearneigh to ensure we obtain 4 of the nearest neighbors to a polygon — first_neigh_directions","text":"","code":"first_neigh_directions(from, destinations)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/first_neigh_directions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"We use spdep::knearneigh to ensure we obtain 4 of the nearest neighbors to a polygon — first_neigh_directions","text":"point surface grid merged. POS ensure lands feature. destinations set kearneigh accepting polygon merges","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/first_neigh_directions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"We use spdep::knearneigh to ensure we obtain 4 of the nearest neighbors to a polygon — first_neigh_directions","text":"preferable simply using nearest feature (e.g. sf:;st_nearest_feature) chain islands elongated may appropriately split across multiple grids downstream .","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/get_elements.html","id":null,"dir":"Reference","previous_headings":"","what":"Recursively grab a named component of a list. — get_elements","title":"Recursively grab a named component of a list. — get_elements","text":"Recursively grab named component list. Recursively grab named component list.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/get_elements.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recursively grab a named component of a list. — get_elements","text":"","code":"get_elements(x, element)  get_elements(x, element)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/get_elements.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recursively grab a named component of a list. — get_elements","text":"x list lists element quoted name list element extract.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/healPolygons.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean up unioned geometries - part 1 — healPolygons","title":"Clean up unioned geometries - part 1 — healPolygons","text":"function uses sf::st_snap remove small lines artifacts associated unioning polygons. ran within snapGrids","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/healPolygons.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean up unioned geometries - part 1 — healPolygons","text":"","code":"healPolygons(x)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/healPolygons.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean up unioned geometries - part 1 — healPolygons","text":"x output snapgrids","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/reduceFinalGrids.html","id":null,"dir":"Reference","previous_headings":"","what":"More sliver fixing — reduceFinalGrids","title":"More sliver fixing — reduceFinalGrids","text":"instances tiny little grids tagged along processing. just give arbitrary nearest feature. annoying chance (... millionth area...) arbitrary random point missed, areas tend remarkable inconsequential, worth randomly reassigning neighbor","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/reduceFinalGrids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"More sliver fixing — reduceFinalGrids","text":"","code":"reduceFinalGrids(final_grids)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/reduceFinalGrids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"More sliver fixing — reduceFinalGrids","text":"final_grids truthfully nearly final point.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. magrittr %>%","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/snapGrids.html","id":null,"dir":"Reference","previous_headings":"","what":"turn the point grid suggestions made by assignGrid_pts into polygons — snapGrids","title":"turn the point grid suggestions made by assignGrid_pts into polygons — snapGrids","text":"function part grid based sampling process turn small grid cells, broken , larger existing grid cells.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/snapGrids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"turn the point grid suggestions made by assignGrid_pts into polygons — snapGrids","text":"","code":"snapGrids(x, neighb_grid, focal_grid)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/snapGrids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"turn the point grid suggestions made by assignGrid_pts into polygons — snapGrids","text":"x output assignGrid_pts neighb_grid neighboring grid options. focal_grid grid reassign area .","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/snapR.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean up unioned geometries - part 2 — snapR","title":"Clean up unioned geometries - part 2 — snapR","text":"function uses sf::st_snap remove small lines artifacts associated unioning polygons","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/snapR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean up unioned geometries - part 2 — snapR","text":"","code":"snapR(x)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/snapR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean up unioned geometries - part 2 — snapR","text":"x output healPolygons","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/trainKNN.html","id":null,"dir":"Reference","previous_headings":"","what":"partition data and train a simple KNN model — trainKNN","title":"partition data and train a simple KNN model — trainKNN","text":"Simply use partition test data quickly train simple model","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/trainKNN.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"partition data and train a simple KNN model — trainKNN","text":"","code":"trainKNN(x, split_prop)"},{"path":"https://sagesteppe.github.io/safeHavens/reference/trainKNN.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"partition data and train a simple KNN model — trainKNN","text":"x weighted matrix including class ID column 'ID' split_prop prop data partitions.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":null,"dir":"Reference","previous_headings":"","what":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"function used write wide range values fitPredictOperationalize process. create multiple subdirectories within user specified path. include: 'Rasters' raster stack four final rasters go, 'Fitting' details model fitting caret placed, 'Models' final fit model go, 'Evaluation' evaluation statistics placed, 'Threshold' results form dismo::threshold placed.","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"","code":"writeSDMresults(   path,   taxon,   cv_model,   pcnm,   model,   cm,   coef_tab,   f_rasts,   thresh )"},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"path root path 5 folders created, exist. taxon name taxonomic entity models created. cv_model cross validation data elasticSDM pcnm pcnm/mem rasters elasticSDM model final glmnet model elasticSDM cm confusion matrix elasticSDM coef_tab coefficient table RescaleRasters f_rasts final rasters RescaleRasters thresh threshold statistics PostProcessSDM","code":""},{"path":"https://sagesteppe.github.io/safeHavens/reference/writeSDMresults.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save the results of SDMs from the elasticSDM, RescaleRasters and PostProcessSDM function in safeHavens — writeSDMresults","text":"objects, objects specified, written disk.","code":""}]
