---
title: "Worked Example"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Worked Example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
description: >
  Downloading species occurrence data, creating hulls, sampling and writing out data for sample design. 
---

You will need to install `rgbif` to follow along with this example. 

```{r load packages}
library(dplyr)
library(ggplot2)
library(sf)
library(rgbif)
library(spData)
library(safeHavens)
library(tidyr)
library(gstat)
library(sp)
```

```{r helper function, echo = F}
vario_estimate <- function(x, bb_dist = 25000, abs_dist = 2500){

  x <- st_transform(x, 5070) ## transform to simpify for buffer distances
  pres <- dplyr::mutate(x, presence = 1)

  ## create bounding box to restrict pseudo-absence records. 
  bb <- st_buffer(st_union(x), dist = bb_dist) 

  ## generate pa records, more than presences exist, and remove the ones proximal
  # to the occurrence points
  abs <- st_sample(bb, size = round(nrow(x)*1.2), 0) |>
    st_as_sf() |>
    st_filter(
      x,
      .predicate = st_is_within_distance,
      dist = abs_dist,
      .negate = TRUE
    ) |>
    mutate(presence = 0) |>
    rename(geometry = x)

  ## prep data to show the empirical variogram
  dat <- bind_rows( ## prepare presence absence data
    pres[, "presence"],
    abs[, "presence"]
  )
  dat_sp <- as(dat, "Spatial")

  vg <- variogram(
    presence ~ 1,
    dat_sp
  )

  ## instead of actually fitting a model to the empirical data, see where the records
  ## approach the sill.   
  fit <- robust_range(vg)

  ## return data
  return(list(
    range = fit,
    variogram = vg
  ))
}

  ## function for 
  robust_range <- function(vg, q = 0.9) {

    sill <- median(vg$gamma[vg$dist > quantile(vg$dist, 0.7)], na.rm = TRUE)
    idx <- which(vg$gamma >= q * sill)[1]
    if (is.na(idx)) return(NA_real_)
    vg$dist[idx]
    }
```

We will download occurrence data for a couple species, just so we have the code set up for `applying` through multiple species in a more realistic manner. 
```{r download species occurrence data from gbif}
## small subset of useful columns for example
cols = c('decimalLatitude', 'decimalLongitude', 'dateIdentified', 'acceptedScientificName', 'datasetName', 
  'coordinateUncertaintyInMeters', 'basisOfRecord', 'institutionCode', 'catalogNumber')

## download species data using scientificName, can use keys and lookup tables for automating many taxa. 
cymu <- rgbif::occ_search(scientificName = "Vesper multinervatus", limit = 1000)

### check to see what CRS are in here, these days usually standardized to wgs84 (epsg:4326)
table( cymu[['data']]['geodeticDatum']) 

## subset the data to relevant columns 
cymu_cols <- cymu[['data']][,cols]

## and repeat this again so a second set of data are on hand 
bowa <- rgbif::occ_search(scientificName = "Bouteloua warnockii", limit = 1000)
bowa_cols <- bowa[['data']][,cols]

## contrived multispecies example. 
spp <- bind_rows(bowa_cols, cymu_cols) |>
  drop_na(decimalLatitude, decimalLongitude) |> # any missing coords need dropped. 
  st_as_sf(coords = c( 'decimalLongitude', 'decimalLatitude'), crs = 4326, remove = F)

rm(cymu, bowa, cols, cymu_cols, bowa_cols)
```

glimpse at the data
```{r}
western_states <- spData::us_states |>
  dplyr::filter(REGION == 'West' & ! NAME %in% c('Montana', 'Washington', 'Idaho', 'Oregon', 'Wyoming') |
   NAME %in% c('Oklahoma', 'Texas', 'Kansas')) |>
  dplyr::select(NAME, geometry) |>
  st_transform(4326)

ggplot() +
  geom_sf(data = western_states) +
  geom_sf(data = spp, aes(color = acceptedScientificName, shape = acceptedScientificName)) +
  theme_void() +
  theme(legend.position = 'bottom')

arrange(spp, by = decimalLatitude, desc=FALSE) |>
  head(5)
spp <- filter(spp, decimalLatitude <= 40)
```

One points location is incorrect, it's latitude is great - we remove it above with the filter.

```{r}
bb <- st_transform(spp, 5070) |>
  st_buffer(100000) |>
  st_transform(4326) |>
  st_bbox()

western_states <- st_crop(western_states, bb)

base <- ggplot() +
  geom_sf(data = western_states, color = 'white') +
  geom_sf(data = spp, aes(color = acceptedScientificName, fill = acceptedScientificName)) +
  theme_void() +
  theme(legend.position = 'bottom')

base
```


## Polygon hulls for sampling 
Three options for drawing out polygon distances. 

```{r}
sppL <- split(spp, f = spp$acceptedScientificName)

concavities <- function(x, d, rat){

  acceptedScientificName <- x[['acceptedScientificName']][1]
  out <- st_transform(x, 5070) |>
    st_buffer(dist = d) |>
    st_union() |> 
    st_concave_hull(ratio = rat) |> 
    st_sf() |>
    rename(geometry = 1) |>
    mutate(acceptedScientificName, .before = geometry)

}

spp_concave <- sppL |>
  purrr::map(~ concavities(.x, d = 20000, rat = 0.4))
spp_convex <- sppL |>
  purrr::map(~ concavities(.x, d = 20000, rat = 1.0))

rm(concavities)
```

Concave hulls

```{r plot concave hulls}
base +
  geom_sf(data = spp_concave[[1]], aes(fill = acceptedScientificName), alpha = 0.2) +
  geom_sf(data = spp_concave[[2]],  aes(fill = acceptedScientificName), alpha = 0.2) 
```

Convex hulls 
```{r plot convex hulls}
base +
  geom_sf(data = spp_convex[[1]],  aes(fill = acceptedScientificName), alpha = 0.2) +
  geom_sf(data = spp_convex[[2]], aes(fill = acceptedScientificName), alpha = 0.2) 

rm(spp_convex)
```

For the sake of the example we will only use the concave ranges going forward. 

```{r determine radius for buffering points}
buffered <- sppL |>
  purrr::map(~ vario_estimate(.x, bb_dist = 50000))

lapply(buffered, `[[`, 'range')

buffered +
  geom_sf(data = buffered[[1]],  aes(fill = acceptedScientificName), alpha = 0.2) +
  geom_sf(data = buffered[[2]], aes(fill = acceptedScientificName), alpha = 0.2) 
```

## Sampling the polygon hulls 

Perform equal area sampling
```{r perform sampling}
eas <- spp_concave |>
  purrr::map(~ EqualAreaSample(.x, n = 10, pts = 250, planar_projection = 5070, reps = 25))

base +
  geom_sf(data = eas[[2]][['Geometry']], aes(fill = factor(ID)), alpha = 0.2)
```

Perform isolation by distance (IBD) based sampling.

```{r ibd sampling}
# create arbitrary template for example - best to do this over your real range of all species collections
# so that it can be recycled across species. 
template <- terra::rast(terra::ext(bb), crs = "epsg:4326", resolution = c(0.1, 0.1))

# the species range now gets 'burned' into the raster template. 
spp_concave <- purrr::map(spp_concave, \(x) {
  st_as_sf(x) |> 
    mutate(Range = 1, .before = geometry) |>
    st_transform(4326) |> 
    terra::rasterize(template, field = 'Range') 
})

ibd_samples <- spp_concave |>
  purrr::map(~ IBDBasedSample(.x, n = 10, fixedClusters = FALSE, template = template))

base + 
  geom_sf(data = ibd_samples[[1]][['Geometry']], aes(fill = factor(ID)), alpha = 0.2)

```

## prioritize sample areas 

Prioritize sampling areas
```{r prioritize sample areas}
ibd_samples_priority <- ibd_samples %>%
  purrr::map(~ list(Geometry = st_transform(.x$Geometry, 5070))) |> 
  purrr::map(~ PrioritizeSample(.x$Geometry, method = 'centered', reps = 99, n_breaks = 3))

base + 
  geom_sf(data = ibd_samples_priority[[2]][['Geometry']], aes(fill = factor(Level)), alpha = 0.2)
```

Write out the data for long term storage. 
```{r write out data for storage}
## create a directory to hold the outputs
p2Collections <- file.path('~', 'Documents', 'WorkedExample_Output')
dir.create(p2Collections, showWarnings = FALSE)

## write out the raster template for future use. 
dir.create(file.path(p2Collections, 'IBD_raster_template'), showWarnings = FALSE)
terra::writeRaster(template, 
  filename = file.path(p2Collections, 'IBD_raster_template', 'IBD_template.tif'), 
  overwrite = TRUE)

## save each species as geopackage. ## 1) points, 2) hull 3) zones, 4) prioritized samples. 


```

Clean up environment etc. 
```{r clean up environment}

```

