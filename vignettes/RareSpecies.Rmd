---
title: "Rare Species Sampling Schema"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Rare Species Sampling Schema}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
description: >
  Running an additional sampling scheme for rare species. 
---

## prepare data
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Load the required packages. 

```{r required packages, message=FALSE}
library(safeHavens)
library(ggplot2)
library(patchwork)
set.seed(99)
```

Here we will use the Bradypus data included in the `dismo` package again. 

```{r import bradypus data for testing}
x <- read.csv(file.path(system.file(package="dismo"), 'ex', 'bradypus.csv'))
x <- x[,c('lon', 'lat')]
x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)
```

And we will create the same base map used in `GettingStarted`. 

```{r create map for visualizing results, echo = F}
planar_proj <- '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs'

americas <- spData::world

x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)
x_buff <- sf::st_transform(x, planar_proj) |>
  sf::st_buffer(125000) |> 
  sf::st_as_sfc() |> 
  sf::st_union() |>
  sf::st_transform(4326) |>
  sf::st_buffer(100000) |>
  sf::st_bbox()

americas <- spData::world 
americas <- sf::st_crop(americas, x_buff) |>
  dplyr::select(name_long)

bb <- sf::st_bbox(x_buff)

map <- ggplot() + 
  geom_sf(data = americas) + 
  theme(
    legend.position = 'none', 
    panel.background = element_rect(fill = "aliceblue"), 
    panel.grid.minor.x = element_line(colour = "red", linetype = 3, linewidth  = 0.5), 
    axis.ticks=element_blank(),
    axis.text=element_blank(),
    plot.background=element_rect(colour="steelblue"),
    plot.margin=grid::unit(c(0,0,0,0),"cm"),
    axis.ticks.length = unit(0, "pt"))+ 
  coord_sf(xlim = c(bb[1], bb[3]), ylim = c(bb[2], bb[4]), expand = FALSE)

rm(americas, bb, planar_proj, x_buff)
```

While all other functions in the package handle `sf` objects directly, this function will actually just use a simple data frame of sites, to simplfy handing data off to C++ for optimization routines. 

The input to the `maximizeDispersion` function is a list with two elements: a distance matrix, and a data frame of site locations and attributes. 
The data frame must contain the following columns. 

```{r prep bradypus for sampling}
n_sites <- nrow(x) 
df <- data.frame(
  site_id = seq_len(n_sites),
  required = FALSE,
  coord_uncertainty = 0, 
  lon = sf::st_coordinates(x)[,1], 
  lat = sf::st_coordinates(x)[,2]
)

head(df)
```

The second required element, the distance matrix, can be calculated with the `greatCircleDistance` function in the package. 
Please use this rather than st_distance from `sf` for consistency, as the units differ slightly. 
If you want to use `sf::st_distance`, make sure to convert the units to match the scale of the `greatCircleDistance` function, otherwise the results will be incorrect.

```{r calculate distance matrix}
dist_mat <- sapply(1:nrow(df), function(i) {
   greatCircleDistance(
     df$lat[i], df$lon[i],
     df$lat, df$lon
   )
 })
```

The optimization routine requires at least one 'required' site to be specified. 
Here we will select the site closest to the geographic center of all sites as the required site.

Normally this can refer to existing accessions, or administrative units, or preserves which are helping to implement the germplasm collection, and are fortunate enough to already have some samples or at least guaranteed access.

```{r required points}
dists2c <- greatCircleDistance(
  median(df$lat), 
  median(df$lon), 
  df$lat, 
  df$lon
)
df[order(dists2c)[1],'required'] <- TRUE
```

This function not only bootstraps sites to simulate the true distribution distribution of the species, but it also bootstraps coordinate uncertainty for each site. 
Here we will randomly assign 20% of the sites to have coordinate uncertainty between 1 km and 40 km. 
Note that he argument is always in meters. 

```{r simulating coordinate uncertainty}
uncertain_sites <- sample(
  setdiff(seq_len(n_sites), 
  which(df$required)), 
  size = round(n_sites*0.2, 0)
  )
df$coord_uncertainty[uncertain_sites] <- runif(length(uncertain_sites), 1000, 40000) # meters
```

## Run dispersion maximization based only on geographic distances

The input to the function is the distance matrix, and the site data. 

```{r combine the input data}
test_data <- list(
  distances = dist_mat,
  sites = df
  )

str(test_data)

rm(x, n_sites, uncertain_sites, dists2c)
```

The funtion `maximizeDispersion` has several parameters to control the optimization routine.

```{r run with geograpihc distances}
st <- system.time( {
    geo_res <- maximizeDispersion(  ## reduce some parameters for faster run. 
      input_data = test_data,
      n_sites = 10,
      lambda_var = 0.05,
      n_bootstrap = 500,
      objective = "sum",
      n_local_search_iter = 50,
      n_restarts = 2
    )
  }
)
```

The function operates relatively quick with few bootstraps and few sites, but will take considerably longer with more complex scenarios.
We recommened using at least 999 bootstraps for real world applications. 

```{r}
st
rm(st)
```


### return output structure
Various elements are returned in the output list.
```{r structure of output}
str(geo_res)
```

The stability score shows how often the most frquently selected network of sites was selected from the bootstrapped runs. 
```{r}
head(geo_res$stability_score)
```

The stability data frame shows how often each site was selected across all bootstrap runs. 
```{r}
head(geo_res$stability)
```

Many users may find the combindation of their input data with a few columns, to be all they need to carry on after the results. 
```{r}
head(geo_res$input_data)
```

Run parameters are saved in the settings element.
```{r}
head(geo_res$settings)
```

### visualize the selection results

```{r first selection of target sites}
map + 
  geom_point(data = geo_res$input_data, 
  aes(
    x = lon, 
    y = lat, 
    shape = required, 
    size = cooccur_strength,
    color = selected
    )
  ) +
 # ggrepel::geom_label_repel(aes(label = site_id), size = 4) + 
  theme_minimal() + 
  labs(title = 'Priority Selection Status of Sites; Geographic Distances')
```


```{r priority ranking plot}
map + 
  geom_point(data = geo_res$input_data, 
    aes(
      x = lon, 
      y = lat, 
      shape = required, 
      size = -sample_rank,
      color = sample_rank
      )
    ) +
 # ggrepel::geom_label_repel(aes(label = sample_rank), size = 4) +
  theme_minimal()   
```


## run dispersion maximization with geographic and environmental distances

### extract prep environmental distances 

```{r prep environmental distance matrix}
files <- list.files(
  path = file.path(system.file(package="dismo"), 'ex'), 
  pattern = 'grd',  full.names=TRUE )
predictors <- terra::rast(files) # import the independent variables
rm(files)
```

For our environmental distances, we will use a PCA transformation of the environmental variables.
We will simply scrape 100 random points from the raster layers to calculate the PCA.
Then predict the PCA raster layers across the entire study area.
We will take the first two layers, and calculate environmental distances based on these two layers.

```{r run with geographic and environmental distances}
pts <- terra::spatSample(predictors, 100, na.rm = TRUE)
pts <- pts[, names(pts)!='biome' ] # remove categorical variable for distance calc

pca_results <- stats::prcomp(pts, scale = TRUE)
round(pca_results$sdev^2 / sum(pca_results$sdev^2), 2) # variance explained
pca_raster <- terra::predict(predictors, pca_results)

terra::plot(terra::subset(pca_raster, c(1:2))) # prediction of the pca onto a new raster
rm(pts, predictors, pca_results)
```

we keep the first two PCA layers for environmental distance calculation.
More layers will increase dimenstionality, and may lead to less useful results.
Note that it's fine to use a euclidean distance calculation for these, as the values are truly in the position of the plot. 

```{r extract environmental values and calculate distance matrix}
env_values <- terra::extract(pca_raster, 
  sf::st_coordinates(
    sf::st_as_sf(
      df, 
      coords = c('lon', 'lat'), 
      crs = 4326
    )
  )
)[,1:2]
plot(env_values, main = 'environmental distance of points from first two PCA axis')

env_dist_mat <- as.matrix(
    dist(env_values)
  )

rm(pca_raster)
```

```{r run with environmental distance}
test_data <- list(
  distances = simplify2array(list(dist_mat, env_dist_mat)),
  sites = df
  )

st <- system.time( 
  {
    env_res <- maximizeDispersion(  ## reduce some parameters for faster run. 
      input_data = test_data,
      n_sites = 10,
      lambda_var = 0.1,
      weight_1 = 0.1, 
      weight_2 = 0.9,
      n_bootstrap = 99, # considerably slower function!!!
      objective = "sum",
      n_local_search_iter = 50,
      n_restarts = 2
    )
  }
)

rm(dist_mat, env_dist_mat)
```

Adding the second distance matrix into the mix slows things down considerable. 

Note that when this function has jittered points, not only is the point jittered randomly for the geographic component, the environmental distance matrix is also jittered. 
The jitter for the latter matrix come from fitting a linear model where environmental distance is fit as a function of geographic distance. 
The conditional mean is then used as a noise to alter the environmental distance to/from each record. 
However, this is unrelated to the slowdown. 

```{r}
st
rm(st)
```

```{r}
head(env_res$stability_score)
```

```{r }
map + 
  geom_point(data = env_res$input_data, 
    aes(
      x = lon, 
      y = lat, 
      shape = required, 
      size = cooccur_strength,
      color = selected
      )
    ) +
 # ggrepel::geom_label_repel(aes(label = site_id), size = 4) + 
  theme_minimal() + 
  labs(title = 'Priority Selection Status of Sites; Environmental')
```

## alternative methods for required central points

In the example above we use a point at the median geographic center of the populations. 

We can also identify the population which is *most* near the highest density of populations. 
Intuitively, this would be suggested as a population with a very high genetic diversity.  

```{r required point method - population centroid - heat map}
dens <- with(df, MASS::kde2d(lon, lat, n = 200))
max_idx <- which(dens$z == max(dens$z), arr.ind = TRUE)[1,]
max_point <- c(dens$x[max_idx[1]], dens$y[max_idx[2]])

pops_centre <- sweep(df[c('lon', 'lat')], 2, max_point, "-")
pop_centered_id <- which.min(rowSums(abs(pops_centre^2)))

rm(dens, max_idx, max_point, pops_centre)
```

Likewise we can identify the *population* which is most near the 'center' of the environmental variable space. 

```{r required point method - environmental centroid}
env_centered <- sweep(env_values, 2, sapply(env_values, median), "-")
env_centered_id <- which.min(rowSums(abs(env_centered^2)))

rm(env_values)
```

All three of these options can be combined to feed in three center points for required sampling locations, providing a robust set of 'core' diversity for the species. 
Personally I would consider the 'pop centered' population to be the most important required site to center a design off of. 

Here we will just showcase their positions

```{r plot alternative centroids}
# geographic centroid was pt 47
centers <- df[ c(env_centered_id, pop_centered_id, 47), ] 
centers$type <- c('Environmental', 'Population', 'Geographic')

map +
  geom_point(
    data = df, 
    aes(x = lon, y = lat)
    ) + 
  geom_point(
    data = centers,  
    aes(x = lon, y = lat),
    col = '#FF1493', size = 4
    ) + 
  ggrepel::geom_label_repel(
    data = centers, 
    aes(label = type, x = lon, y = lat)
    ) + 
  theme_minimal() + 
  labs(title = 'Possbilities for required centers')

rm(env_centered_id, env_centered, pop_centered_id)
```

Using these three centers will give results as below 

```{r run multi required sites and plot}
test_data$sites$required[centers$site_id] <- TRUE

env_res <- maximizeDispersion(  ## reduce some parameters for faster run. 
  input_data = test_data,
  n_sites = 10,
  lambda_var = 0.1,
  weight_1 = 0.3, 
  weight_2 = 0.7,
  n_bootstrap = 99, 
  objective = "sum",
  n_local_search_iter = 50,
  n_restarts = 2
)

map + 
  geom_point(data = env_res$input_data, 
    aes(
      x = lon, 
      y = lat, 
      shape = required, 
      size = cooccur_strength,
      color = selected
      )
    ) +
  theme_minimal() + 
  labs(title = 'Priority Selection Status of Sites with 3 required sites')

rm(centers)
```

## show differences in lambda parameters

`maximizeDispersion` optimizes on two parameters. 
The first if seeking to maximize the distance between selected populations. 
If this optimizer runs alone ($\lambda$ = 0.0), it results in *only* drawing a set of populations along the margin of the species. 
By introducting a regularization term ($\lambda$), a balance of optimizing distance, with also decreasing the variance between points is achieved. 

Theoretically maintaining a low $\lambda$ helps maximize the geographic coverage of populations at a small sample size. 
However, we find that the best selection of populations are likely to come from a two-stage approach. 
One where a user specifies the:
  - 1) required sites, 
  - 2) runs the algorithm to indentify 'fringing' sites, which may have more allelic diversity, 
  - 3) subjectively placing a couple sites across the range as they see fit. 

However, I believe that grid-based approaches may still be the most suitable for most practicioners situations. 

```{r}
lambda_viewer <- function(lambda_val){

  env_res <- maximizeDispersion(   
    input_data = test_data,
    n_sites = 10,
    lambda_var = lambda_val,
    n_bootstrap = 50, 
    objective = "sum",
    n_local_search_iter = 50,
    n_restarts = 2, 
    verbose = FALSE
  )

  m <- map + 
    geom_point(data = env_res$input_data, 
      aes(
        x = lon, 
        y = lat, 
        shape = required, 
        size = cooccur_strength,
        color = selected
        )
      ) +
    labs(title = paste('lambda: ', lambda_val), x = NULL, y = NULL)

}

lambda_view <- lapply(c(0, 0.025, 0.05, 0.075, 0.1, 0.15), lambda_viewer)

lambda_view[[1]] + lambda_view[[2]] + lambda_view[[3]] + 
  lambda_view[[4]]  +  lambda_view[[5]] + lambda_view[[6]] + 
  plot_layout(ncol = 3)

rm(lambda_view, lambda_viewer, map)
```

## closing thoughts

This approach is highly experimental. 
Small permutations in lambda may lead to large differences in the nature of results. 
Further, results are often 'ring like', a consequence of both maximizing distance between sites while minimizing variance. 