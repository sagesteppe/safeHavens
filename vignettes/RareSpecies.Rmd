---
title: "Rare Species Sampling Schema"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Rare Species Sampling Schema}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
description: >
  Running an additional sampling scheme for rare species. 
---

## prepare data
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Load the required packages. 

```{r required packages, message=FALSE}
#remotes::install_github('sagesteppe/safeHavens')
library(safeHavens)
library(ggplot2)
library(patchwork)
set.seed(99)
```

Here we will use the Bradypus data included in the `dismo` package again. 

```{r import bradypus data for testing}
x <- read.csv(file.path(system.file(package="dismo"), 'ex', 'bradypus.csv'))
x <- x[,c('lon', 'lat')]
x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)
```

And we will create the same base map used in `GettingStarted`. 

```{r create map for visualizing results, echo = F}
planar_proj <- '+proj=laea +lon_0=-421.171875 +lat_0=-16.8672134 +datum=WGS84 +units=m +no_defs'

americas <- spData::world

x <- sf::st_as_sf(x, coords = c('lon', 'lat'), crs = 4326)
x_buff <- sf::st_transform(x, planar_proj) |>
  sf::st_buffer(125000) |> 
  sf::st_as_sfc() |> 
  sf::st_union() |>
  sf::st_transform(4326) |>
  sf::st_buffer(100000) |>
  sf::st_bbox()

americas <- spData::world 
americas <- sf::st_crop(americas, x_buff) |>
  dplyr::select(name_long)

bb <- sf::st_bbox(x_buff)

map <- ggplot() + 
  geom_sf(data = americas) + 
  theme(
    legend.position = 'none', 
    panel.background = element_rect(fill = "aliceblue"), 
    panel.grid.minor.x = element_line(colour = "red", linetype = 3, linewidth  = 0.5), 
    axis.ticks=element_blank(),
    axis.text=element_blank(),
    plot.background=element_rect(colour="steelblue"),
    plot.margin=grid::unit(c(0,0,0,0),"cm"),
    axis.ticks.length = unit(0, "pt"))+ 
  coord_sf(xlim = c(bb[1], bb[3]), ylim = c(bb[2], bb[4]), expand = FALSE)

rm(americas, bb, planar_proj, x_buff)
```

While all other functions in the package handle `sf` objects directly, this function will actually just use a simple data frame of sites, to simplfy handing data off to C++ for optimization routines. 

The input to the `maximizeDispersion` function is a list with two elements: a distance matrix, and a data frame of site locations and attributes. 
The data frame must contain the following columns. 

```{r prep bradypus for sampling}
n_sites <- nrow(x) 
df <- data.frame(
  site_id = seq_len(n_sites),
  required = FALSE,
  coord_uncertainty = 0, 
  lon = sf::st_coordinates(x)[,1], 
  lat = sf::st_coordinates(x)[,2]
)

head(df)
```

The second required element, the distance matrix, can be calculated with the `greatCircleDistance` function in the package. 
Please use this rather than st_distance from `sf` for consistency, as the units differ slightly. 
If you want to use `sf::st_distance`, make sure to convert the units to match the scale of the `greatCircleDistance` function, otherwise the results will be incorrect.

```{r calculate distance matrix}
dist_mat <- sapply(1:nrow(df), function(i) {
   greatCircleDistance(
     df$lat[i], df$lon[i],
     df$lat, df$lon
   )
 })
```

The optimization routine requires at least one 'required' site to be specified. 
Here we will select the site closest to the geographic center of all sites as the required site.

Normally this can refer to existing accessions, or administrative units, or preserves which are helping to implement the germplasm collection, and are fortunate enough to already have some samples or at least guaranteed access.

```{r required points}
dists2c <- greatCircleDistance(
  median(df$lat), 
  median(df$lon), 
  df$lat, 
  df$lon
)
df[order(dists2c)[1],'required'] <- TRUE
```

This function not only bootstraps sites to simulate the true distribution distribution of the species, but it also bootstraps coordinate uncertainty for each site. 
Here we will randomly assign 20% of the sites to have coordinate uncertainty between 1 km and 40 km. 
Note that he argument is always in meters. 

```{r simulating coordinate uncertainty}
uncertain_sites <- sample(
  setdiff(seq_len(n_sites), 
  which(df$required)), 
  size = round(n_sites*0.2, 0)
  )
df$coord_uncertainty[uncertain_sites] <- runif(length(uncertain_sites), 1000, 40000) # meters
```

## Run KMedoidsBasedSample based only on geographic distances

The input to the function is the distance matrix, and the site data. 

```{r combine the input data}
test_data <- list(
  distances = dist_mat,
  sites = df
  )

str(test_data)

rm(x, n_sites, uncertain_sites, dists2c)
```

The funtion `KMedoidsBasedSample` has several parameters to control run parameters.

```{r run with geograpihc distances}
st <- system.time( {
    geo_res <- KMedoidsBasedSample(  ## reduce some parameters for faster run. 
      input_data = test_data,
      n = 5,
      n_bootstrap = 10,
      dropout_prob = 0.1,
      n_local_search_iter = 10,
      n_restarts = 2
    )
  }
)
```

The function operates relatively quick with few bootstraps and few sites, but will take a smidge of time longer  with more complex scenarios.
We recommened using at least 999 bootstraps for real world applications. 

```{r}
st
rm(st)
```


### return output structure
Various elements are returned in the output list.
```{r structure of output}
str(geo_res)
```

The stability score shows how often the most frquently selected network of sites was selected from the bootstrapped runs. 
```{r}
head(geo_res$stability_score)
```

The stability data frame shows how often each site was selected across all bootstrap runs. 
```{r}
head(geo_res$stability)
```

Many users may find the combindation of their input data with a few columns, to be all they need to carry on after the results. 
```{r}
head(geo_res$input_data)
```

Run parameters are saved in the settings element.
```{r}
head(geo_res$settings)
```

### visualize the selection results

```{r first selection of target sites}
map + 
  geom_point(data = geo_res$input_data, 
  aes(
    x = lon, 
    y = lat, 
    shape = required, 
    size = cooccur_strength,
    color = selected
    )
  ) +
 # ggrepel::geom_label_repel(aes(label = site_id), size = 4) + 
  theme_minimal() + 
  labs(title = 'Priority Selection Status of Sites; Geographic Distances')
```


```{r priority ranking plot}
map + 
  geom_point(data = geo_res$input_data, 
    aes(
      x = lon, 
      y = lat, 
      shape = required, 
      size = -sample_rank,
      color = sample_rank
      )
    ) +
 # ggrepel::geom_label_repel(aes(label = sample_rank), size = 4) +
  theme_minimal()   
```


## run KMedoidsBasedSample with environmental distances

### extract prep environmental distances 

```{r prep environmental distance matrix}
files <- list.files(
  path = file.path(system.file(package="dismo"), 'ex'), 
  pattern = 'grd',  full.names=TRUE )
predictors <- terra::rast(files) # import the independent variables
rm(files)
```

For our environmental distances, we will use a PCA transformation of the environmental variables.
We will simply scrape 100 random points from the raster layers to calculate the PCA.
Then predict the PCA raster layers across the entire study area.
We will take the first two layers, and calculate environmental distances based on these two layers.

```{r run with geographic and environmental distances}
pts <- terra::spatSample(predictors, 100, na.rm = TRUE)
pts <- pts[, names(pts)!='biome' ] # remove categorical variable for distance calc

pca_results <- stats::prcomp(pts, scale = TRUE)
round(pca_results$sdev^2 / sum(pca_results$sdev^2), 2) # variance explained
pca_raster <- terra::predict(predictors, pca_results)

terra::plot(terra::subset(pca_raster, c(1:2))) # prediction of the pca onto a new raster
rm(pts, predictors, pca_results)
```

we keep the first two PCA layers for environmental distance calculation.
More layers will increase dimenstionality, and may lead to less useful results.
Note that it's fine to use a euclidean distance calculation for these, as the values are truly in the position of the plot. 

```{r extract environmental values and calculate distance matrix}
env_values <- terra::extract(pca_raster, 
  sf::st_coordinates(
    sf::st_as_sf(
      df, 
      coords = c('lon', 'lat'), 
      crs = 4326
    )
  )
)[,1:2]
plot(env_values, main = 'environmental distance of points from first two PCA axis')

env_dist_mat <- as.matrix(
    dist(env_values)
  )

rm(pca_raster)
```

```{r run with environmental distance}
test_data <- list(
  distances = env_dist_mat,
  sites = df
  )

st <- system.time( 
  {
    env_res <- KMedoidsBasedSample(  ## reduce some parameters for shorter run time.
      input_data = test_data,
      n = 5,
      n_bootstrap = 10,
      dropout_prob = 0.1,
      n_local_search_iter = 50,
      n_restarts = 2
    )
  }
)

rm(dist_mat, env_dist_mat)
```

```{r}
st
rm(st)
```

```{r}
head(env_res$stability_score)
```

```{r }
map + 
  geom_point(data = env_res$input_data, 
    aes(
      x = lon, 
      y = lat, 
      shape = required, 
      size = cooccur_strength,
      color = selected
      )
    ) +
 # ggrepel::geom_label_repel(aes(label = site_id), size = 4) + 
  theme_minimal() + 
  labs(title = 'Priority Selection Status of Sites; Environmental')
```

## alternative methods for required central points

In the example above we use a point at the median geographic center of the populations. 

We can also identify the population which is *most* near the highest density of populations. 
Intuitively, this would be suggested as a population with a very high genetic diversity.  

```{r required point method - population centroid - heat map}
dens <- with(df, MASS::kde2d(lon, lat, n = 200))
max_idx <- which(dens$z == max(dens$z), arr.ind = TRUE)[1,]
max_point <- c(dens$x[max_idx[1]], dens$y[max_idx[2]])

pops_centre <- sweep(df[c('lon', 'lat')], 2, max_point, "-")
pop_centered_id <- which.min(rowSums(abs(pops_centre^2)))

rm(dens, max_idx, max_point, pops_centre)
```

Likewise we can identify the *population* which is most near the 'center' of the environmental variable space. 

```{r required point method - environmental centroid}
env_centered <- sweep(env_values, 2, sapply(env_values, median), "-")
env_centered_id <- which.min(rowSums(abs(env_centered^2)))

rm(env_values)
```

Personally I would consider the 'pop centered' population to be the most important required site to center a design off of. 

```{r plot alternative centroids}
# geographic centroid was pt 47
centers <- df[ c(env_centered_id, pop_centered_id, 47), ] 
centers$type <- c('Environmental', 'Population', 'Geographic')

map +
  geom_point(
    data = df, 
    aes(x = lon, y = lat)
    ) + 
  geom_point(
    data = centers,  
    aes(x = lon, y = lat),
    col = '#FF1493', size = 4
    ) + 
  ggrepel::geom_label_repel(
    data = centers, 
    aes(label = type, x = lon, y = lat)
    ) + 
  theme_minimal() + 
  labs(title = 'Possbilities for centers')

rm(env_centered_id, env_centered, pop_centered_id)
```

