---
title: "Predictive Provenance"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Predictive Provenance}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
description: >
  Predictive Provenancing using Species Distribution Models.
---

## Overview
We have already shown that the beta-coefficients from species distribution models can be used in hierarchical clustering to group a species populations into clusters that experience more similar - and relevant to their ecology - climate. 
While running `EnvironmentalBasedSample` we focus on also optimizing the species occupied range using moran eigenvector matrix surfaces (MEMs, or PCNM). 
In this alternative approach we drop the PCNMs from the SDMs and focus solely on environmental predictors, and work to identify clusters in **current** environmental space, and then classify **future** environmental (and geographic) space with these cluster parameters. 
This gives us sets of areas with populations that may be more pre-adapted to future conditions. 

In addition to transferring the realized environmental clusters of populations forward, we also identify novel climate spaces, using MESS (Elith 2010), and hierarchical clustering. 
This results in us having *n* identified clusters, and we can traverse the clustering branches to determine which existing clusters are *most* similar to novel clusters. 

This is the sole workflow in `safeHavens` that seeks to link current climates to future scenarios, and directly suggest how the size of clusters may change, where they will shift to, and allow a germplasm curator to identify relevant seed sources for predictive provenancing. 
The approach employed here is adequate to guide *current* collections, however decisions on what sources to use at a particular restoration site are relatively well identified by tools such as the *Seed Lot Selection Tool* in North America, and COSST in South America. 

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = ""
)
```

## Analysis 

### Data prep

This workflow will require the `geodata` package; `geodata` is used for retrieving climate data from a variety of sources as raster data. 
In this vignette we will use [Worldclim](https://www.worldclim.org/). 
For most `safeHavens` users around the world this may be your ideal source for climate data. 
`geodata` is developed and maintained by Robert Hijman, similar to `terra`, and `dismo`, which are doing a ton of heavy lifting internally on our functions. 

```{r attach packages}
library(safeHavens)
library(terra)
library(geodata)
library(sf)
library(dplyr)
library(tidyr)
library(ggplot2)
```

For this example we will leverage data from GBIF, and shift our geographic focus to the Colorado Plateau in the Southwestern USA. 
The Colorado Plateau is predicted to experience some of the highest rates of climate change on the planet, and is already an area with considerably native seed development activities. 
For our focal species we will use *Helianthella microcephala* for no other reason than that it exists in relatively homogenous portions of the Colorado Plateau - meaning we can download relatively coarse rasters for the vignettes, and that it has a *stunning* inflorescence.  

```{r Download species occurrence data}
cols = c('decimalLatitude', 'decimalLongitude', 'dateIdentified', 'species', 'acceptedScientificName', 'datasetName', 
  'coordinateUncertaintyInMeters', 'basisOfRecord', 'institutionCode', 'catalogNumber')

## download species data using scientificName, can use keys and lookup tables for automating many taxa. 
hemi <- rgbif::occ_search(scientificName = "Helianthella microcephala")
hemi <- hemi[['data']][,cols]  |>
  drop_na(decimalLatitude, decimalLongitude) |> # any missing coords need dropped. 
  distinct(decimalLatitude, decimalLongitude, .keep_all = TRUE) |> # no dupes can be present
  st_as_sf(coords = c( 'decimalLongitude', 'decimalLatitude'), crs = 4326, remove = F) 

western_states <- spData::us_states |> ## for making a quick basemap. 
  dplyr::filter(REGION == 'West' | NAME %in% c('Texas', 'Oklahoma', 'Kansas', 'Nebraska', 'South Dakota')) |>
  dplyr::select(NAME, geometry) |>
  st_transform(4326)

wsub <- dplyr::filter(western_states, # skip Colorado to try and bring clip in. 
    NAME %in% c('Arizona', 'Utah', 'Wyoming', 'Nevada')) 
bbox_ss <- st_bbox(wsub)

bmap <- ggplot() + 
    geom_sf(data = western_states) + 
    geom_sf(data = hemi) +
    theme_minimal() +
    coord_sf(
        xlim = c(bbox_ss[['xmin']], bbox_ss[['xmax']]), 
        ylim = c(bbox_ss[['ymin']], bbox_ss[['ymax']])
        ) 

bmap +
    labs(title = 'Helianthella microcephala occurrence records')
```

```{r remove cols and western states, echo = F}
rm(cols, western_states)
```

We will download worldclim data using `geodata` the results from `worldclim_global` should be what we loaded from `dismo` in the various sloth examples.  
For our future scenario we will use the [CMIP6](https://wcrp-cmip.org/cmip-phases/cmip6/) modelled future climate data for the 2041-2060 time window. 
We will download the coarse 2.5 minute, ca ~70km at equator, resolution data. 
Do note that the source also includes a ~ 1km at equator resolution data set (30 seconds), but this would take too long for the vignette. 

```{r download data}
# Download WorldClim bioclim at ~10 km
bio_current <- worldclim_global(var="bioc", res=2.5)
bio_future <- cmip6_world(
  model = "CNRM-CM6-1", ## modelling method
  ssp   = "245", ## "Middle of the Road" scenario
  time  = "2041-2060", # time period
  var   = "bioc", # just use the bioclim variables
  res   = 2.5
)

# Crop to domain - use a large BB to accomodate range shift
# under future time points. 
bbox <- ext(vect(wsub))

bio_current <- crop(bio_current, bbox)
bio_future <- crop(bio_future, bbox)
```

`safeHavens` does not offer explicit functionality to rename / align the naming of raster surfaces. 
However, the modelling process requires that both current and future raster products have perfectly matching raster names. 
The chunk below shows how we will standardize the names by extracting the bioclim variable number, at the end of the name, and pad 'bio_' back onto the front of it with a leading zero so we have: 'bio_01' instead of 'bio_1'. 
A minor variation of this should work for most data sources, after customizing the `gsub` to erase earlier portions of the file name. 

```{r standardize raster layer names}
simplify_names <- function(x){
    paste0('bio_', sprintf("%02d", as.numeric(gsub('\\D+','', names(x)))))
}

names(bio_current) <- gsub('^.*5m_', '', names(bio_current))
names(bio_future) <- gsub('^.*2060_', '', names(bio_future))

names(bio_current) <- simplify_names(bio_current)
names(bio_future) <- simplify_names(bio_future)

# TRUE means all names match, and are in the same position. 
all(names(bio_current) == names(bio_future))
```

If you are following the vignette along locally and decided to `plot(bio_current)` and `plot(bio_future)` you would find that they look very similar. 
While the plots look very similar, when showing one raster stack after another, we can diff the two products to see where the largest changes in geographic space will occur. 

```{r diff the raster layers to show areas of difference}
difference <- abs(bio_current - bio_future)
plot(difference)
```

We see that variables have inconsistences in *where* they will change, and to what extent they will change. 
This mismatch of conditions will almost certainly lead to *novel* climate conditions - unique combinations not yet known to this species. 
This will be handled using MESS surfaces and a second stage clustering approach in the function. 

```{r remove diff and simplify_names, echo = F}
rm(difference, simplify_names)
```

### Analytical workstream

The workflow for predictive provenance is quite similar to the `EnvironmentalBasedSample` workflow, relying on many of the same internal helpers, and user facing functions. 
Note that when using `elasticSDM` we need to specify the flag `pcnm = FALSE` to bypass fitting a model with PCNM surfaces - their is no analog for these under future scenarios so they must be dropped. 

### Fit SDM and rescale raster surfaces 
The starting point for this analysis is again fitting a quick SDM for our species. 

```{r fit elasticSDM for species}
## note we subset to just the geometries for the prediction records. 
# this is because we feed in all columns to the elastic net model internally. 
hemi <- select(hemi, geometry) 

# as always verify our coordinate reference systems (crs) match. 
st_crs(bio_current) == st_crs(hemi) 

# and fit the elasticnet model 
eSDM_model <- elasticSDM(
    x = hemi, 
    predictors = bio_current, 
    planar_projection = 5070, 
    PCNM = FALSE ## set to FALSE for this workstream!!!!!
    )
```

Using the beta-coefficients from the model we will rescale both the current and future climate scenarios rasters.  

```{r rescale rasters}
bio_current_rs <- RescaleRasters(
    model = eSDM_model$Model, 
    predictors = eSDM_model$Predictors,
    training_data = eSDM_model$Train,
    pred_mat = eSDM_model$PredictMatrix
    )

plot(bio_current_rs$RescaledPredictors)

bio_future_rs <- rescaleFuture(
  eSDM_model$Model, 
  bio_future, 
  eSDM_model$Predictors,
  training_data = eSDM_model$Train,
  pred_mat = eSDM_model$PredictMatrix
)

bio_future_rs
```

Similar to how we differenced the rasters at the two time poitns above, we can take the absolute difference of the relevant raster layers to see where the largest changes will be. 

```{r rescaled raster difference }
difference_rs <- abs(bio_current_rs$RescaledPredictors - bio_future_rs)
plot(difference_rs)
```

```{r removed rs differnce surfaces, echo = F}
rm(difference_rs)
```

### Cluster surfaces

Clusters can be identified using NbClust, allowing it to determine the optimal *n* using the METHOD. 
To use NbClust, rather than manually specifying *n*, the argument `fixedClusters` needs to be set to FALSE. 

```{r identify current clusters}
threshold_rasts <- PostProcessSDM(
  rast_cont = eSDM_model$RasterPredictions, 
  test = eSDM_model$TestData,
  train = eSDM_model$TrainData,
  planar_proj = 5070,
  thresh_metric = 'equal_sens_spec', 
  quant_amt = 0.25
  )

ENVIbs <- EnvironmentalBasedSample(
  pred_rescale = bio_current_rs$RescaledPredictors, 
  write2disk = FALSE, # we are not writing, but showing how to provide some arguments
  f_rasts = threshold_rasts$FinalRasters, 
  coord_wt = 0.001, 
  fixedClusters = FALSE,
  lyr = 'Supplemented',
  n_pts = 500, 
  planar_proj = "epsg:5070",
  buffer_d = 3,
  prop_split = 0.8,
  min.nc = 5, 
  max.nc = 15
  )
plot(threshold_rasts$FinalRasters['Predictions'])

plot(ENVIbs$Geometry)
```

We can apply the current clustering to the future scenario. 
This will show us how and where the currently identified clusters would exist in future geographic space. 


```{r classify future climate surface with clusters}
obby <- projectClusters(
  current_model = eSDM_model$Model, 
  current_clusters = ENVIbs,
  future_predictors = bio_future,
  current_predictors = bio_current,
  test_points = eSDM_model$TestData,
  train_points = eSDM_model$TrainData,
  thresholds = threshold_rasts$Threshold, 
  predict_matrix = eSDM_model$PredictMatrix,
  planar_proj = "epsg:5070", 
  thresh_metric = 'equal_sens_spec'
)

names(obby)

### need to clipt clusters sf and raster to suitable_habitat
plot(obby$suitable_habitat)
plot(obby$clusters_sf)
plot(obby$novel_mask)

plot(obby$mess)
obby$changes
```

However, our classifications cannot accomodate new climate conditions. 
We can identify these areas, which are outside of the training conditions for our SDM using MESS. 
Note that in these areas our SDM *is* extrapolating, so it's performance cannot be evaluated. 
However, it seems that these will possibly be suitable habitat, and we can try to plan for that scenario. 

Once we identify these areas, if they are sufficiently large we can pass them to a new NbClust scenario and it can cluster them anew. 

```{r identify additional clusters under future climate}

```

However, we also need to determine which current areas are most similar to these novel climate clusters. 
We can take values from both classifcation scenarios, and cluster them, and identify which new climate clusters share branches with existing clusters. 
This is the method we employ to identify the most similar existing climate groups. 

```{r determine analog current climate clusters for the novel groups}

```

It is from these groups that we are most likely to collect germplasm relevant for the future scenarios. 

### saving data

There is also a helper function for writing out the results of these analyses to save them to disk. 

```{r write out results}

```

## Conclusion 

All functions in `safeHavens` work to identify areas where populations have the potential to maximize the coverage of neutral and allelic (genetic) diversity across the species. 
Large scale restoration requires the availability of seed sources as the need, and funding opportunities arise to use them. 
Developping germplasm materials from populations that seem adapted to future climate conditions is essential for future opportunities. 
While this function does not in any way seek to identify the best available seed source for a restoration, ala SST or COSST, it does seek to ensure that the best available option can be timely applied as the occasion arises. 

This is of particular importance for areas that are not able to plan restorations and develop their own seed sources using point-based analyses. 

```{r clean up environment, echo = F}

```