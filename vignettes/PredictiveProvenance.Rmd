---
title: "Predictive Provenance"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Predictive Provenance}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
description: >
  Predictive Provenancing using Species Distribution Models.
---

## Overview
We have already shown that the beta-coefficients from species distribution models can be used in hierarchical clustering to group a species populations into clusters that experience more similar - and relevant to their ecology - climate. 
While running `EnvironmentalBasedSample` we focus on also optimizing the species occupied range using moran eigenvector matrix surfaces (MEMs, or PCNM). 
In this alternative approach we drop the PCNMs from the SDMs and focus solely on environmental predictors, and work to identify clusters in **current** environmental space, and then classify **future** environmental (and geographic) space with these cluster parameters. 
This gives us sets of areas with populations that may be more pre-adapted to future conditions. 

In addition to transferring the realized environmental clusters of populations forward, we also identify novel climate spaces, using MESS (Elith 2010), and hierarchical clustering. 
This results in us having *n* identified clusters, and we can traverse the clustering branches to determine which existing clusters are *most* similar to novel clusters. 

This is the sole workflow in `safeHavens` that seeks to link current climates to future scenarios, and directly suggest how the size of clusters may change, where they will shift to, and allow a germplasm curator to identify relevant seed sources for predictive provenancing. 
The approach employed here is adequate to guide *current* collections, however decisions on what sources to use at a particular restoration site are relatively well identified by tools such as the *Seed Lot Selection Tool* in North America, and COSST in South America. 

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = ""
)
```

## Analysis 

This workstream leans heavily on the `EnvironmentalBasedSample` workflow, and passes it's final objects off to `PolygonBasedSample` for completion. 
Please review `Species Distribution Models` and `Getting Started` before proceeding here. 

### Data prep

This workflow will require the `geodata` package; `geodata` is used for retrieving climate data from a variety of sources as raster data. 
In this vignette we will use [Worldclim](https://www.worldclim.org/). 
For most `safeHavens` users around the world this may be your ideal source for climate data. 
`geodata` is developed and maintained by Robert Hijman, similar to `terra`, and `dismo`, which are doing a ton of heavy lifting internally on our functions. 

```{r attach packages, message = F, warning = F}
library(safeHavens)
library(terra)
library(geodata)
library(sf)
library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)
set.seed(22)
```

For this example we will leverage data from GBIF, and shift our geographic focus to the Colorado Plateau in the Southwestern USA. 
The Colorado Plateau is predicted to experience some of the highest rates of climate change on the planet, and is already an area with considerably native seed development activities. 
For our focal species we will use *Helianthella microcephala* for no other reason than that it exists in relatively homogenous portions of the Colorado Plateau - meaning we can download relatively coarse rasters for the vignettes, and that it has a *stunning* inflorescence.  

```{r Download species occurrence data}
cols = c('decimalLatitude', 'decimalLongitude', 'dateIdentified', 'species', 'acceptedScientificName', 'datasetName', 
  'coordinateUncertaintyInMeters', 'basisOfRecord', 'institutionCode', 'catalogNumber')

## download species data using scientificName, can use keys and lookup tables for automating many taxa. 
hemi <- rgbif::occ_search(scientificName = "Helianthella microcephala")
hemi <- hemi[['data']][,cols]  |>
  drop_na(decimalLatitude, decimalLongitude) |> # any missing coords need dropped. 
  distinct(decimalLatitude, decimalLongitude, .keep_all = TRUE) |> # no dupes can be present
  st_as_sf(coords = c( 'decimalLongitude', 'decimalLatitude'), crs = 4326, remove = F) 

western_states <- spData::us_states |> ## for making a quick basemap. 
  dplyr::filter(NAME %in% 
    c('Utah', 'Arizona', 'Colorado', 'New Mexico', 'Wyoming', 'Nevada', 'Idaho', 'California')) |>
  dplyr::select(NAME, geometry) |>
  st_transform(4326)

bb <- st_bbox(
  c(
    xmin = -116, 
    xmax = -105, 
    ymax = 44, 
    ymin = 33.5),
    crs = st_crs(4326)
    )

western_states <- st_crop(western_states, bb)

bmap <- ggplot() + 
    geom_sf(data = western_states) + 
    geom_sf(data = hemi) +
    theme_minimal() +
    coord_sf(
        xlim = c(bb[['xmin']], bb[['xmax']]), 
        ylim = c(bb[['ymin']], bb[['ymax']])
        )  +
    theme(
      axis.title.x=element_blank(),
      axis.text.x=element_blank(),
      axis.ticks.x=element_blank()
      )

bmap +
    labs(title = 'Helianthella microcephala\noccurrence records')
```

```{r remove cols and western states, echo = F}
rm(cols, western_states)
```

We will download worldclim data using `geodata` the results from `worldclim_global` should be what we loaded from `dismo` in the various sloth examples.  
For our future scenario we will use the [CMIP6](https://wcrp-cmip.org/cmip-phases/cmip6/) modelled future climate data for the 2041-2060 time window. 
We will download the coarse 2.5 minute, ca ~70km at equator, resolution data. 
Do note that the source also includes a ~ 1km at equator resolution data set (30 seconds), but this would take too long for the vignette. 

```{r download data}
# Download WorldClim bioclim at ~10 km
bio_current <- worldclim_global(var="bioc", res=2.5)
bio_future <- cmip6_world(
  model = "CNRM-CM6-1", ## modelling method
  ssp   = "245", ## "Middle of the Road" scenario
  time  = "2041-2060", # time period
  var   = "bioc", # just use the bioclim variables
  res   = 2.5
)

# Crop to domain - use a large BB to accomodate range shift
# under future time points. 
# but going too large will dilute the absence records
bbox <- ext(bb)

bio_current <- crop(bio_current, bbox)
bio_future <- crop(bio_future, bbox)
```

`safeHavens` does not offer explicit functionality to rename / align the naming of raster surfaces. 
However, the modelling process requires that both current and future raster products have perfectly matching raster names. 

The chunk below shows how we will standardize the names by extracting the bioclim variable number, at the end of the name, and pad 'bio_' back onto the front of it with a leading zero so we have: 'bio_01' instead of 'bio_1'. 
A minor variation of this should work for most data sources, after customizing the `gsub` to erase earlier portions of the file name. 

```{r standardize raster layer names}
simplify_names <- function(x){
    paste0('bio_', sprintf("%02d", as.numeric(gsub('\\D+','', names(x)))))
}

names(bio_current) <- gsub('^.*5m_', '', names(bio_current))
names(bio_future) <- gsub('^.*2060_', '', names(bio_future))

names(bio_current) <- simplify_names(bio_current)
names(bio_future) <- simplify_names(bio_future)

# TRUE means all names match, and are in the same position. 
all(names(bio_current) == names(bio_future))

## we will also drop some variables that are essentially identical in the study area
drops <- c(
  'bio_05' , 'bio_02',
  'bio_11', 'bio_12'
  ) 

bio_current <- subset(bio_current, negate = TRUE, drops)
bio_future <- subset(bio_future, negate = TRUE, drops)
```

If you are following the vignette along locally and decided to `plot(bio_current)` and `plot(bio_future)` you would find that they look very similar. 
While the plots look very similar, when showing one raster stack after another, we can diff the two products to see where the largest changes in geographic space will occur. 

```{r diff the raster layers to show areas of difference}
pct_diff <- function(x, y){((x - y)/((x + y)/2)) * 100}
difference <- pct_diff(bio_current, bio_future)
plot(difference)
```

We see that variables have inconsistences in *where* they will change, and to what extent they will change. 
This mismatch of conditions will almost certainly lead to *novel* climate conditions - unique combinations not yet known to this species. 
This will be handled using MESS surfaces and a second stage clustering approach in the function. 

```{r remove diff and simplify_names, echo = F}
rm(difference, simplify_names, drops)
```

### Analytical workstream

The workflow for predictive provenance builds upon the `EnvironmentalBasedSample` workflow, relying on many of the same internal helpers, and user facing functions. 
Note that when using `elasticSDM` we need to specify the flag `pcnm = FALSE` to bypass fitting a model with PCNM surfaces - their is no analog for these under future scenarios so they must be dropped. 

### Fit SDM and rescale raster surfaces 
The starting point for this analysis is again fitting a quick SDM for our species. 

```{r fit elasticSDM for species,message = FALSE}
## note we subset to just the geometries for the prediction records. 
# this is because we feed in all columns to the elastic net model internally. 
hemi <- select(hemi, geometry) 

# as always verify our coordinate reference systems (crs) match. 
st_crs(bio_current) == st_crs(hemi) 

# and fit the elasticnet model 
eSDM_model <- elasticSDM(
    x = hemi, 
    predictors = bio_current, 
    planar_projection = 5070, 
    PCNM = FALSE ## set to FALSE for this workstream!!!!!
    )

# Test with just 3 runs to see variability quickly
run1 <- elasticSDM(hemi, bio_current, 5070)
run2 <- elasticSDM(hemi, bio_current, 5070)
run3 <- elasticSDM(hemi, bio_current, 5070)

# Compare coefficients
coef1 <- as.matrix(coef(run1$Model))
coef2 <- as.matrix(coef(run2$Model))
coef3 <- as.matrix(coef(run3$Model))

# Quick comparison
all_vars <- unique(c(rownames(coef1), rownames(coef2), rownames(coef3)))
compare <- data.frame(
  var = all_vars,
  run1 = coef1[match(all_vars, rownames(coef1)), 1],
  run2 = coef2[match(all_vars, rownames(coef2)), 1],
  run3 = coef3[match(all_vars, rownames(coef3)), 1]
)
compare[is.na(compare)] <- 0
compare$cv <- apply(compare[,-1], 1, function(x) sd(x)/mean(x))
print(compare)
```

We will use the eSDM_model function for this workstream, however is is essential that PCNM is set to FALSE. 
There is no way to forecast of predict PCNM to future scenarios, so we cannot fit models with it. 

If a warning is emitted here it is likely due to collinearity in the bioclim variables. 
Rather than using `eDist` from sdm for generating background points we will use the `eRandom` method instead. 

```{r current sdm threshold results}
eSDM_model$ConfusionMatrix$byClass[
  c('Sensitivity', 'Specificity', 'Recall', 'Balanced Accuracy')]
plot(eSDM_model$RasterPredictions)
```

We can view some of the results for the model, it is OK, but would benefit from using only a subset of the bioclim predictors to allow for `eDist` sampling upstream. 
It is a bit too generous with classifying areas as being possible suitable habitat. 
While this model would be unsuitable for applications, certain CRAN and Github checks make it difficult to improve upon as an example. 

Using the beta-coefficients from the model we will rescale both the current and future climate scenarios rasters. 
This will happen internally in the next function, but it is good to see the different between the current and future scenarios to have an idea of expectations for the final results. 

```{r rescale rasters}
bio_current_rs <- RescaleRasters(
    model = eSDM_model$Model, 
    predictors = eSDM_model$Predictors,
    training_data = eSDM_model$Train,
    pred_mat = eSDM_model$PredictMatrix
    )

plot(bio_current_rs$RescaledPredictors)

bio_future_rs <- rescaleFuture(
  eSDM_model$Model, 
  bio_future, 
  eSDM_model$Predictors,
  training_data = eSDM_model$Train,
  pred_mat = eSDM_model$PredictMatrix
)

plot(bio_future_rs)
```

If any of the layers show as a single color, and 0, it means that glmnet shrunk them from the model. 

Similar to how we differenced the rasters at the two time points above, we can take the absolute difference of the relevant raster layers to see where the largest changes will be. 

```{r rescaled raster difference }
difference_rs <- pct_diff(bio_current_rs$RescaledPredictors, bio_future_rs)
plot(difference_rs)
```

The above plot shows us the areas with the largest changes between current and predicted conditions. 

```{r remove rs differnce surfaces, echo = F}
rm(difference_rs, bio_future_rs)
```

### Cluster surfaces

Clusters can be identified using NbClust, allowing it to determine the optimal *n* using the METHOD. 
To use NbClust, rather than manually specifying *n*, the argument `fixedClusters` needs to be set to `FALSE`. 
When using PostProcessSDM the same threshhold metric should be used that will be applied to `PredictiveProvenance`. 

```{r identify current clusters, message = FALSE, warning = FALSE}
threshold_rasts <- PostProcessSDM(
  rast_cont = eSDM_model$RasterPredictions, 
  test = eSDM_model$TestData,
  train = eSDM_model$TrainData,
  planar_proj = 5070,
  thresh_metric =
     'equal_sens_spec', 
  quant_amt = 0.25
  )

plot(threshold_rasts$FinalRasters)
threshold_rasts$Threshold$equal_sens_spec

bmap + 
  geom_sf(data = 
  sf::st_as_sf(
    terra::as.polygons(
      threshold_rasts$FinalRasters['Threshold'])
      ), fill = 'cornsilk'
    ) + 
    geom_sf(data = hemi) 

```

We will make the EnvironmentalBasedSample to find a suitable number of climate clusters at the current time period. 
When using EBS for predictive provencing also set the coord_wt parameter to a small value. 
This will remove the effect of the spatial clustering. 
Similar to PCNM, we cannot know the distribution of actual populations in the future.

```{r EnvironmentalBasedSample, message=F, warning = F}
ENVIbs <- EnvironmentalBasedSample(
  pred_rescale = bio_current_rs$RescaledPredictors, 
  write2disk = FALSE, # we are not writing, but showing how to provide some arguments
  f_rasts = threshold_rasts$FinalRasters, 
  coord_wt = 0.001, 
  fixedClusters = FALSE,
  lyr = 'Threshold',
  n_pts = 500, 
  planar_proj = "epsg:5070",
  buffer_d = 3,
  prop_split = 0.8,
  min.nc = 5, 
  max.nc = 15
  )

bmap + 
  geom_sf(data = ENVIbs$Geometry, aes(fill = factor(ID))) + 
  geom_sf(data = hemi) + 
  theme(legend.position = 'bottom') + 
  labs(fill = 'Cluster', title = 'Current')
```

We can apply the current clustering to the future scenario. 
This will show us how and where the currently identified clusters would exist in future geographic space. 
`ProjectClusters` is really the whole heart of this workflow, it requires the outputs from `elasticSDM`, `PostProcessSDM`, and `EnvironmentalBasedSample`. 

```{r classify future climate surface with clusters, warning = FALSE, message = FALSE}
future_clusts <- projectClusters(
  eSDM_object = eSDM_model, 
  current_clusters = ENVIbs,
  future_predictors = bio_future,
  current_predictors = bio_current,
  thresholds = threshold_rasts$Threshold, 
  planar_proj = "epsg:5070", 
  thresh_metric = 'equal_sens_spec',
  n_sample_per_cluster = 20
)
```

However, our classifications cannot accomodate new climate conditions. 
We can identify these areas, which are outside of the training conditions for our SDM using MESS. 
Note that in these areas our SDM *is* extrapolating, so it's performance cannot be evaluated. 
However, it seems that these will possibly be suitable habitat, and we can try to plan for that scenario. 

```{r plot the mess surfaces}
plot(future_clusts$mess)
bmap +
  geom_sf(data = 
    st_as_sf(terra::as.polygons(future_clusts$novel_mask)), 
    fill = 'red') + 
  labs(title = 'MESS regions')
```

Once we identify these areas, if they are sufficiently large we can pass them to a new NbClust scenario and it can cluster them anew. 

```{r identify additional clusters under future climate, message=F,warning=F}
current = bmap + 
  geom_sf(data = ENVIbs$Geometry, aes(fill = factor(ID))) +
  labs(title = 'Current', fill = 'Cluster') +
  theme(legend.position = "none")

future = bmap + 
  geom_sf(data = future_clusts$clusters_sf, aes(fill = factor(ID))) + 
  labs(title = '2041-2060', fill = 'Cluster') 

current + future
```

However, we also need to determine which current areas are most similar to these novel climate clusters. 
We can take values from both classifcation scenarios, and cluster them, and identify which new climate clusters share branches with existing clusters. 
This is the method we employ to identify the most similar existing climate groups. 

```{r determine analog current climate clusters for the novel groups}
future_clusts$novel_similarity
```

It is from these groups that we are most likely to collect germplasm relevant for the future scenarios. 

We can also take a very quick look at how cluster sizes change between the scenarios
```{r cluster changes}
future_clusts$changes
```

## Conclusion 

All functions in `safeHavens` work to identify areas where populations have the potential to maximize the coverage of neutral and allelic (genetic) diversity across the species. 
Large scale restoration requires the availability of seed sources as the need, and funding opportunities arise to use them. 
Developping germplasm materials from populations that seem adapted to future climate conditions is essential for future opportunities. 
While this function does not in any way seek to identify the best available seed source for a restoration, ala SST or COSST, it does seek to ensure that the best available option can be timely applied as the occasion arises. 

This is of particular importance for areas that are not able to plan restorations and develop their own seed sources using point-based analyses. 

