---
title: "Bayesian Approaches"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Bayesian Approaches}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
description: >
  Using Bayesian approaches for EnvironmentalBasedSample.
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = ""
)
```

# Species Distribution Modelling

Please refer to all frequentist methods before viewing this vignette. 
The rationale is roughly similar, but this approach provides better estimates of uncertainty for the processing. 

## Background

This vignette details the steps required to create a bayesian Species Distribution Model (SDM) using the functions provided in `safeHavens`, which form a wrapper around brms::brms(). 
The SDM is then post-processed to create a binary raster map of suitable and unsuitable habitat, that is used to rescale the environmental predictor variables according to the parameter posteriors for the model. 
These parameter posteriors are then used in a clustering algorithm to partition the species range into environmentally distinct regions for germplasm sampling.

The goal of most SDM's is to create a model that accurately predicts where a species will be located in environmental space and can then be predicted in geographic space.
The goal of these models is to understand the degree and direction to which various environmental features correlate with a species observed range. 

## About 

While the authors are prone to creating many 'advanced' SDMs models and in-house pipelines for developing them, we think that the juice is not worth the squeeze and rely on a few powerhouse R packages to do the heavy lifting.
The main packages used are `dismo`, `caret`, `brms`, `CAST`, and `terra`. 

# Steps to create an SDM

### prep data 

The below steps are all present at the start of the 'Predictive Provenance' vignette. 
We will 'hide' some of the processing steps to keep this vignette entry relatively short. 

```{r load libraries, message =F, warning = F}
library(safeHavens)
library(terra)
library(geodata)
library(sf)
library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)
```


```{r Download species occurrence data, echo = FALSE}
cols = c('decimalLatitude', 'decimalLongitude', 'dateIdentified', 'species', 'acceptedScientificName', 'datasetName', 
  'coordinateUncertaintyInMeters', 'basisOfRecord', 'institutionCode', 'catalogNumber')

## download species data using scientificName, can use keys and lookup tables for automating many taxa. 
hemi <- rgbif::occ_search(scientificName = "Helianthella microcephala")
hemi <- hemi[['data']][,cols]  |>
  drop_na(decimalLatitude, decimalLongitude) |> # any missing coords need dropped. 
  distinct(decimalLatitude, decimalLongitude, .keep_all = TRUE) |> # no dupes can be present
  st_as_sf(coords = c( 'decimalLongitude', 'decimalLatitude'), crs = 4326, remove = F) 
```


```{r prep basemap, echo = F}
western_states <- spData::us_states |> ## for making a quick basemap. 
  dplyr::filter(NAME %in% 
    c('Utah', 'Arizona', 'Colorado', 'New Mexico', 'Wyoming', 'Nevada', 'Idaho', 'California')) |>
  dplyr::select(NAME, geometry) |>
  st_transform(4326)

bb <- st_bbox(
  c(
    xmin = -114.5, 
    xmax = -106, 
    ymax = 42, 
    ymin = 33.5),
    crs = st_crs(4326)
    )

western_states <- st_crop(western_states, bb)

bmap <- ggplot() + 
    geom_sf(data = western_states) + 
    geom_sf(data = hemi) +
    theme_minimal() +
    coord_sf(
        xlim = c(bb[['xmin']], bb[['xmax']]), 
        ylim = c(bb[['ymin']], bb[['ymax']])
        )  +
    theme(
      axis.title.x=element_blank(),
      axis.text.x=element_blank(),
      axis.ticks.x=element_blank()
      )

bmap +
    labs(title = 'Helianthella microcephala\noccurrence records')
```

```{r remove cols and western states, echo = F}
rm(cols, western_states)
```

```{r download data, echo = FALSE}
# Download WorldClim bioclim at ~10 km
bio_current <- worldclim_global(var="bioc", res=2.5)
bio_future <- cmip6_world(
  model = "CNRM-CM6-1", ## modelling method
  ssp   = "245", ## "Middle of the Road" scenario
  time  = "2041-2060", # time period
  var   = "bioc", # just use the bioclim variables
  res   = 2.5
)

# Crop to domain - use a large BB to accomodate range shift
# under future time points. 
# but going too large will dilute the absence records
bbox <- ext(bb)

bio_current <- crop(bio_current, bbox)
bio_future <- crop(bio_future, bbox)
```

```{r standardize raster layer names, echo = FALSE}
simplify_names <- function(x){
    paste0('bio_', sprintf("%02d", as.numeric(gsub('\\D+','', names(x)))))
}

names(bio_current) <- gsub('^.*5m_', '', names(bio_current))
names(bio_future) <- gsub('^.*2060_', '', names(bio_future))

names(bio_current) <- simplify_names(bio_current)
names(bio_future) <- simplify_names(bio_future)

rm(simplify_names)
```


## fit the model
Here we fit as SDM using `bayesianSDM`. 
For arguments it requires occurrence data `x`, a raster stack of `predictors`, a `quantile_v` that is an offset used to create pseudo-absence data, and a `planar_proj` used to calculate distances between occurrence data and possible pseudo-absence points. 

```{r Create Species Distribution Model, message=F, warning=F}
hemi <- select(hemi) 

sdModel <- bayesianSDM( 
  x = hemi,
  pca_predictors = FALSE,
  predictors = bio_current,
  quantile_v = 0.025,
  planar_proj = 5070, 
  resample = TRUE,
  feature_selection = 'none',
  min_ffs_var = 5, 
  fact = 2, 
  silent = 2 # print fewer messages during model fit.
  )
```

Under the hood this function uses `brms`, which dispatches to Stan, to fit a bayesian hierarchical model (GLMM), where the distances between occurrence records are the fixed effect, useful to reduce the effects of spatial autocorrelation to increase interpret-ability of the models parameters.  

### explore the output

```{r}
ggplot() + 
  geom_sf(data = sdModel$TestData, aes(color = occurrence)) + 
  geom_sf(data = sdModel$TrainData, aes(color = occurrence))
```

Plot the predictions 
```{r Explore SDM output - Different alpha}
sdm_td <- sdModel$TrainData
plot(sdModel$RasterPredictions)
points(vect(sdm_td[sdm_td$occurrence==1,]),col = 'black')
points(vect(sdm_td[sdm_td$occurrence==0,]),col = 'red')

plot(sdModel$RasterPredictions_sd)
points(vect(sdm_td[sdm_td$occurrence==1,]),col = 'black')
points(vect(sdm_td[sdm_td$occurrence==0,]),col = 'red')
```

```{r}
sdModel$Model
```

```{r check diagnostics}
sdModel$Diagnostics
```

We used Accuracy rather than Kappa as the main criterion for model selection. 

```{r Explore SDM output - Confusion Matrix}
sdModel$ConfusionMatrix
```

Here we can see how our selected model works on predicting the state of test data.
Note these accuracy results are slightly higher than those from the CV folds. 
This is not a bug, CV folds are testing on their own holdouts, while this is a brand new set of holdouts. 
The main reason the confusion matrix results are likely to be higher is due to spatial auto-correlation which are not address in a typical 'random split' of test and train data.
In order to minimize the effects of spatial-autocorrelation on our model we use `CAST` under the hood which allows for spatially informed cross validation. 

Consider the output from CVStructure to be a bit more realistic. 

### binarize the output

```{r Explore SDM output - Map of Predictions}
terra::plot(sdModel$RasterPredictions)
```


```{r Threshold the SDM output}
threshold_rasts <- PostProcessSDM(
  rast_cont = sdModel$RasterPredictions, 
  test = sdModel$TestData,
  train = sdModel$TrainData,
  planar_proj = 5070,
  thresh_metric = 'sensitivity', 
  quant_amt = 0.2
  )
```

We can compare the results of applying this function side by side using the output from the function. 

```{r Compare Threshold Results}
terra::plot(threshold_rasts$FinalRasters)
```








```{r save the ouputs for vignettes, echo = F, eval = F}
#terra::writeRaster(threshold_rasts$FinalRasters, filename = file.path('..', 'inst', 'extdata', 'SDM_thresholds.tif'))
```

### rescale predictor variables
 

Below we create a copy of the raster predictions where we have standardized each variable, so it is equivalent to the input to glmnet, and then mulitplied by its beta-coefficient.
We will also write out these beta coefficients with the `writeSDMresults` function right afterwards. 
```{r Rescale Predictor Variables}
rr <- RescaleRasters_bayes(
  model = sdModel$Model,
  predictors = sdModel$Predictors, 
  training_data = sdModel$TrainData, 
  pred_mat = sdModel$PredictMatrix,
  include_uncertainty = TRUE
  )

terra::plot(rr$RescaledPredictors)
```

We can see that the variables are 'close' to being on the same scale, this will work in a clustering algorithm. 
If any of the layers are all the same color (maybe yellow?) that means they have no variance, that's a term that was shrunk from the model. 
It will be dealt with internally in a future function. 
The scales of the variables are not exact, because they are weighed by their \beta coefficients in the model

```{r Show beta Coefficients from model
print(rr$BetaCoefficients)
```




```{r}
PosteriorCluster(
  model = sdModel$Model,
  predictors = subset(rr$RescaledPredictors, 'coef_uncertainty', negate = TRUE),
  f_rasts = threshold_rasts$FinalRasters[['Threshold']],
  pred_mat = sdModel$PredictMatrix,
  training_data = sdModel$Train,
)
```



### save results 

```{r Save SDM results, eval = F}
bp <- '~/Documents/assoRted/StrategizingGermplasmCollections'

writeSDMresults(
  cv_model = sdModel$CVStructure, 
  pcnm = sdModel$PCNM, 
  model = sdModel$Model, 
  cm = sdModel$ConfusionMatrix, 
  coef_tab = rr$BetaCoefficients, 
  f_rasts = threshold_rasts$FinalRasters,
  thresh = threshold_rasts$Threshold,
  file.path(bp, 'results', 'SDM'), 'Bradypus_test')

# we can see that the files were placed here using this. 
list.files( file.path(bp, 'results', 'SDM'), recursive = TRUE )
```

### wrapping up 

And there you have it, all the steps to make a species distribution model - or rather to get the coefficients from a species distribution model!
We will play around with it as an example data set to compare our buffered distance results to at the end - head over to the next vignette!

```{r Clean up SDM variables, warning=FALSE, echo = FALSE}
rm(rr, predictors, files, sdModel, bp)
```
